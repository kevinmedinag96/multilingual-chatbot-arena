{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to create two types of personalized datasets for training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'datasets_creator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinmg96/miniconda3/envs/wsdm-cup/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "root_repo_directory = pathlib.Path().resolve().parent.__str__()\n",
    "sys.path.append(root_repo_directory)\n",
    "#from multilingual_chatbot_arena import initialize\n",
    "import src.constants as c\n",
    "import src.utils as utils\n",
    "import pandas as pd\n",
    "from fire import Fire\n",
    "from pydantic import BaseModel\n",
    "from typing import List,Optional,Dict,Union\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import opik\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig,AutoModelForSequenceClassification\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colors\n",
    "black = mcolors.CSS4_COLORS[\"black\"]\n",
    "lime = mcolors.CSS4_COLORS[\"lime\"]\n",
    "aqua = mcolors.CSS4_COLORS[\"aqua\"]\n",
    "magenta = mcolors.CSS4_COLORS[\"magenta\"]\n",
    "red = mcolors.CSS4_COLORS[\"red\"]\n",
    "grey = mcolors.CSS4_COLORS[\"grey\"]\n",
    "orange = mcolors.CSS4_COLORS[\"orangered\"]\n",
    "gold = mcolors.CSS4_COLORS[\"gold\"]\n",
    "blue = mcolors.CSS4_COLORS[\"blue\"]\n",
    "indigo = mcolors.CSS4_COLORS[\"indigo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading OG challenge's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = root_repo_directory + c.SLASH + 'data/original'\n",
    "train_df = pd.read_parquet(path + c.SLASH + \"train.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48439, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'prompt', 'response_a', 'response_b', 'winner', 'model_a',\n",
       "       'model_b', 'language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New baseline dataset: dropping languages's records whose items are below or equal to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: ('Abkhazian',)\n",
      "['model_a']\n",
      "Group: ('Afar',)\n",
      "['model_a']\n",
      "Group: ('Afrikaans',)\n",
      "['model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a']\n",
      "Group: ('Akan',)\n",
      "['model_b']\n",
      "Group: ('Albanian',)\n",
      "['model_b' 'model_a' 'model_a']\n",
      "Group: ('Amharic',)\n",
      "['model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Arabic',)\n",
      "['model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b']\n",
      "Group: ('Armenian',)\n",
      "['model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Assamese',)\n",
      "['model_b']\n",
      "Group: ('Aymara',)\n",
      "['model_a']\n",
      "Group: ('Azerbaijani',)\n",
      "['model_b' 'model_b']\n",
      "Group: ('Bangla',)\n",
      "['model_b' 'model_b' 'model_b' 'model_b']\n",
      "Group: ('Bashkir',)\n",
      "['model_a']\n",
      "Group: ('Basque',)\n",
      "['model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a']\n",
      "Group: ('Belarusian',)\n",
      "['model_a' 'model_b' 'model_a' 'model_b']\n",
      "Group: ('Bislama',)\n",
      "['model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a']\n",
      "Group: ('Bosnian',)\n",
      "['model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b']\n",
      "Group: ('Breton',)\n",
      "['model_b' 'model_a' 'model_b']\n",
      "Group: ('Bulgarian',)\n",
      "['model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a']\n",
      "Group: ('Catalan',)\n",
      "['model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a']\n",
      "Group: ('Chinese',)\n",
      "['model_b' 'model_b' 'model_a' ... 'model_b' 'model_b' 'model_b']\n",
      "Group: ('Corsican',)\n",
      "['model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b']\n",
      "Group: ('Croatian',)\n",
      "['model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b']\n",
      "Group: ('Czech',)\n",
      "['model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Danish',)\n",
      "['model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b']\n",
      "Group: ('Dutch',)\n",
      "['model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b']\n",
      "Group: ('Dzongkha',)\n",
      "['model_b']\n",
      "Group: ('English',)\n",
      "['model_b' 'model_a' 'model_a' ... 'model_b' 'model_a' 'model_a']\n",
      "Group: ('Esperanto',)\n",
      "['model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Estonian',)\n",
      "['model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Faroese',)\n",
      "['model_b' 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Finnish',)\n",
      "['model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a']\n",
      "Group: ('French',)\n",
      "['model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Galician',)\n",
      "['model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b']\n",
      "Group: ('Ganda',)\n",
      "['model_a' 'model_b' 'model_a']\n",
      "Group: ('German',)\n",
      "['model_a' 'model_a' 'model_b' ... 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Greek',)\n",
      "['model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b']\n",
      "Group: ('Guarani',)\n",
      "['model_a']\n",
      "Group: ('Haitian Creole',)\n",
      "['model_a' 'model_a']\n",
      "Group: ('Hausa',)\n",
      "['model_a']\n",
      "Group: ('Hawaiian',)\n",
      "['model_a']\n",
      "Group: ('Hebrew',)\n",
      "['model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b']\n",
      "Group: ('Hindi',)\n",
      "['model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Hmong',)\n",
      "['model_b' 'model_b']\n",
      "Group: ('Hungarian',)\n",
      "['model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a']\n",
      "Group: ('Icelandic',)\n",
      "['model_b' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Indonesian',)\n",
      "['model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b']\n",
      "Group: ('Interlingua',)\n",
      "['model_a' 'model_a' 'model_a' 'model_b' 'model_a']\n",
      "Group: ('Interlingue',)\n",
      "['model_b' 'model_a' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Inuktitut',)\n",
      "['model_b' 'model_a']\n",
      "Group: ('Irish',)\n",
      "['model_b' 'model_a' 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Italian',)\n",
      "['model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Japanese',)\n",
      "['model_a' 'model_b' 'model_b' ... 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Javanese',)\n",
      "['model_a' 'model_b' 'model_a' 'model_b']\n",
      "Group: ('Kalaallisut',)\n",
      "['model_b' 'model_b']\n",
      "Group: ('Kazakh',)\n",
      "['model_a' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Khasi',)\n",
      "['model_a' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Khmer',)\n",
      "['model_a']\n",
      "Group: ('Kinyarwanda',)\n",
      "['model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Klingon',)\n",
      "['model_b']\n",
      "Group: ('Korean',)\n",
      "['model_a' 'model_a' 'model_a' ... 'model_b' 'model_b' 'model_a']\n",
      "Group: ('Kurdish',)\n",
      "['model_a']\n",
      "Group: ('Kyrgyz',)\n",
      "['model_a' 'model_a' 'model_b']\n",
      "Group: ('Latin',)\n",
      "['model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a']\n",
      "Group: ('Latvian',)\n",
      "['model_a' 'model_a' 'model_b']\n",
      "Group: ('Lingala',)\n",
      "['model_b']\n",
      "Group: ('Lithuanian',)\n",
      "['model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Luxembourgish',)\n",
      "['model_b' 'model_b']\n",
      "Group: ('Macedonian',)\n",
      "['model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a']\n",
      "Group: ('Malagasy',)\n",
      "['model_b' 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Malay',)\n",
      "['model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a']\n",
      "Group: ('Manx',)\n",
      "['model_b' 'model_a' 'model_b' 'model_a' 'model_a']\n",
      "Group: ('Mongolian',)\n",
      "['model_b' 'model_b' 'model_a']\n",
      "Group: ('Nauru',)\n",
      "['model_a' 'model_a' 'model_a' 'model_b']\n",
      "Group: ('Norwegian',)\n",
      "['model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b']\n",
      "Group: ('Norwegian Nynorsk',)\n",
      "['model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a']\n",
      "Group: ('Nyanja',)\n",
      "['model_a' 'model_b' 'model_a']\n",
      "Group: ('Occitan',)\n",
      "['model_a' 'model_b' 'model_a' 'model_b']\n",
      "Group: ('Oromo',)\n",
      "['model_b' 'model_b' 'model_b']\n",
      "Group: ('Pashto',)\n",
      "['model_b' 'model_b']\n",
      "Group: ('Persian',)\n",
      "['model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b']\n",
      "Group: ('Polish',)\n",
      "['model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a']\n",
      "Group: ('Portuguese',)\n",
      "['model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a']\n",
      "Group: ('Quechua',)\n",
      "['model_a' 'model_b']\n",
      "Group: ('Romanian',)\n",
      "['model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Romansh',)\n",
      "['model_a' 'model_b' 'model_b' 'model_a']\n",
      "Group: ('Rundi',)\n",
      "['model_a']\n",
      "Group: ('Russian',)\n",
      "['model_a' 'model_b' 'model_a' ... 'model_b' 'model_a' 'model_a']\n",
      "Group: ('Samoan',)\n",
      "['model_a' 'model_a']\n",
      "Group: ('Sanskrit',)\n",
      "['model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Scots',)\n",
      "['model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a']\n",
      "Group: ('Scottish Gaelic',)\n",
      "['model_b']\n",
      "Group: ('Serbian',)\n",
      "['model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a']\n",
      "Group: ('Shona',)\n",
      "['model_a']\n",
      "Group: ('Sindhi',)\n",
      "['model_b']\n",
      "Group: ('Slovak',)\n",
      "['model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Slovenian',)\n",
      "['model_a' 'model_a' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Southern Sotho',)\n",
      "['model_b' 'model_b' 'model_a']\n",
      "Group: ('Spanish',)\n",
      "['model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b']\n",
      "Group: ('Sundanese',)\n",
      "['model_a' 'model_a']\n",
      "Group: ('Swahili',)\n",
      "['model_b' 'model_b']\n",
      "Group: ('Swati',)\n",
      "['model_a' 'model_a']\n",
      "Group: ('Swedish',)\n",
      "['model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b']\n",
      "Group: ('Tagalog',)\n",
      "['model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b']\n",
      "Group: ('Tamil',)\n",
      "['model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b']\n",
      "Group: ('Tatar',)\n",
      "['model_b' 'model_b' 'model_b' 'model_a' 'model_a']\n",
      "Group: ('Telugu',)\n",
      "['model_b']\n",
      "Group: ('Thai',)\n",
      "['model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a']\n",
      "Group: ('Tibetan',)\n",
      "['model_b' 'model_a']\n",
      "Group: ('Tongan',)\n",
      "['model_a' 'model_a' 'model_a']\n",
      "Group: ('Tsonga',)\n",
      "['model_a' 'model_b' 'model_b']\n",
      "Group: ('Tswana',)\n",
      "['model_b' 'model_b']\n",
      "Group: ('Turkish',)\n",
      "['model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b']\n",
      "Group: ('Turkmen',)\n",
      "['model_b' 'model_a' 'model_b']\n",
      "Group: ('Ukrainian',)\n",
      "['model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a'\n",
      " 'model_b' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_a' 'model_a' 'model_b' 'model_a' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_a' 'model_a' 'model_a' 'model_a' 'model_b' 'model_a'\n",
      " 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_b' 'model_a'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b'\n",
      " 'model_a' 'model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b'\n",
      " 'model_b' 'model_b' 'model_a' 'model_b' 'model_a' 'model_b']\n",
      "Group: ('Uzbek',)\n",
      "['model_b' 'model_a' 'model_a' 'model_b' 'model_b' 'model_a' 'model_a'\n",
      " 'model_a' 'model_b']\n",
      "Group: ('Vietnamese',)\n",
      "['model_b' 'model_a' 'model_b' ... 'model_a' 'model_a' 'model_a']\n",
      "Group: ('Volapük',)\n",
      "['model_a' 'model_b']\n",
      "Group: ('Waray',)\n",
      "['model_b']\n",
      "Group: ('Welsh',)\n",
      "['model_a' 'model_a' 'model_a' 'model_b' 'model_a']\n",
      "Group: ('Wolof',)\n",
      "['model_a' 'model_a' 'model_a' 'model_b']\n",
      "Group: ('Xhosa',)\n",
      "['model_b' 'model_a' 'model_b' 'model_a' 'model_b' 'model_b' 'model_b'\n",
      " 'model_b']\n",
      "Group: ('Yiddish',)\n",
      "['model_b']\n",
      "Group: ('Yoruba',)\n",
      "['model_b' 'model_b' 'model_a' 'model_a' 'model_a' 'model_b' 'model_b'\n",
      " 'model_b' 'model_b']\n",
      "Group: ('Zhuang',)\n",
      "['model_a']\n",
      "Group: ('unknown',)\n",
      "['model_a' 'model_a' 'model_b' ... 'model_b' 'model_a' 'model_a']\n",
      "Group: ('xx',)\n",
      "['model_a' 'model_b' 'model_a' 'model_b']\n",
      "Group: ('zzp',)\n",
      "['model_b' 'model_a']\n"
     ]
    }
   ],
   "source": [
    "groupby_language = train_df.groupby([\"language\"])\n",
    "\n",
    "for name, group in groupby_language:\n",
    "    print(f\"Group: {name}\")\n",
    "    print(group['winner'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1533/458869094.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df_baseline = groupby_language.apply(func,**kwargs).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def func(x,**kwargs):\n",
    "\n",
    "    if x.shape[0] > kwargs['num_group_records']:\n",
    "        return x\n",
    "    return \n",
    "\n",
    "\n",
    "num_group_records = 10\n",
    "\n",
    "kwargs = {\"num_group_records\" : num_group_records}\n",
    "\n",
    "\n",
    "train_df_baseline = groupby_language.apply(func,**kwargs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_language = train_df[\"language\"].value_counts(normalize=True)[(train_df[\"language\"].value_counts(normalize=True) > 0.01).values]\n",
    "\n",
    "df_series_language = pd.DataFrame({\n",
    "    \"languages\" : series_language.keys(),\n",
    "    \"percentages\" : series_language.values * 100 \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize= (5,5))\n",
    "sns.barplot(data=df_series_language,x=\"languages\",y=\"percentages\",ax=ax,color=grey)\n",
    "\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "ax.set_xlabel(\"Language\",fontsize=10)\n",
    "ax.set_ylabel(\"Percentage (%)\",fontsize=10)\n",
    "ax.set_title(\"Language's Percentage Distribution in Training Set\",fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"language\"].value_counts()[(train_df[\"language\"].value_counts() >= 10).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"language\"].value_counts()[(train_df[\"language\"].value_counts() <= 10).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "German       1402\n",
       "Japanese     1159\n",
       "unknown      1128\n",
       "Korean       1041\n",
       "Spanish       790\n",
       "             ... \n",
       "Yiddish         1\n",
       "Zhuang          1\n",
       "Abkhazian       1\n",
       "Waray           1\n",
       "Sindhi          1\n",
       "Name: count, Length: 124, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"language\"].value_counts()[(train_df[\"language\"].value_counts() <= 1402).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting language for each record with unknown category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use an LLM language detector to the records whose language is categorized as unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_dataset = train_df_baseline.loc[train_df_baseline['language'] == \"unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting LanguageDecectorDataset & LanguageDetectorDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDetectorDataset(Dataset):\n",
    "    def __init__(self,data : pd.DataFrame):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            data pd.DataFrame : data from dataset,\n",
    "            comet_id Optional[str] : dataset's name id from comet ML\n",
    "            prompt_id Optional[str] : prompt's template id from comet ML\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "          \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get either a single data point or a pandas Dataframe window of data points\n",
    "        data_window = self.data.iloc[idx]    \n",
    "\n",
    "        return data_window.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDetectorDataloader(DataLoader):\n",
    "    def __init__(self, tokenizer :  PreTrainedTokenizer | PreTrainedTokenizerFast, **kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        kwargs[\"collate_fn\"] = self.chatbot_collate\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    \n",
    "    def chatbot_collate(self,batch):\n",
    "        \"\"\"Custom collate function to teach the Dataloader class how to parse the batches into an llm friendly format\n",
    "        Args:\n",
    "            original_batch : List of batch elements with len -> batch_size. Each list's element strictly follows \n",
    "            the format inside __getitem__ from Dataset class. \n",
    "        \n",
    "        \"\"\"\n",
    "        prompts,records_id = [],[]\n",
    "\n",
    "        \n",
    "        for dic in batch:\n",
    "                \n",
    "            prompts.append(dic['prompt'])\n",
    "            records_id.append(dic['id'])\n",
    "\n",
    "\n",
    "        #tokenize batch of prompts and answers\n",
    "        prompt_tokenize = self.tokenizer(prompts,\n",
    "                padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"inputs\" : prompt_tokenize, #Dict[str,torch.Tensor]\n",
    "            \"records_id\" : records_id, #list[str]\n",
    "            \"longest_seq\" : prompt_tokenize[\"input_ids\"].shape[1] #int\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "@torch.no_grad()\n",
    "def language_detection_pipeline(model,dataloader):\n",
    "\n",
    "    data_pipeline = defaultdict(list)\n",
    "    try:\n",
    "\n",
    "        i = 0\n",
    "        for i,batch in enumerate(tqdm(dataloader,desc=f\"Language detection\")):\n",
    "            #if i < resume_idx:\n",
    "            #    continue\n",
    "\n",
    "            # Let's send current batch into model device\n",
    "\n",
    "            inputs= batch[\"inputs\"].to(model.device)\n",
    "\n",
    "            logger.info(f\"Batch: {i}. Max Batch Input tokens size : {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "\n",
    "            #forward batch of input tokens into the model, get output language ids\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "            logits = logits.detach().cpu()\n",
    "\n",
    "            prob_dists = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # Map raw predictions to languages\n",
    "            probs, idxs = torch.max(prob_dists, dim=1)\n",
    "\n",
    "            for language_id,record_id,prob in zip(idxs,batch['records_id'], probs):\n",
    "\n",
    "                data_pipeline['languages'].append(model.config.id2label[language_id.item()])\n",
    "                data_pipeline['records_ids'].append(record_id)\n",
    "                data_pipeline['probabilities'].append(prob.item())\n",
    "\n",
    "\n",
    "            #clear GPU cache\n",
    "            torch.cuda.empty_cache()                \n",
    "    except KeyboardInterrupt as k:\n",
    "        print(k)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        return data_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run language detection pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Language detection:   0%|          | 0/564 [00:00<?, ?it/s]\u001b[32m2025-02-19 08:53:05.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 0. Max Batch Input tokens size : 181\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 1. Max Batch Input tokens size : 31\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 2. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 3. Max Batch Input tokens size : 26\u001b[0m\n",
      "Language detection:   1%|          | 4/564 [00:00<00:14, 38.93it/s]\u001b[32m2025-02-19 08:53:05.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 4. Max Batch Input tokens size : 95\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.574\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 5. Max Batch Input tokens size : 422\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 6. Max Batch Input tokens size : 165\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 7. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:   1%|▏         | 8/564 [00:00<00:15, 35.08it/s]\u001b[32m2025-02-19 08:53:05.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 8. Max Batch Input tokens size : 205\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 9. Max Batch Input tokens size : 94\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 10. Max Batch Input tokens size : 87\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 11. Max Batch Input tokens size : 25\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 12. Max Batch Input tokens size : 27\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 13. Max Batch Input tokens size : 93\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 14. Max Batch Input tokens size : 124\u001b[0m\n",
      "Language detection:   3%|▎         | 15/564 [00:00<00:11, 46.90it/s]\u001b[32m2025-02-19 08:53:05.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 15. Max Batch Input tokens size : 31\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 16. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 17. Max Batch Input tokens size : 104\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 18. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 19. Max Batch Input tokens size : 28\u001b[0m\n",
      "Language detection:   4%|▎         | 20/564 [00:00<00:14, 37.43it/s]\u001b[32m2025-02-19 08:53:05.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 20. Max Batch Input tokens size : 445\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:05.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 21. Max Batch Input tokens size : 195\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 22. Max Batch Input tokens size : 77\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 23. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:   4%|▍         | 24/564 [00:00<00:14, 36.98it/s]\u001b[32m2025-02-19 08:53:06.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 24. Max Batch Input tokens size : 40\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.093\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 25. Max Batch Input tokens size : 7\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 26. Max Batch Input tokens size : 261\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 27. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 28. Max Batch Input tokens size : 85\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 29. Max Batch Input tokens size : 92\u001b[0m\n",
      "Language detection:   5%|▌         | 30/564 [00:00<00:12, 42.69it/s]\u001b[32m2025-02-19 08:53:06.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 30. Max Batch Input tokens size : 20\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 31. Max Batch Input tokens size : 9\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 32. Max Batch Input tokens size : 88\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 33. Max Batch Input tokens size : 33\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 34. Max Batch Input tokens size : 263\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 35. Max Batch Input tokens size : 83\u001b[0m\n",
      "Language detection:   6%|▋         | 36/564 [00:00<00:11, 47.48it/s]\u001b[32m2025-02-19 08:53:06.288\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 36. Max Batch Input tokens size : 17\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 37. Max Batch Input tokens size : 37\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 38. Max Batch Input tokens size : 22\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.329\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 39. Max Batch Input tokens size : 15\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 40. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 41. Max Batch Input tokens size : 21\u001b[0m\n",
      "Language detection:   7%|▋         | 42/564 [00:00<00:10, 50.95it/s]\u001b[32m2025-02-19 08:53:06.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 42. Max Batch Input tokens size : 48\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 43. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 44. Max Batch Input tokens size : 509\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 45. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.526\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 46. Max Batch Input tokens size : 73\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 47. Max Batch Input tokens size : 12\u001b[0m\n",
      "Language detection:   9%|▊         | 48/564 [00:01<00:11, 44.95it/s]\u001b[32m2025-02-19 08:53:06.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 48. Max Batch Input tokens size : 201\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 49. Max Batch Input tokens size : 237\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 50. Max Batch Input tokens size : 13\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 51. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 52. Max Batch Input tokens size : 208\u001b[0m\n",
      "Language detection:   9%|▉         | 53/564 [00:01<00:11, 43.51it/s]\u001b[32m2025-02-19 08:53:06.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 53. Max Batch Input tokens size : 93\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 54. Max Batch Input tokens size : 35\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 55. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 56. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 57. Max Batch Input tokens size : 480\u001b[0m\n",
      "Language detection:  10%|█         | 58/564 [00:01<00:12, 40.03it/s]\u001b[32m2025-02-19 08:53:06.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 58. Max Batch Input tokens size : 109\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 59. Max Batch Input tokens size : 19\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 60. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 61. Max Batch Input tokens size : 316\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 62. Max Batch Input tokens size : 148\u001b[0m\n",
      "Language detection:  11%|█         | 63/564 [00:01<00:12, 39.27it/s]\u001b[32m2025-02-19 08:53:06.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 63. Max Batch Input tokens size : 45\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 64. Max Batch Input tokens size : 122\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:06.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 65. Max Batch Input tokens size : 21\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 66. Max Batch Input tokens size : 13\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 67. Max Batch Input tokens size : 214\u001b[0m\n",
      "Language detection:  12%|█▏        | 68/564 [00:01<00:12, 39.73it/s]\u001b[32m2025-02-19 08:53:07.091\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 68. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 69. Max Batch Input tokens size : 405\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 70. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 71. Max Batch Input tokens size : 16\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 72. Max Batch Input tokens size : 346\u001b[0m\n",
      "Language detection:  13%|█▎        | 73/564 [00:01<00:13, 35.77it/s]\u001b[32m2025-02-19 08:53:07.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 73. Max Batch Input tokens size : 23\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 74. Max Batch Input tokens size : 17\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 75. Max Batch Input tokens size : 48\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 76. Max Batch Input tokens size : 14\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 77. Max Batch Input tokens size : 52\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 78. Max Batch Input tokens size : 42\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 79. Max Batch Input tokens size : 37\u001b[0m\n",
      "Language detection:  14%|█▍        | 80/564 [00:01<00:11, 43.25it/s]\u001b[32m2025-02-19 08:53:07.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 80. Max Batch Input tokens size : 20\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 81. Max Batch Input tokens size : 170\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 82. Max Batch Input tokens size : 180\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 83. Max Batch Input tokens size : 90\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 84. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  15%|█▌        | 85/564 [00:02<00:10, 43.63it/s]\u001b[32m2025-02-19 08:53:07.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 85. Max Batch Input tokens size : 68\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 86. Max Batch Input tokens size : 65\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 87. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 88. Max Batch Input tokens size : 321\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 89. Max Batch Input tokens size : 123\u001b[0m\n",
      "Language detection:  16%|█▌        | 90/564 [00:02<00:10, 43.24it/s]\u001b[32m2025-02-19 08:53:07.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 90. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 91. Max Batch Input tokens size : 68\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 92. Max Batch Input tokens size : 221\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 93. Max Batch Input tokens size : 336\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 94. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  17%|█▋        | 95/564 [00:02<00:11, 40.83it/s]\u001b[32m2025-02-19 08:53:07.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 95. Max Batch Input tokens size : 37\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 96. Max Batch Input tokens size : 32\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 97. Max Batch Input tokens size : 54\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 98. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 99. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  18%|█▊        | 100/564 [00:02<00:11, 41.73it/s]\u001b[32m2025-02-19 08:53:07.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 100. Max Batch Input tokens size : 8\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 101. Max Batch Input tokens size : 138\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 102. Max Batch Input tokens size : 87\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 103. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 104. Max Batch Input tokens size : 14\u001b[0m\n",
      "Language detection:  19%|█▊        | 105/564 [00:02<00:10, 42.01it/s]\u001b[32m2025-02-19 08:53:07.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 105. Max Batch Input tokens size : 28\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:07.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 106. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 107. Max Batch Input tokens size : 65\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 108. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 109. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  20%|█▉        | 110/564 [00:02<00:12, 37.13it/s]\u001b[32m2025-02-19 08:53:08.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 110. Max Batch Input tokens size : 34\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 111. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 112. Max Batch Input tokens size : 500\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 113. Max Batch Input tokens size : 22\u001b[0m\n",
      "Language detection:  20%|██        | 114/564 [00:02<00:12, 36.20it/s]\u001b[32m2025-02-19 08:53:08.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 114. Max Batch Input tokens size : 48\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 115. Max Batch Input tokens size : 25\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 116. Max Batch Input tokens size : 14\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 117. Max Batch Input tokens size : 496\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 118. Max Batch Input tokens size : 418\u001b[0m\n",
      "Language detection:  21%|██        | 119/564 [00:02<00:11, 37.60it/s]\u001b[32m2025-02-19 08:53:08.376\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 119. Max Batch Input tokens size : 47\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 120. Max Batch Input tokens size : 7\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 121. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 122. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  22%|██▏       | 123/564 [00:03<00:12, 36.64it/s]\u001b[32m2025-02-19 08:53:08.493\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 123. Max Batch Input tokens size : 10\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.509\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 124. Max Batch Input tokens size : 25\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 125. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 126. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  23%|██▎       | 127/564 [00:03<00:11, 36.45it/s]\u001b[32m2025-02-19 08:53:08.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 127. Max Batch Input tokens size : 240\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 128. Max Batch Input tokens size : 206\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 129. Max Batch Input tokens size : 65\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 130. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 131. Max Batch Input tokens size : 137\u001b[0m\n",
      "Language detection:  23%|██▎       | 132/564 [00:03<00:10, 39.39it/s]\u001b[32m2025-02-19 08:53:08.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 132. Max Batch Input tokens size : 311\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 133. Max Batch Input tokens size : 32\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 134. Max Batch Input tokens size : 44\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 135. Max Batch Input tokens size : 157\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 136. Max Batch Input tokens size : 310\u001b[0m\n",
      "Language detection:  24%|██▍       | 137/564 [00:03<00:10, 39.98it/s]\u001b[32m2025-02-19 08:53:08.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 137. Max Batch Input tokens size : 20\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 138. Max Batch Input tokens size : 80\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 139. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 140. Max Batch Input tokens size : 63\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 141. Max Batch Input tokens size : 410\u001b[0m\n",
      "Language detection:  25%|██▌       | 142/564 [00:03<00:10, 40.44it/s]\u001b[32m2025-02-19 08:53:08.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 142. Max Batch Input tokens size : 372\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.978\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 143. Max Batch Input tokens size : 59\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:08.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 144. Max Batch Input tokens size : 39\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 145. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 146. Max Batch Input tokens size : 48\u001b[0m\n",
      "Language detection:  26%|██▌       | 147/564 [00:03<00:10, 41.64it/s]\u001b[32m2025-02-19 08:53:09.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 147. Max Batch Input tokens size : 66\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 148. Max Batch Input tokens size : 95\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 149. Max Batch Input tokens size : 58\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 150. Max Batch Input tokens size : 94\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 151. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.155\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 152. Max Batch Input tokens size : 68\u001b[0m\n",
      "Language detection:  27%|██▋       | 153/564 [00:03<00:08, 45.85it/s]\u001b[32m2025-02-19 08:53:09.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 153. Max Batch Input tokens size : 370\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 154. Max Batch Input tokens size : 38\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 155. Max Batch Input tokens size : 18\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 156. Max Batch Input tokens size : 87\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 157. Max Batch Input tokens size : 296\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 158. Max Batch Input tokens size : 21\u001b[0m\n",
      "Language detection:  28%|██▊       | 159/564 [00:03<00:08, 49.16it/s]\u001b[32m2025-02-19 08:53:09.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 159. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 160. Max Batch Input tokens size : 284\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 161. Max Batch Input tokens size : 232\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 162. Max Batch Input tokens size : 16\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.372\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 163. Max Batch Input tokens size : 282\u001b[0m\n",
      "Language detection:  29%|██▉       | 164/564 [00:03<00:08, 46.70it/s]\u001b[32m2025-02-19 08:53:09.394\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 164. Max Batch Input tokens size : 17\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 165. Max Batch Input tokens size : 25\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 166. Max Batch Input tokens size : 46\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 167. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 168. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  30%|██▉       | 169/564 [00:04<00:08, 45.11it/s]\u001b[32m2025-02-19 08:53:09.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 169. Max Batch Input tokens size : 56\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 170. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 171. Max Batch Input tokens size : 81\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 172. Max Batch Input tokens size : 95\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 173. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  31%|███       | 174/564 [00:04<00:08, 43.51it/s]\u001b[32m2025-02-19 08:53:09.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 174. Max Batch Input tokens size : 128\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 175. Max Batch Input tokens size : 282\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 176. Max Batch Input tokens size : 172\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 177. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.733\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 178. Max Batch Input tokens size : 22\u001b[0m\n",
      "Language detection:  32%|███▏      | 179/564 [00:04<00:08, 44.55it/s]\u001b[32m2025-02-19 08:53:09.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 179. Max Batch Input tokens size : 418\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 180. Max Batch Input tokens size : 443\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 181. Max Batch Input tokens size : 269\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 182. Max Batch Input tokens size : 101\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 183. Max Batch Input tokens size : 53\u001b[0m\n",
      "Language detection:  33%|███▎      | 184/564 [00:04<00:08, 43.52it/s]\u001b[32m2025-02-19 08:53:09.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 184. Max Batch Input tokens size : 36\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 185. Max Batch Input tokens size : 490\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 186. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 187. Max Batch Input tokens size : 41\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:09.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 188. Max Batch Input tokens size : 12\u001b[0m\n",
      "Language detection:  34%|███▎      | 189/564 [00:04<00:08, 41.75it/s]\u001b[32m2025-02-19 08:53:09.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 189. Max Batch Input tokens size : 32\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 190. Max Batch Input tokens size : 177\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 191. Max Batch Input tokens size : 56\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 192. Max Batch Input tokens size : 65\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.066\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 193. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  34%|███▍      | 194/564 [00:04<00:08, 43.72it/s]\u001b[32m2025-02-19 08:53:10.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 194. Max Batch Input tokens size : 16\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 195. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 196. Max Batch Input tokens size : 138\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 197. Max Batch Input tokens size : 35\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 198. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  35%|███▌      | 199/564 [00:04<00:08, 41.76it/s]\u001b[32m2025-02-19 08:53:10.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 199. Max Batch Input tokens size : 27\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 200. Max Batch Input tokens size : 430\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 201. Max Batch Input tokens size : 160\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 202. Max Batch Input tokens size : 92\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 203. Max Batch Input tokens size : 27\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 204. Max Batch Input tokens size : 31\u001b[0m\n",
      "Language detection:  36%|███▋      | 205/564 [00:04<00:07, 45.47it/s]\u001b[32m2025-02-19 08:53:10.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 205. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 206. Max Batch Input tokens size : 34\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 207. Max Batch Input tokens size : 17\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 208. Max Batch Input tokens size : 60\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 209. Max Batch Input tokens size : 481\u001b[0m\n",
      "Language detection:  37%|███▋      | 210/564 [00:05<00:07, 44.73it/s]\u001b[32m2025-02-19 08:53:10.462\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 210. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 211. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 212. Max Batch Input tokens size : 94\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 213. Max Batch Input tokens size : 243\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 214. Max Batch Input tokens size : 80\u001b[0m\n",
      "Language detection:  38%|███▊      | 215/564 [00:05<00:08, 43.29it/s]\u001b[32m2025-02-19 08:53:10.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 215. Max Batch Input tokens size : 138\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 216. Max Batch Input tokens size : 11\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 217. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 218. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 219. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  39%|███▉      | 220/564 [00:05<00:08, 39.47it/s]\u001b[32m2025-02-19 08:53:10.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 220. Max Batch Input tokens size : 471\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 221. Max Batch Input tokens size : 28\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 222. Max Batch Input tokens size : 502\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 223. Max Batch Input tokens size : 69\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 224. Max Batch Input tokens size : 129\u001b[0m\n",
      "Language detection:  40%|███▉      | 225/564 [00:05<00:08, 39.90it/s]\u001b[32m2025-02-19 08:53:10.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 225. Max Batch Input tokens size : 328\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 226. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 227. Max Batch Input tokens size : 13\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 228. Max Batch Input tokens size : 463\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:10.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 229. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  41%|████      | 230/564 [00:05<00:08, 38.34it/s]\u001b[32m2025-02-19 08:53:11.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 230. Max Batch Input tokens size : 119\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 231. Max Batch Input tokens size : 315\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 232. Max Batch Input tokens size : 18\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 233. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  41%|████▏     | 234/564 [00:05<00:08, 38.20it/s]\u001b[32m2025-02-19 08:53:11.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 234. Max Batch Input tokens size : 111\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 235. Max Batch Input tokens size : 78\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 236. Max Batch Input tokens size : 60\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 237. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 238. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  42%|████▏     | 239/564 [00:05<00:08, 38.76it/s]\u001b[32m2025-02-19 08:53:11.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 239. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.265\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 240. Max Batch Input tokens size : 52\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 241. Max Batch Input tokens size : 71\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 242. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  43%|████▎     | 243/564 [00:05<00:08, 38.85it/s]\u001b[32m2025-02-19 08:53:11.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 243. Max Batch Input tokens size : 87\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 244. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 245. Max Batch Input tokens size : 35\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.418\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 246. Max Batch Input tokens size : 171\u001b[0m\n",
      "Language detection:  44%|████▍     | 247/564 [00:05<00:08, 38.76it/s]\u001b[32m2025-02-19 08:53:11.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 247. Max Batch Input tokens size : 74\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 248. Max Batch Input tokens size : 251\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 249. Max Batch Input tokens size : 36\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.497\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 250. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  45%|████▍     | 251/564 [00:06<00:08, 38.21it/s]\u001b[32m2025-02-19 08:53:11.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 251. Max Batch Input tokens size : 89\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 252. Max Batch Input tokens size : 82\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 253. Max Batch Input tokens size : 259\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 254. Max Batch Input tokens size : 59\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 255. Max Batch Input tokens size : 31\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 256. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  46%|████▌     | 257/564 [00:06<00:07, 41.20it/s]\u001b[32m2025-02-19 08:53:11.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 257. Max Batch Input tokens size : 12\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 258. Max Batch Input tokens size : 43\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 259. Max Batch Input tokens size : 24\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 260. Max Batch Input tokens size : 167\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 261. Max Batch Input tokens size : 128\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 262. Max Batch Input tokens size : 75\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.766\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 263. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  47%|████▋     | 264/564 [00:06<00:06, 45.46it/s]\u001b[32m2025-02-19 08:53:11.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 264. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 265. Max Batch Input tokens size : 148\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 266. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 267. Max Batch Input tokens size : 105\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 268. Max Batch Input tokens size : 26\u001b[0m\n",
      "Language detection:  48%|████▊     | 269/564 [00:06<00:06, 44.69it/s]\u001b[32m2025-02-19 08:53:11.917\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 269. Max Batch Input tokens size : 10\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 270. Max Batch Input tokens size : 45\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 271. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:11.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 272. Max Batch Input tokens size : 120\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 273. Max Batch Input tokens size : 27\u001b[0m\n",
      "Language detection:  49%|████▊     | 274/564 [00:06<00:06, 45.59it/s]\u001b[32m2025-02-19 08:53:12.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 274. Max Batch Input tokens size : 72\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 275. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 276. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 277. Max Batch Input tokens size : 30\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 278. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  49%|████▉     | 279/564 [00:06<00:06, 41.48it/s]\u001b[32m2025-02-19 08:53:12.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 279. Max Batch Input tokens size : 141\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 280. Max Batch Input tokens size : 17\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 281. Max Batch Input tokens size : 21\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 282. Max Batch Input tokens size : 241\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 283. Max Batch Input tokens size : 15\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 284. Max Batch Input tokens size : 51\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 285. Max Batch Input tokens size : 71\u001b[0m\n",
      "Language detection:  51%|█████     | 286/564 [00:06<00:05, 48.34it/s]\u001b[32m2025-02-19 08:53:12.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 286. Max Batch Input tokens size : 411\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 287. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 288. Max Batch Input tokens size : 62\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 289. Max Batch Input tokens size : 36\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.387\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 290. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  52%|█████▏    | 291/564 [00:06<00:06, 43.25it/s]\u001b[32m2025-02-19 08:53:12.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 291. Max Batch Input tokens size : 112\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 292. Max Batch Input tokens size : 91\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 293. Max Batch Input tokens size : 110\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 294. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 295. Max Batch Input tokens size : 14\u001b[0m\n",
      "Language detection:  52%|█████▏    | 296/564 [00:07<00:06, 44.49it/s]\u001b[32m2025-02-19 08:53:12.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 296. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 297. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 298. Max Batch Input tokens size : 77\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 299. Max Batch Input tokens size : 71\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 300. Max Batch Input tokens size : 37\u001b[0m\n",
      "Language detection:  53%|█████▎    | 301/564 [00:07<00:06, 42.32it/s]\u001b[32m2025-02-19 08:53:12.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 301. Max Batch Input tokens size : 58\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 302. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 303. Max Batch Input tokens size : 39\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 304. Max Batch Input tokens size : 166\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 305. Max Batch Input tokens size : 13\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 306. Max Batch Input tokens size : 28\u001b[0m\n",
      "Language detection:  54%|█████▍    | 307/564 [00:07<00:05, 45.67it/s]\u001b[32m2025-02-19 08:53:12.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 307. Max Batch Input tokens size : 269\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 308. Max Batch Input tokens size : 28\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 309. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 310. Max Batch Input tokens size : 77\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.873\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 311. Max Batch Input tokens size : 98\u001b[0m\n",
      "Language detection:  55%|█████▌    | 312/564 [00:07<00:05, 44.11it/s]\u001b[32m2025-02-19 08:53:12.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 312. Max Batch Input tokens size : 439\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 313. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 314. Max Batch Input tokens size : 81\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 315. Max Batch Input tokens size : 89\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:12.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 316. Max Batch Input tokens size : 163\u001b[0m\n",
      "Language detection:  56%|█████▌    | 317/564 [00:07<00:05, 43.21it/s]\u001b[32m2025-02-19 08:53:13.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 317. Max Batch Input tokens size : 85\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 318. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 319. Max Batch Input tokens size : 87\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.075\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 320. Max Batch Input tokens size : 50\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 321. Max Batch Input tokens size : 195\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 322. Max Batch Input tokens size : 18\u001b[0m\n",
      "Language detection:  57%|█████▋    | 323/564 [00:07<00:05, 46.60it/s]\u001b[32m2025-02-19 08:53:13.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 323. Max Batch Input tokens size : 121\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.147\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 324. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 325. Max Batch Input tokens size : 42\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 326. Max Batch Input tokens size : 122\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 327. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  58%|█████▊    | 328/564 [00:07<00:05, 44.13it/s]\u001b[32m2025-02-19 08:53:13.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 328. Max Batch Input tokens size : 222\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.268\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 329. Max Batch Input tokens size : 56\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 330. Max Batch Input tokens size : 26\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 331. Max Batch Input tokens size : 60\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 332. Max Batch Input tokens size : 32\u001b[0m\n",
      "Language detection:  59%|█████▉    | 333/564 [00:07<00:05, 44.85it/s]\u001b[32m2025-02-19 08:53:13.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 333. Max Batch Input tokens size : 75\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 334. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 335. Max Batch Input tokens size : 101\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 336. Max Batch Input tokens size : 66\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 337. Max Batch Input tokens size : 141\u001b[0m\n",
      "Language detection:  60%|█████▉    | 338/564 [00:08<00:04, 46.17it/s]\u001b[32m2025-02-19 08:53:13.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 338. Max Batch Input tokens size : 33\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.474\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 339. Max Batch Input tokens size : 25\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 340. Max Batch Input tokens size : 13\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 341. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 342. Max Batch Input tokens size : 63\u001b[0m\n",
      "Language detection:  61%|██████    | 343/564 [00:08<00:04, 46.80it/s]\u001b[32m2025-02-19 08:53:13.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 343. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 344. Max Batch Input tokens size : 74\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 345. Max Batch Input tokens size : 456\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 346. Max Batch Input tokens size : 37\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 347. Max Batch Input tokens size : 15\u001b[0m\n",
      "Language detection:  62%|██████▏   | 348/564 [00:08<00:04, 46.35it/s]\u001b[32m2025-02-19 08:53:13.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 348. Max Batch Input tokens size : 59\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 349. Max Batch Input tokens size : 104\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 350. Max Batch Input tokens size : 9\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 351. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 352. Max Batch Input tokens size : 46\u001b[0m\n",
      "Language detection:  63%|██████▎   | 353/564 [00:08<00:04, 43.18it/s]\u001b[32m2025-02-19 08:53:13.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 353. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 354. Max Batch Input tokens size : 44\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 355. Max Batch Input tokens size : 21\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 356. Max Batch Input tokens size : 76\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 357. Max Batch Input tokens size : 54\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 358. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  64%|██████▎   | 359/564 [00:08<00:04, 44.62it/s]\u001b[32m2025-02-19 08:53:13.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 359. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:13.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 360. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 361. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 362. Max Batch Input tokens size : 36\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.073\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 363. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  65%|██████▍   | 364/564 [00:08<00:05, 38.26it/s]\u001b[32m2025-02-19 08:53:14.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 364. Max Batch Input tokens size : 81\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 365. Max Batch Input tokens size : 47\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.150\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 366. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 367. Max Batch Input tokens size : 9\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 368. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  65%|██████▌   | 369/564 [00:08<00:05, 38.77it/s]\u001b[32m2025-02-19 08:53:14.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 369. Max Batch Input tokens size : 53\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 370. Max Batch Input tokens size : 158\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 371. Max Batch Input tokens size : 26\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 372. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 373. Max Batch Input tokens size : 27\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 374. Max Batch Input tokens size : 26\u001b[0m\n",
      "Language detection:  66%|██████▋   | 375/564 [00:08<00:04, 42.74it/s]\u001b[32m2025-02-19 08:53:14.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 375. Max Batch Input tokens size : 30\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 376. Max Batch Input tokens size : 453\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.397\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 377. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 378. Max Batch Input tokens size : 37\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 379. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  67%|██████▋   | 380/564 [00:09<00:04, 41.08it/s]\u001b[32m2025-02-19 08:53:14.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 380. Max Batch Input tokens size : 105\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.490\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 381. Max Batch Input tokens size : 36\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 382. Max Batch Input tokens size : 144\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 383. Max Batch Input tokens size : 35\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 384. Max Batch Input tokens size : 130\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 385. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  68%|██████▊   | 386/564 [00:09<00:04, 44.30it/s]\u001b[32m2025-02-19 08:53:14.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 386. Max Batch Input tokens size : 130\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 387. Max Batch Input tokens size : 21\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 388. Max Batch Input tokens size : 16\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 389. Max Batch Input tokens size : 44\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 390. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  69%|██████▉   | 391/564 [00:09<00:03, 45.28it/s]\u001b[32m2025-02-19 08:53:14.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 391. Max Batch Input tokens size : 491\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 392. Max Batch Input tokens size : 20\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 393. Max Batch Input tokens size : 10\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 394. Max Batch Input tokens size : 212\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 395. Max Batch Input tokens size : 224\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 396. Max Batch Input tokens size : 72\u001b[0m\n",
      "Language detection:  70%|███████   | 397/564 [00:09<00:03, 47.36it/s]\u001b[32m2025-02-19 08:53:14.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 397. Max Batch Input tokens size : 6\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 398. Max Batch Input tokens size : 71\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 399. Max Batch Input tokens size : 207\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 400. Max Batch Input tokens size : 396\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 401. Max Batch Input tokens size : 119\u001b[0m\n",
      "Language detection:  71%|███████▏  | 402/564 [00:09<00:03, 47.70it/s]\u001b[32m2025-02-19 08:53:14.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 402. Max Batch Input tokens size : 7\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 403. Max Batch Input tokens size : 69\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 404. Max Batch Input tokens size : 69\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 405. Max Batch Input tokens size : 377\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 406. Max Batch Input tokens size : 31\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:14.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 407. Max Batch Input tokens size : 93\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 408. Max Batch Input tokens size : 94\u001b[0m\n",
      "Language detection:  73%|███████▎  | 409/564 [00:09<00:02, 51.68it/s]\u001b[32m2025-02-19 08:53:15.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 409. Max Batch Input tokens size : 151\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 410. Max Batch Input tokens size : 23\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 411. Max Batch Input tokens size : 64\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.078\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 412. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 413. Max Batch Input tokens size : 44\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 414. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  74%|███████▎  | 415/564 [00:09<00:02, 49.74it/s]\u001b[32m2025-02-19 08:53:15.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 415. Max Batch Input tokens size : 16\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 416. Max Batch Input tokens size : 283\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 417. Max Batch Input tokens size : 189\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 418. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 419. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 420. Max Batch Input tokens size : 14\u001b[0m\n",
      "Language detection:  75%|███████▍  | 421/564 [00:09<00:03, 47.63it/s]\u001b[32m2025-02-19 08:53:15.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 421. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 422. Max Batch Input tokens size : 18\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 423. Max Batch Input tokens size : 215\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 424. Max Batch Input tokens size : 81\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 425. Max Batch Input tokens size : 151\u001b[0m\n",
      "Language detection:  76%|███████▌  | 426/564 [00:09<00:02, 47.65it/s]\u001b[32m2025-02-19 08:53:15.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 426. Max Batch Input tokens size : 10\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 427. Max Batch Input tokens size : 79\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 428. Max Batch Input tokens size : 42\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 429. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 430. Max Batch Input tokens size : 26\u001b[0m\n",
      "Language detection:  76%|███████▋  | 431/564 [00:10<00:02, 46.53it/s]\u001b[32m2025-02-19 08:53:15.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 431. Max Batch Input tokens size : 116\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 432. Max Batch Input tokens size : 8\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 433. Max Batch Input tokens size : 42\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 434. Max Batch Input tokens size : 120\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.581\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 435. Max Batch Input tokens size : 493\u001b[0m\n",
      "Language detection:  77%|███████▋  | 436/564 [00:10<00:02, 47.37it/s]\u001b[32m2025-02-19 08:53:15.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 436. Max Batch Input tokens size : 33\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 437. Max Batch Input tokens size : 243\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 438. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 439. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 440. Max Batch Input tokens size : 106\u001b[0m\n",
      "Language detection:  78%|███████▊  | 441/564 [00:10<00:02, 44.69it/s]\u001b[32m2025-02-19 08:53:15.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 441. Max Batch Input tokens size : 74\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 442. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 443. Max Batch Input tokens size : 15\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 444. Max Batch Input tokens size : 15\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 445. Max Batch Input tokens size : 336\u001b[0m\n",
      "Language detection:  79%|███████▉  | 446/564 [00:10<00:02, 44.87it/s]\u001b[32m2025-02-19 08:53:15.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 446. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 447. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 448. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:15.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 449. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 450. Max Batch Input tokens size : 19\u001b[0m\n",
      "Language detection:  80%|███████▉  | 451/564 [00:10<00:02, 39.15it/s]\u001b[32m2025-02-19 08:53:16.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 451. Max Batch Input tokens size : 430\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 452. Max Batch Input tokens size : 251\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.074\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 453. Max Batch Input tokens size : 7\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.086\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 454. Max Batch Input tokens size : 22\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 455. Max Batch Input tokens size : 27\u001b[0m\n",
      "Language detection:  81%|████████  | 456/564 [00:10<00:02, 41.60it/s]\u001b[32m2025-02-19 08:53:16.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 456. Max Batch Input tokens size : 148\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 457. Max Batch Input tokens size : 179\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 458. Max Batch Input tokens size : 57\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 459. Max Batch Input tokens size : 132\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 460. Max Batch Input tokens size : 19\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 461. Max Batch Input tokens size : 92\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 462. Max Batch Input tokens size : 57\u001b[0m\n",
      "Language detection:  82%|████████▏ | 463/564 [00:10<00:02, 47.24it/s]\u001b[32m2025-02-19 08:53:16.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 463. Max Batch Input tokens size : 282\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 464. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 465. Max Batch Input tokens size : 141\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.338\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 466. Max Batch Input tokens size : 405\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 467. Max Batch Input tokens size : 34\u001b[0m\n",
      "Language detection:  83%|████████▎ | 468/564 [00:10<00:02, 43.20it/s]\u001b[32m2025-02-19 08:53:16.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 468. Max Batch Input tokens size : 386\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 469. Max Batch Input tokens size : 35\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 470. Max Batch Input tokens size : 105\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 471. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 472. Max Batch Input tokens size : 40\u001b[0m\n",
      "Language detection:  84%|████████▍ | 473/564 [00:11<00:02, 43.79it/s]\u001b[32m2025-02-19 08:53:16.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 473. Max Batch Input tokens size : 439\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 474. Max Batch Input tokens size : 48\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 475. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 476. Max Batch Input tokens size : 70\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 477. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  85%|████████▍ | 478/564 [00:11<00:02, 40.28it/s]\u001b[32m2025-02-19 08:53:16.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 478. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.681\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 479. Max Batch Input tokens size : 100\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 480. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 481. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 482. Max Batch Input tokens size : 50\u001b[0m\n",
      "Language detection:  86%|████████▌ | 483/564 [00:11<00:02, 37.46it/s]\u001b[32m2025-02-19 08:53:16.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 483. Max Batch Input tokens size : 435\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 484. Max Batch Input tokens size : 45\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 485. Max Batch Input tokens size : 185\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 486. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  86%|████████▋ | 487/564 [00:11<00:02, 36.37it/s]\u001b[32m2025-02-19 08:53:16.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 487. Max Batch Input tokens size : 67\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 488. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.963\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 489. Max Batch Input tokens size : 335\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:16.989\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 490. Max Batch Input tokens size : 201\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 491. Max Batch Input tokens size : 16\u001b[0m\n",
      "Language detection:  87%|████████▋ | 492/564 [00:11<00:01, 39.15it/s]\u001b[32m2025-02-19 08:53:17.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 492. Max Batch Input tokens size : 222\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 493. Max Batch Input tokens size : 42\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 494. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 495. Max Batch Input tokens size : 323\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 496. Max Batch Input tokens size : 112\u001b[0m\n",
      "Language detection:  88%|████████▊ | 497/564 [00:11<00:01, 40.77it/s]\u001b[32m2025-02-19 08:53:17.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 497. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 498. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 499. Max Batch Input tokens size : 105\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 500. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 501. Max Batch Input tokens size : 85\u001b[0m\n",
      "Language detection:  89%|████████▉ | 502/564 [00:11<00:01, 34.80it/s]\u001b[32m2025-02-19 08:53:17.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 502. Max Batch Input tokens size : 24\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 503. Max Batch Input tokens size : 219\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 504. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 505. Max Batch Input tokens size : 23\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 506. Max Batch Input tokens size : 29\u001b[0m\n",
      "Language detection:  90%|████████▉ | 507/564 [00:11<00:01, 37.98it/s]\u001b[32m2025-02-19 08:53:17.429\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 507. Max Batch Input tokens size : 241\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 508. Max Batch Input tokens size : 16\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 509. Max Batch Input tokens size : 29\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 510. Max Batch Input tokens size : 255\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 511. Max Batch Input tokens size : 119\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 512. Max Batch Input tokens size : 49\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 513. Max Batch Input tokens size : 33\u001b[0m\n",
      "Language detection:  91%|█████████ | 514/564 [00:12<00:01, 45.20it/s]\u001b[32m2025-02-19 08:53:17.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 514. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.571\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 515. Max Batch Input tokens size : 296\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 516. Max Batch Input tokens size : 55\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 517. Max Batch Input tokens size : 17\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 518. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  92%|█████████▏| 519/564 [00:12<00:01, 38.77it/s]\u001b[32m2025-02-19 08:53:17.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 519. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.748\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 520. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 521. Max Batch Input tokens size : 34\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 522. Max Batch Input tokens size : 40\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 523. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  93%|█████████▎| 524/564 [00:12<00:01, 37.94it/s]\u001b[32m2025-02-19 08:53:17.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 524. Max Batch Input tokens size : 186\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 525. Max Batch Input tokens size : 21\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 526. Max Batch Input tokens size : 44\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.891\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 527. Max Batch Input tokens size : 148\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 528. Max Batch Input tokens size : 16\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 529. Max Batch Input tokens size : 94\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 530. Max Batch Input tokens size : 40\u001b[0m\n",
      "Language detection:  94%|█████████▍| 531/564 [00:12<00:00, 45.33it/s]\u001b[32m2025-02-19 08:53:17.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 531. Max Batch Input tokens size : 109\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:17.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 532. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 533. Max Batch Input tokens size : 37\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 534. Max Batch Input tokens size : 74\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 535. Max Batch Input tokens size : 236\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.049\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 536. Max Batch Input tokens size : 160\u001b[0m\n",
      "Language detection:  95%|█████████▌| 537/564 [00:12<00:00, 46.79it/s]\u001b[32m2025-02-19 08:53:18.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 537. Max Batch Input tokens size : 15\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 538. Max Batch Input tokens size : 72\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 539. Max Batch Input tokens size : 171\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 540. Max Batch Input tokens size : 21\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 541. Max Batch Input tokens size : 79\u001b[0m\n",
      "Language detection:  96%|█████████▌| 542/564 [00:12<00:00, 47.42it/s]\u001b[32m2025-02-19 08:53:18.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 542. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 543. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 544. Max Batch Input tokens size : 326\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 545. Max Batch Input tokens size : 27\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 546. Max Batch Input tokens size : 25\u001b[0m\n",
      "Language detection:  97%|█████████▋| 547/564 [00:12<00:00, 44.76it/s]\u001b[32m2025-02-19 08:53:18.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 547. Max Batch Input tokens size : 59\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.312\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 548. Max Batch Input tokens size : 9\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 549. Max Batch Input tokens size : 34\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 550. Max Batch Input tokens size : 89\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 551. Max Batch Input tokens size : 392\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 552. Max Batch Input tokens size : 310\u001b[0m\n",
      "Language detection:  98%|█████████▊| 553/564 [00:12<00:00, 47.61it/s]\u001b[32m2025-02-19 08:53:18.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 553. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 554. Max Batch Input tokens size : 29\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.465\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 555. Max Batch Input tokens size : 25\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 556. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 557. Max Batch Input tokens size : 512\u001b[0m\n",
      "Language detection:  99%|█████████▉| 558/564 [00:13<00:00, 42.40it/s]\u001b[32m2025-02-19 08:53:18.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 558. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 559. Max Batch Input tokens size : 70\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 560. Max Batch Input tokens size : 43\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 561. Max Batch Input tokens size : 512\u001b[0m\n",
      "\u001b[32m2025-02-19 08:53:18.693\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 562. Max Batch Input tokens size : 28\u001b[0m\n",
      "Language detection: 100%|█████████▉| 563/564 [00:13<00:00, 39.63it/s]\u001b[32m2025-02-19 08:53:18.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mlanguage_detection_pipeline\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mBatch: 563. Max Batch Input tokens size : 20\u001b[0m\n",
      "Language detection: 100%|██████████| 564/564 [00:13<00:00, 42.51it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt,padding_side=\"left\",legacy=False)\n",
    "unknown_dataset_pt = LanguageDetectorDataset(unknown_dataset)\n",
    "unknown_dataset_dataloader = LanguageDetectorDataloader(tokenizer,dataset =unknown_dataset_pt,batch_size=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt,device_map=\"auto\")\n",
    "model.eval()\n",
    "\n",
    "data_language_detection = language_detection_pipeline(model,unknown_dataset_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Let's remove the 1124 records whose language is unknown from the OG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_baseline = train_df_baseline.loc[train_df_baseline['language'] != 'unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated - smaller datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VZ0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First filter: Removing records with language determined as **unknown** from OG dataset\n",
    "\n",
    "Second filter: I'll be keeping all records from previous filter, even if language's distribution classes is not uniform.\n",
    "\n",
    "Third filter: Ensure that output class (model_a or model_b) is uniformly distributed. If not, then remove records from filter two\n",
    "until achieving this constraint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'prompt', 'response_a', 'response_b', 'winner', 'model_a',\n",
       "       'model_b', 'language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_baseline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_baseline_winner_series = train_df_baseline[\"winner\"].value_counts(normalize=True)\n",
    "df_train_df_baseline_winner_series = pd.DataFrame({\n",
    "    \"winner\" : train_df_baseline_winner_series.keys(),\n",
    "    \"proportion\" : train_df_baseline_winner_series.values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, \"Winner Model's Proportion in Training Set\")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize= (5,5))\n",
    "sns.barplot(data=df_train_df_baseline_winner_series,x=\"winner\",y=\"proportion\",ax=ax,color=magenta)\n",
    "\n",
    "\n",
    "#ax.tick_params(axis='x',labelrotation=90,labelsize=12)\n",
    "ax.set_xlabel(\"Winner model\",fontsize=10)\n",
    "ax.set_ylabel(\"Proportion\",fontsize=10)\n",
    "ax.set_title(\"Winner Model's Proportion in Training Set\",fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution appears to be almost uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_df_z0_0 = train_df_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save VZ0.0 to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(curated_df_z0_0)\n",
    "\n",
    "# Write Arrow Table to Parquet file\n",
    "filename = \"curated_small_vz0_0.parquet\"\n",
    "file = root_repo_directory + c.SLASH + \"data/datasets/curated/\" + filename\n",
    "pq.write_table(table, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VZ0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as dataset: ZV0.0 plus the following contraints:\n",
    "\n",
    "filter 1: Augment data by switching response A's with response B's, of course changing the actual winner accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_baseline_reverse = train_df_baseline.copy(deep=True)\n",
    "\n",
    "reverse_winner = {\n",
    "    \"model_a\" : 'model_b',\n",
    "    \"model_b\" : \"model_a\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_col = train_df_baseline_reverse['response_a']\n",
    "\n",
    "train_df_baseline_reverse['response_a'] = train_df_baseline_reverse['response_b']\n",
    "train_df_baseline_reverse['response_b'] = tmp_col\n",
    "\n",
    "train_df_baseline_reverse['winner'] = train_df_baseline_reverse['winner'].apply(lambda x : reverse_winner[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_df_z0_1 = pd.concat([train_df_baseline,train_df_baseline_reverse],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save VZ0.1 to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(curated_df_z0_1)\n",
    "\n",
    "# Write Arrow Table to Parquet file\n",
    "filename = \"curated_small_vz0_1.parquet\"\n",
    "file = root_repo_directory + c.SLASH + \"data/datasets/curated/\" + filename\n",
    "pq.write_table(table, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated dataset v0 - baseline\n",
    "This dataset consists of 5 (if possible) random records from each language. if not, then all the available records\n",
    "will be sampled.\n",
    "\n",
    "This baseline will be used to evaluate zero shot inference LLMs to find how well they can generalize to different\n",
    "languages, I'll load their responses in comet ML, and rank them.\n",
    "\n",
    "Results will provide a set point on fine tuning the correct (probably) LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87991/2821983180.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  curated_df_v0 = groupby_language.apply(func,**kwargs).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def func(x,**kwargs):\n",
    "\n",
    "    kwargs['generator'].shuffle(x.to_numpy(),axis= 0)\n",
    "\n",
    "    if x.shape[0] >= kwargs['num_group_records']:\n",
    "        return x.iloc[:kwargs['num_group_records']]\n",
    "    return x\n",
    "\n",
    "\n",
    "num_group_records = 5\n",
    "generator = np.random.default_rng(seed=142)\n",
    "kwargs = {\"num_group_records\" : num_group_records, 'generator' : generator}\n",
    "\n",
    "\n",
    "\n",
    "curated_df_v0 = groupby_language.apply(func,**kwargs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118c074e1e3880f61f1ecc82b392e6ba250b4cd3bb383f...</td>\n",
       "      <td>Макроэкономика</td>\n",
       "      <td>## Макроэкономика: Обзор\\n\\nМакроэкономика - э...</td>\n",
       "      <td>Макроэкономика — это раздел экономической наук...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemini-1.5-pro-001</td>\n",
       "      <td>gpt-4o-2024-05-13</td>\n",
       "      <td>Abkhazian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49aa24778c38c727c3665dd418632176e523999397dc50...</td>\n",
       "      <td>devala maharshi</td>\n",
       "      <td>Devala Maharshi is a revered figure in Hinduis...</td>\n",
       "      <td>Devala Maharshi is a revered sage in Hindu myt...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>deepseek-v2.5</td>\n",
       "      <td>qwen-plus-0828</td>\n",
       "      <td>Afar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5fcf043fe50553461dcd3998c4b0a0d373d404291598ee...</td>\n",
       "      <td>Wat is de betekenis van het lied aramsamsam ?</td>\n",
       "      <td>Een interessante vraag!\\n\\n\"Aramsamsam\" (ook w...</td>\n",
       "      <td>Ik kan geen informatie vinden over een lied ge...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>llama-3.1-nemotron-70b-instruct</td>\n",
       "      <td>llama-3.1-405b-instruct-fp8</td>\n",
       "      <td>Afrikaans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>632f631eceb574551ef09a497f77405cbf146fc2e49666...</td>\n",
       "      <td>Vertel me een goede mop</td>\n",
       "      <td>Natuurlijk, hier is een klassieke mop voor je:...</td>\n",
       "      <td>Natuurlijk! Here is a classic Dutch joke for y...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>qwen-plus-0828</td>\n",
       "      <td>deepseek-v2.5</td>\n",
       "      <td>Afrikaans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c35fbab864975e41640e0e384e56c57ecc0c6bb2e4254c...</td>\n",
       "      <td>wat is de populairste bezienswaardigheid van D...</td>\n",
       "      <td>De populairste bezienswaardigheid van Duitslan...</td>\n",
       "      <td>De populairste bezienswaardigheid van Duitslan...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>gemini-1.5-pro-002</td>\n",
       "      <td>glm-4-plus</td>\n",
       "      <td>Afrikaans</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  118c074e1e3880f61f1ecc82b392e6ba250b4cd3bb383f...   \n",
       "1  49aa24778c38c727c3665dd418632176e523999397dc50...   \n",
       "2  5fcf043fe50553461dcd3998c4b0a0d373d404291598ee...   \n",
       "3  632f631eceb574551ef09a497f77405cbf146fc2e49666...   \n",
       "4  c35fbab864975e41640e0e384e56c57ecc0c6bb2e4254c...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0                                     Макроэкономика   \n",
       "1                                    devala maharshi   \n",
       "2      Wat is de betekenis van het lied aramsamsam ?   \n",
       "3                            Vertel me een goede mop   \n",
       "4  wat is de populairste bezienswaardigheid van D...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  ## Макроэкономика: Обзор\\n\\nМакроэкономика - э...   \n",
       "1  Devala Maharshi is a revered figure in Hinduis...   \n",
       "2  Een interessante vraag!\\n\\n\"Aramsamsam\" (ook w...   \n",
       "3  Natuurlijk, hier is een klassieke mop voor je:...   \n",
       "4  De populairste bezienswaardigheid van Duitslan...   \n",
       "\n",
       "                                          response_b   winner  \\\n",
       "0  Макроэкономика — это раздел экономической наук...  model_a   \n",
       "1  Devala Maharshi is a revered sage in Hindu myt...  model_a   \n",
       "2  Ik kan geen informatie vinden over een lied ge...  model_a   \n",
       "3  Natuurlijk! Here is a classic Dutch joke for y...  model_a   \n",
       "4  De populairste bezienswaardigheid van Duitslan...  model_a   \n",
       "\n",
       "                           model_a                      model_b   language  \n",
       "0               gemini-1.5-pro-001            gpt-4o-2024-05-13  Abkhazian  \n",
       "1                    deepseek-v2.5               qwen-plus-0828       Afar  \n",
       "2  llama-3.1-nemotron-70b-instruct  llama-3.1-405b-instruct-fp8  Afrikaans  \n",
       "3                   qwen-plus-0828                deepseek-v2.5  Afrikaans  \n",
       "4               gemini-1.5-pro-002                   glm-4-plus  Afrikaans  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_df_v0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_df_v0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save V0 to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root: str = pathlib.Path().resolve().parent.parent.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kevinmg96/Kaggle competitions/WSDM Cup/multilingual-chatbot-arena/datasets_creator'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_repo_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(root + \"/validation/validation_data_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.iloc[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "table = pa.Table.from_pandas(curated_df_v0)\n",
    "\n",
    "# Write Arrow Table to Parquet file\n",
    "filename = \"curated_small_v0.parquet\"\n",
    "file = root_repo_directory + c.SLASH + \"data/datasets/curated/\" + filename\n",
    "pq.write_table(table, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curated_df_v0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated dataset v1 - Random and  uniform\n",
    "Languages: English, Russian, Chinese, Vietnamese, German, Japanese, Unknown, Korean, Spanish\n",
    "French, Portuguese, Persian, Italian, Turkish,Czech, Arabic, Polish , Ukrainian , Dutch, Hungarian\n",
    "\n",
    "\n",
    "\n",
    "100 samples per language\n",
    "\n",
    "Total samples: 2,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated dataset v2 - Random and not uniform\n",
    "\n",
    "English, Russian, Chinese, Vietnamese, German, Japanese, Unknown, Korean :  1,000 samples\n",
    "Spanish, French, Portuguese : 500 samples\n",
    "Persian, Italian, Turkish : 200 samples\n",
    "Czech, Arabic, Polish , Ukrainian , Dutch, Hungarian : 100 samples\n",
    "\n",
    "Total samples: 8 * 1,000 + 3 * 500 + 3 * 200 + 6 * 100 = 10,700 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated dataset v3 - Random,uniform, large multilanguages\n",
    "Languages: English, Russian, Chinese, Vietnamese, German, Japanese, Unknown, Korean, Spanish\n",
    "French, Portuguese, Persian, Italian, Turkish,Czech, Arabic, Polish , Ukrainian , Dutch, Hungarian\n",
    "\n",
    "\n",
    "100 samples per language (above)\n",
    "\n",
    "10 samples per language (below)\n",
    "\n",
    "Indonesian, Slovak, Swedish, Danish, Finnish, Serbian, Bulgarian, Hebrew, Thai, Romanian, Latin,Galician,\n",
    "Norwegian, Catalan, Greek, Lithuanian, Croatian, Sanskrit, Estonian, Scots, Basque       66\n",
    "\n",
    "\n",
    "Total samples: 20 * 100 + 21 * 10 = 2,210 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated dataset v4 - Random,not uniform, large multilanguages\n",
    "Mix between v2 and v3.\n",
    "\n",
    "respect sampling for the languages composed in V2 and include the specified samples from v3 to the remainiding \n",
    "languages. i,e. up to Hungarian is v2, and from Indonesian and so on is v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated dataset v5 - Random, not uniform and English biased\n",
    "\n",
    "Same as v2 except that English samples will be 10k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated dataset v6 - Random,uniform, large multilanguages\n",
    "\n",
    "Same as v3 except that English samples will be 1k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated dataset v7 - Random, not uniform and English biased\n",
    "\n",
    "Same as v2 except that English samples will be 2k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English, Russian, Chinese, Vietnamese, German, Japanese, Unknown, Korean :  1,000 samples\n",
    "Spanish, French, Portuguese : 500 samples\n",
    "Persian, Italian, Turkish : 200 samples\n",
    "Czech, Arabic, Polish , Ukrainian , Dutch, Hungarian : 100 samples\n",
    "\n",
    "Total samples: 8 * 1,000 + 3 * 500 + 3 * 200 + 6 * 100 = 10,700 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x,**kwargs):\n",
    "\n",
    "    kwargs['generator'].shuffle(x.to_numpy(),axis= 0)\n",
    "\n",
    "    main_group = [\"Russian\", \"Chinese\", \"Vietnamese\", \"German\", \"Japanese\",  \"Korean\"]\n",
    "    second_group = [\"Spanish\", \"French\", \"Portuguese\"]\n",
    "    third_group = [\"Czech\", \"Arabic\", \"Polish\" , \"Ukrainian\" , \"Dutch\", \"Hungarian\"]\n",
    "\n",
    "    if x.name == \"English\":\n",
    "        return x.iloc[:kwargs[\"english_records\"]]\n",
    "    elif x.name in main_group:\n",
    "        return x.iloc[:1000]\n",
    "    elif x.name in second_group:\n",
    "        return x.iloc[:500]\n",
    "    elif x.name in third_group:\n",
    "        return x.iloc[:100]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_v5 = np.random.default_rng(seed=142)\n",
    "kwargs = {'generator' : generator_v5,\"english_records\" : 10000}\n",
    "\n",
    "\n",
    "\n",
    "curated_df_v5 = groupby_language.apply(func,**kwargs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save V5 to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table = pa.Table.from_pandas(curated_df_v5)\n",
    "\n",
    "# Write Arrow Table to Parquet file\n",
    "filename = \"curated_small_v5.parquet\"\n",
    "file = root_repo_directory + c.SLASH + \"data/datasets/curated/\" + filename\n",
    "pq.write_table(table, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "generator_v7 = np.random.default_rng(seed=142)\n",
    "kwargs = {'generator' : generator_v7, \"english_records\" : 2000}\n",
    "\n",
    "\n",
    "\n",
    "curated_df_v7 = groupby_language.apply(func,**kwargs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save V7 to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table = pa.Table.from_pandas(curated_df_v7)\n",
    "\n",
    "# Write Arrow Table to Parquet file\n",
    "filename = \"curated_small_v7.parquet\"\n",
    "file = root_repo_directory + c.SLASH + \"data/datasets/curated/\" + filename\n",
    "pq.write_table(table, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curated dataset v8 - Random, uniform, filter by token size\n",
    "\n",
    "First filter: Conserve records whose prompt has less than 3.5k tokens\n",
    "\n",
    "Second filter: \n",
    "Languages: English, Russian, Chinese, Vietnamese, German, Japanese, Unknown, Korean, Spanish\n",
    "French, Portuguese, Persian, Italian, Turkish,Czech, Arabic, Polish , Ukrainian , Dutch, Hungarian\n",
    "\n",
    "100 samples per language (if possible)\n",
    "\n",
    "Third filter:\n",
    "Ensure that output class (model_a or model_b) is uniformly distributed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_TEMPLATE = 'You are a specialist in evaluating multilingual chat responses, with a focus on comparing and ranking outputs from different LLMs. Your primary goal is to determine which response is more likely to be preferred by humans based on factors such as clarity, relevance, tone, and overall quality.\\n'\n",
    "PROMPT_TEMPLATE = \"\"\"Below is a prompt with two possible responses (**Response A** and **Response B**). Evaluate them, select the best one and answer in the following format (it is imperative that you respect the specified format, do not add any more text than what I ask for):\\n1.- Write 'model_a' if the **Response A** is better than **Response B**, otherwise write 'model_b'.\\n\\n**Prompt**:\\n{prompt}\\n\\n**Response A**:\\n{response_a}\\n\\n**Response B**:\\n{response_b}\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "model_name,padding_side=\"left\",legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prompt_num_tokens(x,**kwargs):\n",
    "    tokenizer = kwargs[\"tokenizer\"]\n",
    "\n",
    "    messages = [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_TEMPLATE},\n",
    "                    {\"role\" : \"user\", \"content\" : PROMPT_TEMPLATE.format(\n",
    "                    prompt=x.prompt,response_a=x.response_a,response_b = x.response_b)},\n",
    "                    {\"role\" : \"assistant\", \"content\" : x.winner}\n",
    "                ]\n",
    "    return len(tokenizer.apply_chat_template(messages,add_generation_prompt=True,tokenize=True))\n",
    "\n",
    "kwargs = {\n",
    "    \"tokenizer\" :tokenizer\n",
    "}\n",
    "train_df[\"num_tokens\"] = train_df.apply(add_prompt_num_tokens,axis=1,**kwargs)\n",
    "\n",
    "train_df[\"num_tokens_categories\"] = pd.cut(train_df[\"num_tokens\"],10,labels=np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens_subset =train_df.query('num_tokens_categories == 0')\n",
    "group_num_tokens_subset =num_tokens_subset.groupby(\"language\")\n",
    "\n",
    "def func(x,**kwargs):\n",
    "\n",
    "    kwargs['generator'].shuffle(x.to_numpy(),axis= 0)\n",
    "\n",
    "\n",
    "    group = [\"English\",\"Russian\", \"Chinese\", \"Vietnamese\", \"German\", \"Japanese\",  \"Korean\",\n",
    "                  \"Spanish\", \"French\", \"Portuguese\",\"Persian\", \"Italian\", \"Turkish\",\n",
    "                    \"Czech\", \"Arabic\", \"Polish\" , \"Ukrainian\" , \"Dutch\", \"Hungarian\"]\n",
    "\n",
    "\n",
    "    if x.name in group:\n",
    "        return x.iloc[:kwargs[\"num_records\"]]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "num_records = 100\n",
    "generator = np.random.default_rng(seed=142)\n",
    "kwargs = {\"num_records\" : num_records, 'generator' : generator}\n",
    "\n",
    "\n",
    "curated_df_v8 = group_num_tokens_subset.apply(func,**kwargs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assess output class distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_v8_winners = curated_df_v8.groupby([\"winner\"]).count()[\"id\"]\n",
    "\n",
    "df_series_v8_winners = pd.DataFrame({\n",
    "    \"winners\" : series_v8_winners.index.values,\n",
    "    \"percentages\" : (series_v8_winners.values / curated_df_v8.shape[0]) * 100\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize= (5,5))\n",
    "sns.barplot(data=df_series_v8_winners,x=\"winners\",y=\"percentages\",ax=ax,color=grey)\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Model Winner\",fontsize=10)\n",
    "ax.set_ylabel(\"Percentage (%)\",fontsize=10)\n",
    "ax.set_title(\"Model Winner's distribution for dataset v8\",fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Distribution is almost uniform, continue with the following process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_df_v8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save V8 to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(curated_df_v8)\n",
    "\n",
    "# Write Arrow Table to Parquet file\n",
    "filename = \"curated_small_v8.parquet\"\n",
    "file = root_repo_directory + c.SLASH + \"data/datasets/curated/\" + filename\n",
    "pq.write_table(table, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsdm-cup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
