{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created to test how current open source state-of-the-art LLMs perform for the multilingual-chatbot-arena dataset.\n",
    "\n",
    "The experiment's constraints are as follows:\n",
    "\n",
    "1. Model's inference will be held using an NVIDIA GeForce RTX 4060. Therefore the GPU computing is limited. Recall that is most likely that the current pretrained models with not perform very well on this challenge's data. The benchmarked LLMs must hold around 7-9B parameters in order for the hardware to handle inferencing/fine tuning.\n",
    "2. For the fine tuning of LLMs. The best course of action will be to use QLORA. Due to hardware constraints.\n",
    "3. There are an abundant number of capable open-source LLMs. In this Demo we will be benchmarking 3 model's families: Qwen2.5, Llama 3.X and gemini.\n",
    "4. Dataset for the experiment: training set.\n",
    "5. Performance metric: Accuracy (For the whole dataset, how many prompts what's the proportion of accurately predicted answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading challenge's training data from Comet ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinmg96/miniconda3/envs/multilingual-chatbot-arena/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "\u001b[32m2025-01-14 10:39:00.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mInitializing env vars...\u001b[0m\n",
      "\u001b[32m2025-01-14 10:39:00.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLoading environment variables from: /home/kevinmg96/Kaggle competitions/WSDM Cup/multilingual-chatbot-arena/.env\u001b[0m\n",
      "/home/kevinmg96/miniconda3/envs/multilingual-chatbot-arena/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "root_repo_directory = pathlib.Path().resolve().parent.__str__()\n",
    "sys.path.append(root_repo_directory)\n",
    "from multilingual_chatbot_arena import initialize\n",
    "import datasets_creator.src.constants as c\n",
    "import datasets_creator.src.utils as utils\n",
    "import pandas as pd\n",
    "from fire import Fire\n",
    "from pydantic import BaseModel\n",
    "from typing import List,Optional,Dict,Union\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "\n",
    "import os\n",
    "import opik\n",
    "from loguru import logger\n",
    "initialize()\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting challenge's train dataset\n",
    "client = opik.Opik(workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])\n",
    "dataset_comet = client.get_or_create_dataset(\"multilingual-chatbot-arena-train-1\")\n",
    "data = dataset_comet.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>language</th>\n",
       "      <th>prompt</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>German</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b29-7e5e-7b26-a627-7f8d4db7f396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b29-7e5d-772c-998b-5f8a8413153c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>Czech</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b29-7e5c-71e1-9d4e-a9a503529dcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>English</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b29-7e5b-7b26-9eee-98c6dfc31878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best model is model_a based on its human prefe...</td>\n",
       "      <td>English</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b29-7e5a-75b4-8ec8-7c026e3e0afe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer language  \\\n",
       "0  Best model is model_b based on its human prefe...   German   \n",
       "1  Best model is model_b based on its human prefe...  Russian   \n",
       "2  Best model is model_b based on its human prefe...    Czech   \n",
       "3  Best model is model_b based on its human prefe...  English   \n",
       "4  Best model is model_a based on its human prefe...  English   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  \\nYou are an expert in assesing LLM's model re...   \n",
       "1  \\nYou are an expert in assesing LLM's model re...   \n",
       "2  \\nYou are an expert in assesing LLM's model re...   \n",
       "3  \\nYou are an expert in assesing LLM's model re...   \n",
       "4  \\nYou are an expert in assesing LLM's model re...   \n",
       "\n",
       "                                     id  \n",
       "0  01945b29-7e5e-7b26-a627-7f8d4db7f396  \n",
       "1  01945b29-7e5d-772c-998b-5f8a8413153c  \n",
       "2  01945b29-7e5c-71e1-9d4e-a9a503529dcd  \n",
       "3  01945b29-7e5b-7b26-9eee-98c6dfc31878  \n",
       "4  01945b29-7e5a-75b4-8ec8-7c026e3e0afe  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove from prompt column, system's message declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_substring = \"\"\"\\nYou are an expert in assesing LLM's model response based on a prompt. I will give you an input prompt (**prompt**) with two different responses coming from fellow LLM models; the first model's response is called **response_a** and second model's response is **response_b**. You can find the previous information after the double slashes (//), respecting the correct title based on the proper input.Your task is to assess the content of each response based on its quality and human's language similarity, then choose the model's response which adheres best to the given guidelines.\\nYour response must obey the following format: \"Best model is model_[] based on its human preferability response for the input prompt.\". You will substitute \"[]\" with either \"a\" if you think **response_a** is better than **response_b**, or \"b\" otherwise.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_system_sentence(x):\n",
    "    prompt = x.prompt\n",
    "\n",
    "    return prompt.split(system_substring)[-1]\n",
    "\n",
    "data[\"prompt\"] = data.apply(del_system_sentence,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>language</th>\n",
       "      <th>prompt</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>German</td>\n",
       "      <td>\\n\\n//\\n**prompt**:\\nSchreibe bitte ein Anschr...</td>\n",
       "      <td>01945b29-7e5e-7b26-a627-7f8d4db7f396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>\\n\\n//\\n**prompt**:\\nСделай рерайт предложения...</td>\n",
       "      <td>01945b29-7e5d-772c-998b-5f8a8413153c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>Czech</td>\n",
       "      <td>\\n\\n//\\n**prompt**:\\nVytvoř 4x2 tabulku. V prv...</td>\n",
       "      <td>01945b29-7e5c-71e1-9d4e-a9a503529dcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>English</td>\n",
       "      <td>\\n\\n//\\n**prompt**:\\nПереведи текст на русския...</td>\n",
       "      <td>01945b29-7e5b-7b26-9eee-98c6dfc31878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best model is model_a based on its human prefe...</td>\n",
       "      <td>English</td>\n",
       "      <td>\\n\\n//\\n**prompt**:\\n# Setting\\n\\nAll eyes are...</td>\n",
       "      <td>01945b29-7e5a-75b4-8ec8-7c026e3e0afe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer language  \\\n",
       "0  Best model is model_b based on its human prefe...   German   \n",
       "1  Best model is model_b based on its human prefe...  Russian   \n",
       "2  Best model is model_b based on its human prefe...    Czech   \n",
       "3  Best model is model_b based on its human prefe...  English   \n",
       "4  Best model is model_a based on its human prefe...  English   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  \\n\\n//\\n**prompt**:\\nSchreibe bitte ein Anschr...   \n",
       "1  \\n\\n//\\n**prompt**:\\nСделай рерайт предложения...   \n",
       "2  \\n\\n//\\n**prompt**:\\nVytvoř 4x2 tabulku. V prv...   \n",
       "3  \\n\\n//\\n**prompt**:\\nПереведи текст на русския...   \n",
       "4  \\n\\n//\\n**prompt**:\\n# Setting\\n\\nAll eyes are...   \n",
       "\n",
       "                                     id  \n",
       "0  01945b29-7e5e-7b26-a627-7f8d4db7f396  \n",
       "1  01945b29-7e5d-772c-998b-5f8a8413153c  \n",
       "2  01945b29-7e5c-71e1-9d4e-a9a503529dcd  \n",
       "3  01945b29-7e5b-7b26-9eee-98c6dfc31878  \n",
       "4  01945b29-7e5a-75b4-8ec8-7c026e3e0afe  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom dataset for the imported data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self,data : pd.DataFrame):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get either a single data point or a pandas Dataframe window of data points\n",
    "        data_window = self.data.iloc[idx]    \n",
    "\n",
    "        return data_window.to_dict()\n",
    "    \n",
    "train_dataset = ChatbotDataset(data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing **ChatbotDataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find dataset's max token length (Deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_point_max_len = 0\n",
    "for dic in train_dataset:\n",
    "    if dic[\"inputs\"][\"input_ids\"].squeeze().shape[0] > data_point_max_len:\n",
    "        data_point_max_len =  dic[\"inputs\"][\"input_ids\"].squeeze().shape[0]\n",
    "\n",
    "print(f\"Data point's maximum length after tokenization : {data_point_max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ChatbotDataloader(DataLoader):\n",
    "    def __init__(self, tokenizer :  PreTrainedTokenizer | PreTrainedTokenizerFast, **kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        kwargs[\"collate_fn\"] = self.chatbot_collate\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    \n",
    "    def chatbot_collate(self,batch):\n",
    "        \"\"\"Custom collate function to teach the Dataloader class how to parse the batches into an llm friendly format\n",
    "        Args:\n",
    "            original_batch : List of batch elements with len -> batch_size. Each list's element strictly follows \n",
    "            the format inside __getitem__ from Dataset class. \n",
    "        \n",
    "        \"\"\"\n",
    "        prompts = []\n",
    "        answers = []\n",
    "        languages = []\n",
    "        for dic in batch:\n",
    "            prompt_messages = [\n",
    "                {\"role\": \"system\", \"content\": system_substring},\n",
    "                {\"role\" : \"user\", \"content\" : dic[\"prompt\"]}\n",
    "            ]\n",
    "\n",
    "            prompt_text  = self.tokenizer.apply_chat_template(\n",
    "                prompt_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "\n",
    "            \"\"\" answer_messages = [\n",
    "                {\"role\" : \"user\", \"content\" : dic[\"answer\"]}\n",
    "            ]\n",
    "\n",
    "            answer_text  = self.tokenizer.apply_chat_template(\n",
    "                answer_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            ) \"\"\"\n",
    "\n",
    "\n",
    "            \n",
    "            prompts.append(prompt_text)\n",
    "            #answers.append(answer_text)\n",
    "            answers.append(dic[\"answer\"])\n",
    "            languages.append(dic[\"language\"])\n",
    "\n",
    "        #tokenize batch of prompts and answers\n",
    "        prompt_tokenize = self.tokenizer(prompts,\n",
    "                padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "        \"\"\" answer_tokenize = self.tokenizer(answers,\n",
    "                padding='longest',truncation=True,return_tensors=\"pt\") \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"inputs\" : prompt_tokenize, #Dict[str,torch.Tensor]\n",
    "            \"labels\" : answers, #List[str]  ##answer_tokenize, #Dict[str,torch.Tensor],\n",
    "            \"languages\" : languages, #List[str]\n",
    "            \"longest_seq\" : prompt_tokenize[\"input_ids\"].shape[1] #int\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section model inferencing pipelines for each of the benchmark models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accurracy_metric(predictions,labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    unmatched_idxs = [] #incorrectly predicted records idxs\n",
    "    accurracy = 0\n",
    "    for i,(pred,lab) in enumerate(zip(predictions,labels)):\n",
    "        if pred == lab:\n",
    "            accurracy += 1\n",
    "        else:\n",
    "            unmatched_idxs.append(i)\n",
    "\n",
    "    return accurracy / len(predictions), unmatched_idxs\n",
    "\n",
    "\n",
    "def get_model_winner(matches) -> str:\n",
    "    \"\"\"\n",
    "    Extract which model's reponse is better from input_response\n",
    "    \"\"\"\n",
    "    for match in matches:\n",
    "        if 'a' in match:\n",
    "            return 'a'\n",
    "        elif 'b' in match:\n",
    "            return 'b'\n",
    "    return 'c'\n",
    "\n",
    "def parse_output_llm(responses) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves a list specifying which of the two paired models in each training record adheres the best\n",
    "    to human responses.\n",
    "\n",
    "    Args:\n",
    "        responses List[str]: Batch of LLM's responses.\n",
    "    \"\"\"\n",
    "    pattern = r'Best model(.*?)based on its human preferability response'\n",
    "\n",
    "    model_winner_in_responses = []\n",
    "    for response in responses:\n",
    "        #Extract pattern from response\n",
    "        matches = re.findall(pattern, response,re.DOTALL)        \n",
    "        model_winner_in_responses.append(get_model_winner(matches))\n",
    "    \n",
    "    return model_winner_in_responses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_inference(model,train_dataloader) -> tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Retrieves two lists, the first list specifies the LLM's decisions per record, on which response was more humanly\n",
    "    seen. The other specifies the challenge's ground truth.\n",
    "\n",
    "    Args:\n",
    "        model : HuggingFace Pretrained LLM.\n",
    "    \"\"\"\n",
    "    global_output_winners = []\n",
    "    global_label_winners = []\n",
    "    for batch in tqdm(train_dataloader,desc=\"Training set - Model Inference\"):\n",
    "        # Let's send current batch into device: 'auto'\n",
    "        inputs,labels = batch[\"inputs\"].to(model.device),batch[\"labels\"]\n",
    "        \n",
    "        #forward batch of input tokens into the model, get output token ids\n",
    "        output_token_ids  = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "        )\n",
    "\n",
    "        output_token_ids = output_token_ids.detach().cpu()\n",
    "\n",
    "        #Remove prompt from generated response\n",
    "        output_token_ids = [output_token_ids[i,batch[\"longest_seq\"]:]  for i in range(\n",
    "            output_token_ids.shape[0])]\n",
    "\n",
    "        #Decode batch's output\n",
    "        batch_output_decoded = train_dataloader.tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "        #Parse batch's decoded responses to extract, for each model-pair in input, which one is predicted to \n",
    "        #have better response\n",
    "        batch_output_winners = parse_output_llm(batch_output_decoded)\n",
    "\n",
    "        #Decode batch's labels\n",
    "        #batch_label_decoded = train_dataloader.tokenizer.batch_decode(labels[\"input_ids\"], skip_special_tokens=True)\n",
    "        batch_label_winners = parse_output_llm(labels)#parse_output_llm(batch_label_decoded)\n",
    "\n",
    "        #store batches into dataset\n",
    "        global_output_winners.extend(batch_output_winners)\n",
    "        global_label_winners.extend(batch_label_winners)\n",
    "\n",
    "        #clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return global_output_winners,global_label_winners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Inference Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceArgs:\n",
    "    model_name : str\n",
    "    batch_size : int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\",\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(config.model_name,\n",
    "                                                                                         padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4 max context length : 131072\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model : {config.model_name} max context length : {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"dataset\" :train_dataset,\n",
    "    \"batch_size\" : config.batch_size\n",
    "}\n",
    "\n",
    "qwen_2_5_3b_instruct_4int_dataloader = ChatbotDataloader(\n",
    "    tokenizer,**arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        [151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        [151643, 151643, 151643,  ..., 151644,  77091,    198],\n",
      "        [151644,   8948,    271,  ..., 151644,  77091,    198]])\n",
      "torch.Size([4, 3746])\n",
      "['Best model is model_b based on its human preferability response for the input prompt.', 'Best model is model_b based on its human preferability response for the input prompt.', 'Best model is model_b based on its human preferability response for the input prompt.', 'Best model is model_b based on its human preferability response for the input prompt.']\n",
      "['German', 'Russian', 'Czech', 'English']\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(qwen_2_5_3b_instruct_4int_dataloader)\n",
    "dic = next(iterator)\n",
    "print(dic[\"inputs\"][\"input_ids\"])\n",
    "print(dic[\"inputs\"][\"input_ids\"].shape)\n",
    "print(dic[\"labels\"])\n",
    "print(dic[\"languages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model into VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinmg96/miniconda3/envs/multilingual-chatbot-arena/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/kevinmg96/miniconda3/envs/multilingual-chatbot-arena/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/kevinmg96/miniconda3/envs/multilingual-chatbot-arena/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (k_proj): QuantLinear()\n",
       "          (o_proj): QuantLinear()\n",
       "          (q_proj): QuantLinear()\n",
       "          (v_proj): QuantLinear()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (act_fn): SiLU()\n",
       "          (down_proj): QuantLinear()\n",
       "          (gate_proj): QuantLinear()\n",
       "          (up_proj): QuantLinear()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model's text generation forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a', 'a', 'a']\n",
      "['b', 'b', 'b', 'b']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    my_batch = next(iter(qwen_2_5_3b_instruct_4int_dataloader))\n",
    "    my_batch_input = my_batch[\"inputs\"].to(\"cuda\")\n",
    "\n",
    "    #logits = model(**my_batch_input).logits\n",
    "\n",
    "    output_ids  = model.generate(\n",
    "        **my_batch_input,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    output_ids = output_ids.detach().cpu()\n",
    "\n",
    "    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    print(parse_output_llm(response))\n",
    "\n",
    "    ground_truth_batch = tokenizer.batch_decode(my_batch[\"labels\"][\"input_ids\"], skip_special_tokens=True)\n",
    "    print(parse_output_llm(ground_truth_batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing dataset to Model: Qwen2.5-3B-Int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training set - Model Inference:   0%|          | 15/5147 [02:24<9:19:36,  6.54s/it] "
     ]
    }
   ],
   "source": [
    "output_winners, label_winners = model_inference(model,qwen_2_5_3b_instruct_4int_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_accurracy_metric(output_winners,label_winners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8\",\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(config.model_name,\n",
    "                                                                                         padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model : {config.model_name} max context length : {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"dataset\" :train_dataset,\n",
    "    \"batch_size\" : config.batch_size\n",
    "}\n",
    "\n",
    "llama_3_2_1b_spinquant_4int_dataloader = ChatbotDataloader(\n",
    "    tokenizer,**arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model into VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    my_batch = next(iter(llama_3_2_1b_spinquant_4int_dataloader))\n",
    "    my_batch_input = my_batch[\"inputs\"].to(\"cuda\")\n",
    "\n",
    "    #logits = model(**my_batch_input).logits\n",
    "\n",
    "    output_ids  = model.generate(\n",
    "        **my_batch_input,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    output_ids = output_ids.detach().cpu()\n",
    "\n",
    "    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    print(parse_output_llm(response))\n",
    "\n",
    "    ground_truth_batch = tokenizer.batch_decode(my_batch[\"labels\"][\"input_ids\"], skip_special_tokens=True)\n",
    "    print(parse_output_llm(ground_truth_batch))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-chatbot-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
