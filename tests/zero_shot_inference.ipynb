{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created to test how current open source state-of-the-art LLMs perform for the multilingual-chatbot-arena dataset.\n",
    "\n",
    "The experiment's constraints are as follows:\n",
    "\n",
    "1. Model's inference will be held using an NVIDIA GeForce RTX 4060. Therefore the GPU computing is limited. Recall that is most likely that the current pretrained models with not perform very well on this challenge's data. The benchmarked LLMs must hold around 7-9B parameters in order for the hardware to handle inferencing/fine tuning.\n",
    "2. For the fine tuning of LLMs. The best course of action will be to use QLORA. Due to hardware constraints.\n",
    "3. There are an abundant number of capable open-source LLMs. In this Demo we will be benchmarking 3 model's families: Qwen2.5, Llama 3.X and gemini.\n",
    "4. Dataset for the experiment: training set.\n",
    "5. Performance metric: Accuracy (For the whole dataset, how many prompts what's the proportion of accurately predicted answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-19 11:43:46.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mInitializing env vars...\u001b[0m\n",
      "\u001b[32m2025-02-19 11:43:46.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLoading environment variables from: /home/kevinmg96/Kaggle competitions/WSDM Cup/multilingual-chatbot-arena/.env\u001b[0m\n",
      "/home/kevinmg96/miniconda3/envs/wsdm-cup/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "root_repo_directory = pathlib.Path().resolve().parent.__str__()\n",
    "sys.path.append(root_repo_directory)\n",
    "from multilingual_chatbot_arena import initialize\n",
    "import datasets_creator.src.constants as c\n",
    "import datasets_creator.src.utils as utils\n",
    "import pandas as pd\n",
    "from fire import Fire\n",
    "from pydantic import BaseModel\n",
    "from typing import List,Optional,Dict,Union,Any\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import opik\n",
    "from loguru import logger\n",
    "initialize()\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig, BitsAndBytesConfig\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colors\n",
    "black = mcolors.CSS4_COLORS[\"black\"]\n",
    "lime = mcolors.CSS4_COLORS[\"lime\"]\n",
    "aqua = mcolors.CSS4_COLORS[\"aqua\"]\n",
    "magenta = mcolors.CSS4_COLORS[\"magenta\"]\n",
    "red = mcolors.CSS4_COLORS[\"red\"]\n",
    "grey = mcolors.CSS4_COLORS[\"grey\"]\n",
    "orange = mcolors.CSS4_COLORS[\"orangered\"]\n",
    "gold = mcolors.CSS4_COLORS[\"gold\"]\n",
    "blue = mcolors.CSS4_COLORS[\"blue\"]\n",
    "indigo = mcolors.CSS4_COLORS[\"indigo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting configuration arguments for the whole script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vals_datatype = Union[list[int],int]\n",
    "@dataclass\n",
    "class InferenceArgs:\n",
    "    comet_dataset_name : str\n",
    "    comet_dataset_description : str\n",
    "    comet_prompt_template_name : str\n",
    "    comet_prompt_version : str\n",
    "    model_name : str\n",
    "    max_new_tokens : int\n",
    "    batch_size : Optional[int] = None\n",
    "    cache : Optional[str] = None\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceFile:\n",
    "\n",
    "    data : list[dict[str,dict_vals_datatype]]\n",
    "    inference_dataset : str\n",
    "\n",
    "    def save(self,file_path, file_name,include_version = False):\n",
    "        utils.to_pickle(self.data,file_path,file_name,include_version)\n",
    "\n",
    "\n",
    "config = InferenceArgs(\n",
    "    comet_dataset_name=\"multilingual-chatbot-arena-v0.2.1-train\",\n",
    "    comet_dataset_description=\"Challenge: WSDM CUP. Curated-smal-dataset - version0.2.1 - Training set 1.\",\n",
    "    comet_prompt_template_name = 'Prompt_template_wsdm_cup_1',\n",
    "    comet_prompt_version = \"v2.1.0\",\n",
    "    model_name=\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\",\n",
    "    cache=\"quantized KV cache: quanto\",\n",
    "    max_new_tokens= 2500,\n",
    "    batch_size=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset from Comet ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name=os.environ['COMET_PROJECT_NAME'],\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NAME = \"Prompt_template_wsdm_cup_1\"\n",
    "comet_prompt = opik_client.get_prompt(PROMPT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset's Classes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_winner_series = df[\"answer\"].value_counts()\n",
    "df_winner_series = pd.DataFrame({\n",
    "    \"answer\" : df_winner_series.keys(),\n",
    "    \"proportion\" : df_winner_series.values / df.shape[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize= (5,5))\n",
    "sns.barplot(data=df_winner_series,x=\"answer\",y=\"proportion\",ax=ax,color=magenta)\n",
    "\n",
    "\n",
    "#ax.tick_params(axis='x',labelrotation=90,labelsize=12)\n",
    "ax.set_xlabel(\"Winner model\",fontsize=10)\n",
    "ax.set_ylabel(\"Proportion\",fontsize=10)\n",
    "ax.set_title(\"Winner Model's Proportion in Training Set\",fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_percentage = ((df_winner_series['proportion'][0] - df_winner_series['proportion'][1]) / df_winner_series['proportion'][1]) * 100\n",
    "\n",
    "print(f\"Difference in classes distribution : {diff_percentage} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client.search_traces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a trace\n",
    "trace = opik_client.trace(\n",
    "    name=\"my_trace-2\",\n",
    "    input=\"Hello, how are you?\",\n",
    "    output={\"response\": \"Comment ça va?\"}\n",
    ")\n",
    "\n",
    "opik_client.log_traces_feedback_scores(\n",
    "    scores=[\n",
    "        {\"id\": trace.id, \"name\": \"overall_quality\", \"value\": 0.85, \"reason\": \"The response was helpful and accurate.\"},\n",
    "        {\"id\": trace.id, \"name\": \"coherence\", \"value\": 0.75},\n",
    "        {\"id\" : trace.id, \"name\" : \"correctness\", \"value\" : 1.0}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Add a span\n",
    "trace.span(\n",
    "    name=\"Add prompt template\",\n",
    "    input={\"text\": \"Hello, how are you?\", \"prompt_template\": \"Translate the following text to French: {text}\"},\n",
    "    output={\"text\": \"Translate the following text to French: hello, how are you?\"}\n",
    ")\n",
    "\n",
    "# Add an LLM call\n",
    "trace.span(\n",
    "    name=\"llm_call\",\n",
    "    type=\"llm\",\n",
    "    input={\"prompt\": \"Translate the following text to French: hello, how are you?\"},\n",
    "    output={\"response\": \"Comment ça va?\"}\n",
    ")\n",
    "\n",
    "# End the trace\n",
    "trace.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom dataset for the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self,data : pd.DataFrame):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            data pd.DataFrame : data from dataset,\n",
    "            comet_id Optional[str] : dataset's name id from comet ML\n",
    "            prompt_id Optional[str] : prompt's template id from comet ML\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "          \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get either a single data point or a pandas Dataframe window of data points\n",
    "        data_window = self.data.iloc[idx]    \n",
    "\n",
    "        return data_window.to_dict()\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataloader(DataLoader):\n",
    "    def __init__(self, tokenizer :  PreTrainedTokenizer | PreTrainedTokenizerFast, **kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        kwargs[\"collate_fn\"] = self.chatbot_collate\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    \n",
    "    def chatbot_collate(self,batch):\n",
    "        \"\"\"Custom collate function to teach the Dataloader class how to parse the batches into an llm friendly format\n",
    "        Args:\n",
    "            original_batch : List of batch elements with len -> batch_size. Each list's element strictly follows \n",
    "            the format inside __getitem__ from Dataset class. \n",
    "        \n",
    "        \"\"\"\n",
    "        prompts,answers,languages,records_id = [],[],[],[]\n",
    "\n",
    "        \n",
    "        for dic in batch:\n",
    "            if self.tokenizer.chat_template: #tokenizer has chat template\n",
    "                \n",
    "                prompt_messages = [\n",
    "                    {\"role\": \"system\", \"content\": c.SYSTEM_TEMPLATE},\n",
    "                    {\"role\" : \"user\", \"content\" : dic[\"prompt\"]}\n",
    "                ]\n",
    "\n",
    "                try:\n",
    "                    prompt_text  = self.tokenizer.apply_chat_template(\n",
    "                        prompt_messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True,\n",
    "                    )\n",
    "                except Exception:\n",
    "                    # chat template does not support system role\n",
    "\n",
    "                    prompt_messages = [\n",
    "                    {\"role\": \"user\", \"content\": c.SYSTEM_TEMPLATE},\n",
    "                    {\"role\" : \"assistant\" , \"content\" : \"Ok\"},\n",
    "                    {\"role\" : \"user\", \"content\" : dic[\"prompt\"]}\n",
    "                    ]\n",
    "\n",
    "                    prompt_text  = self.tokenizer.apply_chat_template(\n",
    "                        prompt_messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True,\n",
    "                    )              \n",
    "\n",
    "            else:\n",
    "                prompt_text = \"\"\"\n",
    "                {system}{prompt}\n",
    "                \"\"\".format(system=c.SYSTEM_TEMPLATE,prompt=dic['prompt'])\n",
    "\n",
    "                \n",
    "            answers.append(dic['answer'])\n",
    "            prompts.append(prompt_text)\n",
    "            languages.append(dic['language'])\n",
    "            records_id.append(dic['id'])\n",
    "\n",
    "\n",
    "        #tokenize batch of prompts and answers\n",
    "        prompt_tokenize = self.tokenizer(prompts,\n",
    "                padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"inputs\" : prompt_tokenize, #Dict[str,torch.Tensor]\n",
    "            \"prompts\" : prompts, #list[str],\n",
    "            \"labels\" : answers, #list[str]\n",
    "            \"languages\" : languages, #list[str]\n",
    "            \"records_id\" : records_id, #list[str]\n",
    "            \"longest_seq\" : prompt_tokenize[\"input_ids\"].shape[1] #int\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepSeek models: post generated tokens processing. <br>\n",
    "These models were trained to think about the given prompt, assess its contents, and finally respond to the\n",
    "question.\n",
    "Thus, I'll be clearing its thought process from the answer, which is under the tag: think/think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def postprocess_generated_output_deepseek(pattern : str,outputs : list[str]) -> list[Any]:\n",
    "    processed = []\n",
    "    for output in outputs:\n",
    "        match = re.search(pattern, output)\n",
    "        if match:\n",
    "            processed.append(output[match.end():])\n",
    "        else:\n",
    "            processed.append(output)\n",
    "    return processed\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "@torch.no_grad()\n",
    "def model_inference(dataset_name,model,dataloader,config : InferenceArgs,\n",
    "        resume : Optional[dict[str,dict_vals_datatype]] = None) -> dict[str,dict_vals_datatype]:\n",
    "    \"\"\"\n",
    "    Retrieves two lists, the first list specifies the LLM's decisions per record, on which response was more humanly\n",
    "    seen. The other specifies the challenge's ground truth.\n",
    "\n",
    "    Args:\n",
    "        model : HuggingFace Pretrained LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \"\"\"     if resume:\n",
    "        global_output_winners = resume['predictions']\n",
    "        global_answers = resume['answers']\n",
    "        resume_idx = resume['last_idx'] + 1 \"\"\"\n",
    "    \n",
    "    global_output = []\n",
    "\n",
    "    deepseek_model_pattern = r'DeepSeek'\n",
    "    try:\n",
    "\n",
    "        i = 0\n",
    "        for i,batch in enumerate(tqdm(dataloader,desc=f\"Dataset : {dataset_name} - Model Inference\")):\n",
    "            #if i < resume_idx:\n",
    "            #    continue\n",
    "\n",
    "            # Let's send current batch into model device\n",
    "\n",
    "            inputs= batch[\"inputs\"].to(model.device)\n",
    "\n",
    "            logger.info(f\"Batch: {i}. Max Batch Input tokens size : {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "\n",
    "            #forward batch of input tokens into the model, get output token ids\n",
    "            output_token_ids  = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                #do_sample = False,\n",
    "                #cache_implementation = \"quantized\",\n",
    "                #cache_config= {\"nbits\" : 4, \"backend\" : \"quanto\"}\n",
    "            )\n",
    "\n",
    "            output_token_ids = output_token_ids.detach().cpu()\n",
    "\n",
    "            #Remove prompt from generated response\n",
    "            \n",
    "            output_token_ids = [output_token_ids[i,batch[\"longest_seq\"]:]  for i in range(\n",
    "                output_token_ids.shape[0])]\n",
    "\n",
    "            #Decode batch's output\n",
    "            #list[config.batch_size]\n",
    "            batch_decoded_responses = dataloader.tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "            #if re.search(deepseek_model_pattern, config.model_name):\n",
    "            #    think_pattern = \"</think>\"\n",
    "            #    batch_decoded_responses = postprocess_generated_output_deepseek(think_pattern,batch_decoded_responses)\n",
    "                \n",
    "\n",
    "            \n",
    "            #store batch predictions and dataset's metadata\n",
    "\n",
    "            batch.update({\n",
    "                'predictions' : batch_decoded_responses,\n",
    "                'dataset_name' : [dataset_name] * config.batch_size,\n",
    "                'comet_prompt_template_name' : [config.comet_prompt_template_name] * config.batch_size,\n",
    "                \"comet_prompt_version\" : [config.comet_prompt_version] * config.batch_size\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #transform batch dict, dict[collumn para, list] to store a DS where each element is a record,\n",
    "            #i.e. list[record in batch]\n",
    "\n",
    "            def from_batch_to_records_struct(batch):\n",
    "                \"\"\"\n",
    "                Args:\n",
    "                    batch: dict of parameters, where each param is constituted of a data struct of size batch\n",
    "                returns:\n",
    "                    list of records inside batch\n",
    "                \"\"\"\n",
    "                return [\n",
    "                    {\n",
    "                       'dataset_name' : batch['dataset_name'][i],\n",
    "                       'record_id' : batch['records_id'][i],\n",
    "                       'prompt_template_name' : batch['comet_prompt_template_name'][i],\n",
    "                       \"prompt_version\" : batch['comet_prompt_version'][i],\n",
    "                       'prompt' : batch['prompts'][i],\n",
    "                       'prediction' : batch['predictions'][i],\n",
    "                       'label' : batch['labels'][i],\n",
    "                       'language' : batch['languages'][i]\n",
    "                    }\n",
    "                    for i in range(config.batch_size)\n",
    "                ]\n",
    "            \n",
    "            #store record outputs in global container\n",
    "            global_output.extend(from_batch_to_records_struct(batch))\n",
    "\n",
    "\n",
    "            #clear GPU cache\n",
    "            torch.cuda.empty_cache()                \n",
    "    except KeyboardInterrupt as k:\n",
    "        print(k)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        return {\n",
    "            'output' : global_output,\n",
    "            'last_idx' : i\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pipeline(opik_client,model, tokenizer, num_datasets : int,config : InferenceArgs,\n",
    "                       resume : Optional[list[dict[str,dict_vals_datatype]]] = None):\n",
    "    \n",
    "    global_ouput = []\n",
    "\n",
    "    \"\"\"     resume_dataset_id = 0\n",
    "    resume_last_dict = None\n",
    "    if resume:\n",
    "        global_ouput = resume\n",
    "        resume_dataset_id = len(resume)\n",
    "        resume_last_dict = resume.pop() \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for dataset_id in range(1,num_datasets+1):\n",
    "        \"\"\"         if dataset_id < resume_dataset_id:\n",
    "            continue \"\"\"\n",
    "\n",
    "        #get dataset from commet ML\n",
    "        dataset_name = f\"{config.comet_dataset_name}-{dataset_id}\"\n",
    "        dataset = opik_client.get_or_create_dataset(dataset_name).to_pandas()\n",
    "\n",
    "        #construct Dataset and Dataloader\n",
    "        dataset = ChatbotDataset(dataset)\n",
    "        dataloader = ChatbotDataloader(tokenizer=tokenizer,dataset=dataset,batch_size=config.batch_size)\n",
    "\n",
    "        #run inference per dataset inside function...\n",
    "        output = model_inference(dataset_name,model,dataloader,config)\n",
    "\n",
    "        #store outputs from current dataset in the specified project from comet ML\n",
    "\n",
    "        def store_results_in_project_comet_ml(output):\n",
    "            for i,record in enumerate(output):\n",
    "                trace_dict = {\n",
    "                    \"comet_dataset_name\" : record['dataset_name'],\n",
    "                    \"comet_prompt_template_name\" : record['prompt_template_name'],\n",
    "                    \"comet_prompt_version\" : record['prompt_version']\n",
    "                }\n",
    "                \n",
    "                trace = opik_client.trace(\n",
    "                    name=f\"record_results:{record['record_id']}\",\n",
    "                    metadata=trace_dict\n",
    "                )\n",
    "\n",
    "                # Add llm call\n",
    "                trace.span(\n",
    "                    name=\"llm call\",\n",
    "                    input={'prompt' : record['prompt']},\n",
    "                    output={'response' : record['prediction']},\n",
    "                    metadata={'model' : config.model_name, 'label' : record['label'], 'language' : record['language']}\n",
    "                )\n",
    "\n",
    "                trace.end()\n",
    "\n",
    "        store_results_in_project_comet_ml(output['output'])\n",
    "        \n",
    "        \"\"\"         n = len(dataset)\n",
    "\n",
    "        if resume_last_dict:\n",
    "            outputs = model_inference(dataset_id,dataset,config,server_client,\n",
    "                                      resume_last_dict)\n",
    "            resume_last_dict = None\n",
    "        else:\n",
    "            outputs = model_inference(dataset_id,dataset,config,server_client) \"\"\"\n",
    "\n",
    "        global_ouput.append(output)\n",
    "\n",
    "        \"\"\"         if outputs['last_idx'] < n - 1:\n",
    "            print(f\"Error during batch datasets inferencing...\")\n",
    "            return global_ouput \"\"\"\n",
    "    return global_ouput\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\n",
    "\"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\",padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_cache=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.apply_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Inference Pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Inference Pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: TJUNLP/FuxiTranyu-8B-SFT inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: unsloth/gemma-2b-it-bnb-int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config.model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add special tokens and resize vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['<|im_start|>','<|im_end|>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JINJA_TEMPLATE = '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Gemma. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Gemma. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'\n",
    "\n",
    "tokenizer.chat_template = JINJA_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.9: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4060 Laptop GPU. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config.model_name,\n",
    "    #max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   0%|          | 0/234 [00:00<?, ?it/s]\u001b[32m2025-02-19 11:48:11.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 0. Max Batch Input tokens size : 3843\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   0%|          | 1/234 [01:42<6:37:04, 102.25s/it]\u001b[32m2025-02-19 11:49:53.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 1. Max Batch Input tokens size : 2732\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   1%|          | 2/234 [03:16<6:16:02, 97.25s/it] \u001b[32m2025-02-19 11:51:27.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 2. Max Batch Input tokens size : 955\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   1%|▏         | 3/234 [04:50<6:09:12, 95.90s/it]\u001b[32m2025-02-19 11:53:01.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 3. Max Batch Input tokens size : 1719\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   2%|▏         | 4/234 [06:24<6:05:04, 95.24s/it]\u001b[32m2025-02-19 11:54:35.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 4. Max Batch Input tokens size : 2147\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   2%|▏         | 5/234 [07:57<6:00:25, 94.43s/it]\u001b[32m2025-02-19 11:56:08.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 5. Max Batch Input tokens size : 1886\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   3%|▎         | 6/234 [09:31<5:58:16, 94.28s/it]\u001b[32m2025-02-19 11:57:42.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 6. Max Batch Input tokens size : 7418\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   3%|▎         | 6/234 [09:31<6:02:03, 95.28s/it]\n",
      "OPIK: Started logging traces to the \"inference-wsdm-cup-v5-deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" project at https://www.comet.com/opik/kevinmedinag96/redirect/projects?name=inference-wsdm-cup-v5-deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA driver error: out of memory\n"
     ]
    }
   ],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B inference pipeline for dataset v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B inference pipeline for dataset: v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to beam cloud GPU rental services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam import endpoint\n",
    "\n",
    "\n",
    "@endpoint(gpu=\"A100-40\")\n",
    "def handler():\n",
    "    # Prints the available GPU drivers\n",
    "    import subprocess\n",
    "    print(subprocess.check_output([\"nvidia-smi\"], shell=True))\n",
    "\n",
    "    return {\"gpu\":\"true\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I am going to compare the performance of the different open source models to a variety of different datasets.\n",
    "\n",
    "Metrics:\n",
    "* Accurracy\n",
    "* Precision\n",
    "* Recall\n",
    "* f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 27), match='best response is response a'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'best response[\\s\\S]*[ab]{1}'\n",
    "s = \"best response is response a\"\n",
    "re.search(pattern,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'model_a' : 1,\n",
    "    'model_b' : 0\n",
    "}\n",
    "id2label = {\n",
    "    1 : 'model_a',\n",
    "    0 : 'model_b'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output_llm(response) -> Union[str,int]:\n",
    "    \"\"\"\n",
    "    Retrieves a list specifying which of the two paired models in each training record adheres the best\n",
    "    to human responses.\n",
    "\n",
    "    Args:\n",
    "        responses List[str]: Batch of LLM's responses.\n",
    "    \"\"\"\n",
    "    #if model_abAB or Reponse Aa reponse bB are the last chars in response, then i would keep those, if not then ill keep the first\n",
    "    #occurrence\n",
    "    expression = r'</think>'#r'([mM]odel[_\\s][abAB][\\W]?)|([rR]esponse [abAB][\\W])'#r'model.+[a,b,A,B]'\n",
    "\n",
    "\n",
    "    #Extract pattern from response\n",
    "    match = re.search(expression, response) \n",
    "    #print(matches)\n",
    "\n",
    "    def get_model_winner(match,response : str) -> Union[str,int]:\n",
    "        \"\"\"\n",
    "        Extract which model's reponse is better from input_response\n",
    "        \"\"\"\n",
    "        #now lets extract the last match based on regexp\n",
    "        #if the last match corresponds to the last chars in response, then it will be the output to processed\n",
    "        #if not, then probably the first match is going to be the best model\n",
    "\n",
    "        def eval_string(string):\n",
    "            if 'a' in string:\n",
    "                return 1\n",
    "            return 0 #response B / model_b\n",
    "\n",
    "        if not match:\n",
    "            return response\n",
    "        \n",
    "        response = response[match.end():].strip().lower()\n",
    "\n",
    "        #conditions on finding model's response:\n",
    "        #1.- find tag: <ans>answer here</ans> or **answer here**\n",
    "        #2.- find: Response [ABab] ... better\n",
    "        #3.- best reponse ... [ab]\n",
    "        #4.- ans ... response [ab]\n",
    "        #5.- model_[ab] or _[ab]\n",
    "\n",
    "\n",
    "        pattern = r'(<ans>|\\*\\*){1}[\\s\\S]*(</ans>|\\*\\*){1}'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            return eval_string(response[match.start():match.end()])\n",
    "\n",
    "        #try finding pattern 2\n",
    "\n",
    "        pattern = r'response[\\s]*[ab]{1}[\\s\\S]*better'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            pattern1 = r'response[\\s]*[ab]{1}'\n",
    "            response1 = response[match.start():match.end()]\n",
    "            match1 = re.search(pattern1,response1)\n",
    "            return eval_string(response1[match1.start():match1.end()])\n",
    "                \n",
    "        #pattern 3\n",
    "\n",
    "        pattern = r'best response[\\s\\S]*[ab]{1}'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            pattern1 = r'response[\\s]*[ab]{1}'\n",
    "            response1 = response[match.start():match.end()]\n",
    "            match1 = re.search(pattern1,response1)\n",
    "            return eval_string(response1[match1.start():match1.end()])\n",
    "        \n",
    "        #pattern 4\n",
    "\n",
    "        pattern = r'ans[\\s]*response [ab]{1}'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            pattern1 = r'response [ab]{1}'\n",
    "            response1 = response[match.start():match.end()]\n",
    "            match1 = re.search(pattern1,response1)\n",
    "            return eval_string(response1[match1.start():match1.end()])\n",
    "        \n",
    "        #pattern 5\n",
    "\n",
    "        pattern = r'(model)?_[ab]{1}'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            return eval_string(response[match.start():match.end()])\n",
    "        \n",
    "        #if patter not found return answer\n",
    "        return response\n",
    "\n",
    "\n",
    "    return get_model_winner(match,response)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPerformance:\n",
    "    accuracy : float\n",
    "    precision : Optional[float] = None\n",
    "    recall : Optional[float] = None\n",
    "    f1_score : Optional[float] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(opik_client):\n",
    "    # get traces\n",
    "    traces = opik_client.search_traces()\n",
    "    data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for i,trace in enumerate(traces):\n",
    "        span = opik_client.search_spans(trace_id=trace.id)[0]\n",
    "        label: int = label2id[span.metadata[\"label\"]]\n",
    "        \n",
    "        prediction = span.output[\"response\"]\n",
    "        #regexp prediction to extract best model's response\n",
    "        prediction = parse_output_llm(prediction)\n",
    "\n",
    "        if isinstance(prediction,str):\n",
    "            struct_name = 'think_pattern_not_found'\n",
    "        else:\n",
    "            struct_name = 'correct'\n",
    "\n",
    "        data[struct_name][\"predictions\"].append(prediction)\n",
    "        data[struct_name][\"labels\"].append(label)\n",
    "\n",
    "    return data\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name='inference-wsdm-cup-v0-qwen2.5-1.5b-int4',\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qwen_2_5_instruct_1_5b_int4_v0 = benchmark_model(opik_client)\n",
    "\n",
    "\n",
    "performance_qwen_2_5_instruct_1_5b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_instruct_1_5b_int4_v0[\"labels\"],\n",
    "                                                                        data_qwen_2_5_instruct_1_5b_int4_v0[\"predictions\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_qwen_2_5_instruct_1_5b_int4_v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name='inference-wsdm-cup-v0-qwen2.5-coder-7b-instruct-int4',\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qwen_2_5_coder_7b_int4_v0 = benchmark_model(opik_client)\n",
    "\n",
    "\n",
    "performance_qwen_2_5_coder_7b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_coder_7b_int4_v0[\"labels\"],\n",
    "                                                                        data_qwen_2_5_coder_7b_int4_v0[\"predictions\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_qwen_2_5_coder_7b_int4_v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name=os.environ['COMET_PROJECT_NAME'],\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deepseek_qwen_v0 = benchmark_model(opik_client)\n",
    "data_deepseek_qwen_v0[\"labels\"].pop()\n",
    "data_deepseek_qwen_v0[\"labels\"].pop()\n",
    "data_deepseek_qwen_v0[\"predictions\"].pop()\n",
    "data_deepseek_qwen_v0[\"predictions\"].pop()\n",
    "\n",
    "performance_deepseek_qwen_v0 = ModelPerformance(accuracy=accuracy_score(data_deepseek_qwen_v0[\"labels\"],data_deepseek_qwen_v0[\"predictions\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_deepseek_qwen_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(data_deepseek_qwen_v0['predictions'])\n",
    "_,count = np.unique(arr,return_counts=True)\n",
    "count / arr.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(data_deepseek_qwen_v0['labels'])\n",
    "_,count = np.unique(arr,return_counts=True)\n",
    "count / arr.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B inference pipeline for dataset: v0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name=os.environ['COMET_PROJECT_NAME'],\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' performance_qwen_2_5_coder_7b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_coder_7b_int4_v0[\"labels\"],\\n                                                                        data_qwen_2_5_coder_7b_int4_v0[\"predictions\"])) '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_deepseek_r1_qwen1_5b_v0_2_1 = benchmark_model(opik_client)\n",
    "\n",
    "\n",
    "\"\"\" performance_qwen_2_5_coder_7b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_coder_7b_int4_v0[\"labels\"],\n",
    "                                                                        data_qwen_2_5_coder_7b_int4_v0[\"predictions\"])) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'predictions': ['**Megvétösek és megválasítás:**\\n\\n**1. Az integrált ökológia és AQAL modell:**\\n   - Az integrált ökológia fontosságát és hatékony működése érdekében fontos az egyéni, kollektív, belső és környezett dimenziók. A háromszintű szervezett felületében a stratosiai, operatív és hatósági szintek együttműködés hozzájárul az hatékony működés, amelyet a természetvédelmi integrált folyamok megfelelős megfelelősításával biztosítjuk.\\n\\n**2. Az \"Ökológiai megtérés\" kapcsolatok:**\\n   - Az \"ökológiai megtérés\" szükségességét hangszó a természetvédelmi folyamatok megőrzésére, a különböző szervezetek közötti együttműködés, és a természetvédelmi integrált folyamok megfelelős megfelelősításával. A kapcsolatok egészében azok az ismeret, ismeret, helyelet, fajok, résztép, értéke, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt.\\n\\n**3. Az \"új természetvédelmi tudomány\" kapcsolatok:**\\n   - A \"új természetvédelmi tudomány\" kapcsolatok egészében azok az ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt, ismeret, hely, ismeret, törtelet, dövöny, szervezés, halmaz, függetiszt.\\n\\n**4. A természetvédelem hatékony működésének megfelelős és javítások:**\\n   - A természetvédelem hatékony működésében fontos az egyéni, kollektív, belső és környezett szintek és szervezett felületek közötti együttműködés, és a természetvédelmi integrált folyamokok megfelelős megfelelősításával. A háromszintű szervezett felületén a szervezett függvényeinek hozzájárul az hatékony működés, a természetvédelmi integrált folyamokok megfelelős megfelelősításával. A természetvédelmi integrált folyamokok hozzájárul az hatékony működéshez, míg a kooperáció és a finanszírozás javítás javtalanak.\\n\\n**Kunklúzió:**\\n   - Az ökológiai válság kezelése széles körben érinti a spirituális és vallási aspektusokat, melyek a Katolikus Egyházban is jelentős szerepet játszanak. Szent Ferenc öröksége és Ferenc pápa tanításaival együtt a \"ökológiai megtérés\" elérhetővé válik, amely magában foglaljuk a társadalmi igazságosság és a szolidaritás elkoteleket. Az integrált ökológia és az AQAL modell segít a komplex környezeti kérdések átfogó megértésében, míg a természetvédelmi stratégiák fejlődése és a háromszintű szervezett felületén a szervezett függvényeinek hozzájárul az hatékony működéshez. A természetvédelmi integrált folyamokok megfelelős megfelelősításával a természetvédelem hatékony működéshez jólítik az hatékony működés, és a kooperáció és a finanszírozás javtalanak.\\n\\n**Vélemek:**\\n1. **Az \"Ökológiai Válság\" része:**\\n   - Az ökológiai válság nem csak természeti, hanem spirituális és vallási gyökerekkel rendelkezik.\\n   - A Katolikus Egyház támogatja a \"ökológiai megtérés\", amely segíthet az emberiségnek elmozdulni a természetvédelmi világhoz, és felelős megfelelőséget válik.\\n\\n2. **Szent Ferenc Öröksége:**\\n   - Szent Ferenc élete és tanításaiban megjelennek négy fő elem: Istennel való kapcsolat, személyközi kapcsolatok, teremtett világ gondozása, és önmagunk találó.\\n   - Ferenc pápa a \"Laudato Si\\'\" enciklikában hangsúlyozva a társadalmi igazságosság és szolidaritás fontosságát az ökológiai válszacs megoldása érdekében.\\n\\n3. **Integrált Ökológia és AQAL modell:**\\n   - Az integrált ökológia fontosságát és hatékony működésében fontos az egyéni, kollektív, belső és környezett szintek és szervezett felületek. A háromszintű szervezett felületén a szervezett függvényeinek hozzájárul az hatékony működés, a természetvédelmi integrált folyamokok megfelelős megfelelősításával. A háromszintű szervezett felületén a szervezett függvényeinek hozzájárul az hatékony működés, és a kooperáció és a finanszírozás javtalanak.\\n\\n4. **Természetvédelmi stratégiák:**\\n   - A természetvédelmi stratégiák megfelelős megfelelősításával a természetvédelem hatékony működéshez jólítik az hatékony működés, és a kooperáció és a finanszírozás javtalanak.\\n\\n5. **Természetvédelmi stratégiák fejlődése:**\\n   - A természetvédelmi stratégiák megfelelős megfelelősításával a természetvédelem hatékony működéshez jólítik az hatékony működés, és a kooperáció és a finanszírozás javtalanak.\\n\\n6. **Természetvédelmi stratégiák javtalanak:**\\n   - A természetvédelmi stratégiák fejlődések javtalanak, mivel termeszettes társadalmi asztricit és társadalmi értékeket biztosítanak.\\n\\n**Vélemek:**\\n1. **Az \"Természetvédelem hatékony működésére javtalanak:**\\n   - A természetvédelmi integrált folyamokok hozzájárul az hatékony működéshez, mivel termeszettes társadalmi asztricit és társadalmi értékeket biztosítanak.\\n\\n2. **A kooperáció és a finanszírozás javtalanak:**\\n   - A kooperáció és a finanszírozás javtalanak, mivel a természetvédelmi integrált folyamokok hozzájárul az hatékony működéshez, mivel termeszettes társadalmi asztricit és társadalmi értékeket biztosítanak.\\n\\n3. **Az AQAL modell segít a komplex környezeti kérdékek átfogó megért',\n",
       "              'Okay, so I need to figure out which response is better between Response A and Response B based on factors like clarity, relevance, tone, and overall quality. Let me start by reading both responses carefully.\\n\\nResponse A is \"Hello! 👋 How can I help you today? 😊\". I notice that it starts with \"Hello!\" and uses emojis. The tone seems friendly and approachable, which is good for a human. The use of emojis might make it seem more natural, but I\\'m not sure if that\\'s the main thing to focus on here.\\n\\nResponse B is \"Hi there! How can I help you today?\" It starts with \"Hi there!\" which is a common greeting, and it\\'s also friendly. The tone here is a bit more formal, which might be better for a more serious context.\\n\\nNow, considering the main factors: clarity. Both responses are clear, but Response B uses \"Hi there!\" which is a bit more formal. However, clarity isn\\'t the main issue here.\\n\\nNext, relevance. Both responses are relevant to the same question. The main difference is in the tone. Since the prompt didn\\'t specify any particular tone, but generally, responses should be natural and conversational.\\n\\nTone is important. A more natural tone would be better for a human. Response A uses a friendly emoji, which is a good approach, but Response B is a bit more formal, which might be better for a more serious or formal context.\\n\\nOverall quality: Both are good, but Response A might be a bit more approachable and less formal, which could be better for a human to read and understand.\\n\\nWait, but the prompt didn\\'t specify any context. So, if I have to choose based solely on factors mentioned, the tone is the most important. So, Response A\\'s friendly tone might be better than Response B\\'s formal tone.\\n\\nBut wait, the user just asked me to choose between the two, so maybe I should look at both aspects. The main factors are clarity, relevance, tone, and quality.\\n\\nClarity: Both are clear.\\n\\nRelevance: Both are relevant.\\n\\nTone: Response A is more approachable, Response B is more formal.\\n\\nQuality: Both are good, but maybe Response A is a bit more approachable, which is better for a human.\\n\\nSo, considering all factors, Response A might be better because it\\'s more approachable and less formal, which would be better for a human to read and understand.\\n\\nWait, but sometimes humans prefer more formal language. Hmm, I\\'m a bit conflicted here.\\n\\nWait, the user didn\\'t specify any context, so I should go with the most approachable tone, which is Response A.\\n\\nBut wait, the initial prompt didn\\'t mention any context, so maybe I should consider both.\\n\\nAlternatively, maybe the user wants the most natural and approachable response, so Response A.\\n\\nBut I\\'m not entirely sure. I think Response A is better because it\\'s more approachable, which is likely what humans prefer.\\n\\nWait, but sometimes people prefer more formal responses. I\\'m a bit stuck here.\\n\\nWait, perhaps the best response is Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nAlternatively, Response B is more formal, which might be better for a more serious context.\\n\\nBut since the prompt doesn\\'t specify, I think Response A is better because it\\'s more approachable, which is likely what humans prefer.\\n\\nWait, but I\\'m not entirely sure. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, no, maybe I should consider that humans prefer more natural and approachable responses, so Response A is better.\\n\\nBut I\\'m not confident. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. Maybe I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I\\'m a bit stuck. I think I\\'ll go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nBut I\\'m not entirely sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nWait, but the user didn\\'t specify any context, so maybe I should choose the most natural and approachable response.\\n\\nAlternatively, maybe Response B is more formal, which is better for a serious context.\\n\\nI think I should go with Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nWait, but the user just wants the best response based on the factors, so I should choose the response that is more approachable and less formal, which is Response A.\\n\\nBut I\\'m not sure. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I\\'m stuck. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. Maybe I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I\\'ll go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. Maybe I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI\\'m stuck. I think I should choose Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. Maybe I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI\\'m stuck. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI\\'m stuck. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI\\'m stuck. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait',\n",
       "              'Добре, спочатку я розглянуте цю перелікну історію, яку його використовував Максим. Ця історія це про покращення ілюзіоністі, що зменшується через опитання з докладною інформацією. Це це досвід із фантастичним ефектом, інформаційністю, яку його поглядає.\\n\\nПерша частина історії: Максим взяє папку і відчуває що є чорна хмара. Він змінює її, його пальці нервово погладують на натяжку. Це показує, що його погляд може змінювати та відчатаються. Це звертає увагу на детективність.\\n\\nДалі, Максим звертає увагу на кабінет, що здається має більше міжфілдів, і він зберігає кабінет і папку. Це показує, що його діяці зміняють папку, що може змінювати його історію.\\n\\nДруга частина: Максим поглядає на візит до міста, де його боротьбі з боротьбou зіснанням. Це показує, що його історія може змінювати підхід, і ніжки підхід можуть бути відомішіші.\\n\\nТретя частина: Максим підхідний до вітера, де його боротьбou зіснанням, і це показує, що його історія може змінути інформацію охідців.\\n\\nЧетверташа частина: Максим робить кабінет і відчуває що його боротьбou зіснанням, що може змінювати його історію і підхід. Це показує, що його діяці зміняють папку і підхід, що може змінювати його історію.\\n\\nПісля цього, Максим зупинився у папку, що його боротьбou зіснанням, і він підхідний до міста. Це показує, що його історія може змінути підхід і підхід до міста.\\n\\nОдній інформаційний діалог: Максим сподівається, що його боротьбou зіснанням, що може змінути його історію і підхід. Це показує, що його діяці змінюють папку і підхід, що може змінювати його історію.\\n\\nПісля цього, Максим кивнує, його обличчя було напруженим, і ніч здається, що його історія може змінутиться.\\n\\nВін відкрив папку і підхідний до міста, де його боротьбou зіснанням, і це показує, що його історія може змінути підхід і підхід до міста.\\n\\nПісля цього, Максим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nПісля цього, Максим кивнує, його обличчя було напруженим, і ніч здається, що його історія може змінутиться.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його історію.\\n\\nМаксим звертає увагу на кабінет, де його боротьбou зіснанням, і він зберігає кабінет і папку. Це показує, що його діяці змінюють папку і підхід, що може змінути його іст',\n",
       "              'Okay, so I\\'m trying to figure out which response from Response A or Response B is more likely to be preferred by humans. Let me start by reading both responses carefully.\\n\\nResponse A is: \"toki! mi ken toki e toki pona en lawa nimi ante mute. sina wile toki kepeken seme la mi ken pali e ni.\"\\n\\nResponse B is a long, complex English translation starting with \"Mi moku e toki Inli, toki Espania, toki Latiina, toki Han, toki Rusi, toki Hindi, toki Bengali, toki Portugali, toki Nipon, toki Inli putona, toki Inli sina, toki Inli micin, toki Inli micin sina, toki Inli putona micin, toki Inli putona micin sina, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki In'],\n",
       "             'labels': [1, 0, 1, 1]})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_deepseek_r1_qwen1_5b_v0_2_1['think_pattern_not_found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =data_deepseek_r1_qwen1_5b_v0_2_1['correct']['predictions']\n",
    "labels = data_deepseek_r1_qwen1_5b_v0_2_1['correct']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 13, 213]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_arr = np.array(predictions)\n",
    "np.unique(preds_arr,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 97, 129]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_arr = np.array(labels)\n",
    "np.unique(labels_arr,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5398230088495575"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels_arr,preds_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsdm-cup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
