{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created to test how current open source state-of-the-art LLMs perform for the multilingual-chatbot-arena dataset.\n",
    "\n",
    "The experiment's constraints are as follows:\n",
    "\n",
    "1. Model's inference will be held using an NVIDIA GeForce RTX 4060. Therefore the GPU computing is limited. Recall that is most likely that the current pretrained models with not perform very well on this challenge's data. The benchmarked LLMs must hold around 7-9B parameters in order for the hardware to handle inferencing/fine tuning.\n",
    "2. For the fine tuning of LLMs. The best course of action will be to use QLORA. Due to hardware constraints.\n",
    "3. There are an abundant number of capable open-source LLMs. In this Demo we will be benchmarking 3 model's families: Qwen2.5, Llama 3.X and gemini.\n",
    "4. Dataset for the experiment: training set.\n",
    "5. Performance metric: Accuracy (For the whole dataset, how many prompts what's the proportion of accurately predicted answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "root_repo_directory = pathlib.Path().resolve().parent.__str__()\n",
    "sys.path.append(root_repo_directory)\n",
    "from multilingual_chatbot_arena import initialize\n",
    "import datasets_creator.src.constants as c\n",
    "import datasets_creator.src.utils as utils\n",
    "import pandas as pd\n",
    "from fire import Fire\n",
    "from pydantic import BaseModel\n",
    "from typing import List,Optional,Dict,Union,Any\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import opik\n",
    "from loguru import logger\n",
    "initialize()\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig, BitsAndBytesConfig\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colors\n",
    "black = mcolors.CSS4_COLORS[\"black\"]\n",
    "lime = mcolors.CSS4_COLORS[\"lime\"]\n",
    "aqua = mcolors.CSS4_COLORS[\"aqua\"]\n",
    "magenta = mcolors.CSS4_COLORS[\"magenta\"]\n",
    "red = mcolors.CSS4_COLORS[\"red\"]\n",
    "grey = mcolors.CSS4_COLORS[\"grey\"]\n",
    "orange = mcolors.CSS4_COLORS[\"orangered\"]\n",
    "gold = mcolors.CSS4_COLORS[\"gold\"]\n",
    "blue = mcolors.CSS4_COLORS[\"blue\"]\n",
    "indigo = mcolors.CSS4_COLORS[\"indigo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting configuration arguments for the whole script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vals_datatype = Union[list[int],int]\n",
    "@dataclass\n",
    "class InferenceArgs:\n",
    "    comet_dataset_name : str\n",
    "    comet_datset_description : str\n",
    "    comet_prompt_template_name : str\n",
    "    model_name : str\n",
    "    max_new_tokens : int\n",
    "    batch_size : Optional[int] = None\n",
    "    cache : Optional[str] = None\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceFile:\n",
    "\n",
    "    data : list[dict[str,dict_vals_datatype]]\n",
    "    inference_dataset : str\n",
    "\n",
    "    def save(self,file_path, file_name,include_version = False):\n",
    "        utils.to_pickle(self.data,file_path,file_name,include_version)\n",
    "\n",
    "\n",
    "config = InferenceArgs(\n",
    "    comet_dataset_name=\"multilingual-chatbot-arena-v0-train\",\n",
    "    comet_datset_description=\"Challenge: WSDM CUP. Curated-smal-dataset - version0 - Training set 1.\",\n",
    "    comet_prompt_template_name = 'Prompt_template_wsdm_cup_1',\n",
    "    model_name=\"unsloth/gemma-2b-it-bnb-4bit\",\n",
    "    cache=\"quantized KV cache: quanto\",\n",
    "    max_new_tokens= 4096,\n",
    "    batch_size=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset from Comet ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name=os.environ['COMET_PROJECT_NAME'],\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset's Classes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_winner_series = df[\"answer\"].value_counts()\n",
    "df_winner_series = pd.DataFrame({\n",
    "    \"answer\" : df_winner_series.keys(),\n",
    "    \"proportion\" : df_winner_series.values / df.shape[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize= (5,5))\n",
    "sns.barplot(data=df_winner_series,x=\"answer\",y=\"proportion\",ax=ax,color=magenta)\n",
    "\n",
    "\n",
    "#ax.tick_params(axis='x',labelrotation=90,labelsize=12)\n",
    "ax.set_xlabel(\"Winner model\",fontsize=10)\n",
    "ax.set_ylabel(\"Proportion\",fontsize=10)\n",
    "ax.set_title(\"Winner Model's Proportion in Training Set\",fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_percentage = ((df_winner_series['proportion'][0] - df_winner_series['proportion'][1]) / df_winner_series['proportion'][1]) * 100\n",
    "\n",
    "print(f\"Difference in classes distribution : {diff_percentage} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client.search_traces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a trace\n",
    "trace = opik_client.trace(\n",
    "    name=\"my_trace-2\",\n",
    "    input=\"Hello, how are you?\",\n",
    "    output={\"response\": \"Comment ça va?\"}\n",
    ")\n",
    "\n",
    "opik_client.log_traces_feedback_scores(\n",
    "    scores=[\n",
    "        {\"id\": trace.id, \"name\": \"overall_quality\", \"value\": 0.85, \"reason\": \"The response was helpful and accurate.\"},\n",
    "        {\"id\": trace.id, \"name\": \"coherence\", \"value\": 0.75},\n",
    "        {\"id\" : trace.id, \"name\" : \"correctness\", \"value\" : 1.0}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Add a span\n",
    "trace.span(\n",
    "    name=\"Add prompt template\",\n",
    "    input={\"text\": \"Hello, how are you?\", \"prompt_template\": \"Translate the following text to French: {text}\"},\n",
    "    output={\"text\": \"Translate the following text to French: hello, how are you?\"}\n",
    ")\n",
    "\n",
    "# Add an LLM call\n",
    "trace.span(\n",
    "    name=\"llm_call\",\n",
    "    type=\"llm\",\n",
    "    input={\"prompt\": \"Translate the following text to French: hello, how are you?\"},\n",
    "    output={\"response\": \"Comment ça va?\"}\n",
    ")\n",
    "\n",
    "# End the trace\n",
    "trace.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom dataset for the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self,data : pd.DataFrame):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            data pd.DataFrame : data from dataset,\n",
    "            comet_id Optional[str] : dataset's name id from comet ML\n",
    "            prompt_id Optional[str] : prompt's template id from comet ML\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "          \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get either a single data point or a pandas Dataframe window of data points\n",
    "        data_window = self.data.iloc[idx]    \n",
    "\n",
    "        return data_window.to_dict()\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataloader(DataLoader):\n",
    "    def __init__(self, tokenizer :  PreTrainedTokenizer | PreTrainedTokenizerFast, **kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        kwargs[\"collate_fn\"] = self.chatbot_collate\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    \n",
    "    def chatbot_collate(self,batch):\n",
    "        \"\"\"Custom collate function to teach the Dataloader class how to parse the batches into an llm friendly format\n",
    "        Args:\n",
    "            original_batch : List of batch elements with len -> batch_size. Each list's element strictly follows \n",
    "            the format inside __getitem__ from Dataset class. \n",
    "        \n",
    "        \"\"\"\n",
    "        prompts,answers,languages,records_id = [],[],[],[]\n",
    "\n",
    "        \n",
    "        for dic in batch:\n",
    "            if self.tokenizer.chat_template: #tokenizer has chat template\n",
    "                \n",
    "                prompt_messages = [\n",
    "                    {\"role\": \"system\", \"content\": c.SYSTEM_TEMPLATE},\n",
    "                    {\"role\" : \"user\", \"content\" : dic[\"prompt\"]}\n",
    "                ]\n",
    "\n",
    "                try:\n",
    "                    prompt_text  = self.tokenizer.apply_chat_template(\n",
    "                        prompt_messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True,\n",
    "                    )\n",
    "                except Exception:\n",
    "                    # chat template does not support system role\n",
    "\n",
    "                    prompt_messages = [\n",
    "                    {\"role\": \"user\", \"content\": c.SYSTEM_TEMPLATE},\n",
    "                    {\"role\" : \"assistant\" , \"content\" : \"Ok\"},\n",
    "                    {\"role\" : \"user\", \"content\" : dic[\"prompt\"]}\n",
    "                    ]\n",
    "\n",
    "                    prompt_text  = self.tokenizer.apply_chat_template(\n",
    "                        prompt_messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True,\n",
    "                    )              \n",
    "\n",
    "            else:\n",
    "                prompt_text = \"\"\"\n",
    "                {system}{prompt}\n",
    "                \"\"\".format(system=c.SYSTEM_TEMPLATE,prompt=dic['prompt'])\n",
    "\n",
    "                \n",
    "            answers.append(dic['answer'])\n",
    "            prompts.append(prompt_text)\n",
    "            languages.append(dic['language'])\n",
    "            records_id.append(dic['id'])\n",
    "\n",
    "\n",
    "        #tokenize batch of prompts and answers\n",
    "        prompt_tokenize = self.tokenizer(prompts,\n",
    "                padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"inputs\" : prompt_tokenize, #Dict[str,torch.Tensor]\n",
    "            \"prompts\" : prompts, #list[str],\n",
    "            \"labels\" : answers, #list[str]\n",
    "            \"languages\" : languages, #list[str]\n",
    "            \"records_id\" : records_id, #list[str]\n",
    "            \"longest_seq\" : prompt_tokenize[\"input_ids\"].shape[1] #int\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepSeek models: post generated tokens processing. <br>\n",
    "These models were trained to think about the given prompt, assess its contents, and finally respond to the\n",
    "question.\n",
    "Thus, I'll be clearing its thought process from the answer, which is under the tag: think/think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def postprocess_generated_output_deepseek(pattern : str,outputs : list[str]) -> list[Any]:\n",
    "    processed = []\n",
    "    for output in outputs:\n",
    "        match = re.search(pattern, output)\n",
    "        if match:\n",
    "            processed.append(output[match.end():])\n",
    "        else:\n",
    "            processed.append(output)\n",
    "    return processed\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "@torch.no_grad()\n",
    "def model_inference(dataset_name,model,dataloader,config : InferenceArgs,\n",
    "        resume : Optional[dict[str,dict_vals_datatype]] = None) -> dict[str,dict_vals_datatype]:\n",
    "    \"\"\"\n",
    "    Retrieves two lists, the first list specifies the LLM's decisions per record, on which response was more humanly\n",
    "    seen. The other specifies the challenge's ground truth.\n",
    "\n",
    "    Args:\n",
    "        model : HuggingFace Pretrained LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \"\"\"     if resume:\n",
    "        global_output_winners = resume['predictions']\n",
    "        global_answers = resume['answers']\n",
    "        resume_idx = resume['last_idx'] + 1 \"\"\"\n",
    "    \n",
    "    global_output = []\n",
    "\n",
    "    deepseek_model_pattern = r'DeepSeek'\n",
    "    try:\n",
    "\n",
    "        i = 0\n",
    "        for i,batch in enumerate(tqdm(dataloader,desc=f\"Dataset : {dataset_name} - Model Inference\")):\n",
    "            #if i < resume_idx:\n",
    "            #    continue\n",
    "\n",
    "            # Let's send current batch into model device\n",
    "\n",
    "            inputs= batch[\"inputs\"].to(model.device)\n",
    "\n",
    "            logger.info(f\"Batch: {i}. Max Batch Input tokens size : {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "\n",
    "            #forward batch of input tokens into the model, get output token ids\n",
    "            output_token_ids  = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                do_sample = False,\n",
    "                cache_implementation = \"quantized\",\n",
    "                cache_config= {\"nbits\" : 4, \"backend\" : \"quanto\"}\n",
    "            )\n",
    "\n",
    "            output_token_ids = output_token_ids.detach().cpu()\n",
    "\n",
    "            #Remove prompt from generated response\n",
    "            \n",
    "            output_token_ids = [output_token_ids[i,batch[\"longest_seq\"]:]  for i in range(\n",
    "                output_token_ids.shape[0])]\n",
    "\n",
    "            #Decode batch's output\n",
    "            #list[config.batch_size]\n",
    "            batch_decoded_responses = dataloader.tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "            if re.search(deepseek_model_pattern, config.model_name):\n",
    "                think_pattern = \"</think>\"\n",
    "                batch_decoded_responses = postprocess_generated_output_deepseek(think_pattern,batch_decoded_responses)\n",
    "                \n",
    "\n",
    "            \n",
    "            #store batch predictions and dataset's metadata\n",
    "\n",
    "            batch.update({\n",
    "                'predictions' : batch_decoded_responses,\n",
    "                'dataset_name' : [dataset_name] * config.batch_size,\n",
    "                'comet_prompt_template_name' : [config.comet_prompt_template_name] * config.batch_size\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #transform batch dict, dict[collumn para, list] to store a DS where each element is a record,\n",
    "            #i.e. list[record in batch]\n",
    "\n",
    "            def from_batch_to_records_struct(batch):\n",
    "                \"\"\"\n",
    "                Args:\n",
    "                    batch: dict of parameters, where each param is constituted of a data struct of size batch\n",
    "                returns:\n",
    "                    list of records inside batch\n",
    "                \"\"\"\n",
    "                return [\n",
    "                    {\n",
    "                       'dataset_name' : batch['dataset_name'][i],\n",
    "                       'record_id' : batch['records_id'][i],\n",
    "                       'prompt_template_name' : batch['comet_prompt_template_name'][i],\n",
    "                       'prompt' : batch['prompts'][i],\n",
    "                       'prediction' : batch['predictions'][i],\n",
    "                       'label' : batch['labels'][i],\n",
    "                       'language' : batch['languages'][i]\n",
    "                    }\n",
    "                    for i in range(config.batch_size)\n",
    "                ]\n",
    "            \n",
    "            #store record outputs in global container\n",
    "            global_output.extend(from_batch_to_records_struct(batch))\n",
    "\n",
    "\n",
    "            #clear GPU cache\n",
    "            torch.cuda.empty_cache()                \n",
    "    except KeyboardInterrupt as k:\n",
    "        print(k)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        return {\n",
    "            'output' : global_output,\n",
    "            'last_idx' : i\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pipeline(opik_client,model, tokenizer, num_datasets : int,config : InferenceArgs,\n",
    "                       resume : Optional[list[dict[str,dict_vals_datatype]]] = None):\n",
    "    \n",
    "    global_ouput = []\n",
    "\n",
    "    \"\"\"     resume_dataset_id = 0\n",
    "    resume_last_dict = None\n",
    "    if resume:\n",
    "        global_ouput = resume\n",
    "        resume_dataset_id = len(resume)\n",
    "        resume_last_dict = resume.pop() \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for dataset_id in range(1,num_datasets+1):\n",
    "        \"\"\"         if dataset_id < resume_dataset_id:\n",
    "            continue \"\"\"\n",
    "\n",
    "        #get dataset from commet ML\n",
    "        dataset_name = f\"{config.comet_dataset_name}-{dataset_id}\"\n",
    "        dataset = opik_client.get_or_create_dataset(dataset_name).to_pandas()\n",
    "\n",
    "        #construct Dataset and Dataloader\n",
    "        dataset = ChatbotDataset(dataset)\n",
    "        dataloader = ChatbotDataloader(tokenizer=tokenizer,dataset=dataset,batch_size=config.batch_size)\n",
    "\n",
    "        #run inference per dataset inside function...\n",
    "        output = model_inference(dataset_name,model,dataloader,config)\n",
    "\n",
    "        #store outputs from current dataset in the specified project from comet ML\n",
    "\n",
    "        def store_results_in_project_comet_ml(output):\n",
    "            for i,record in enumerate(output):\n",
    "                trace_dict = {\n",
    "                    \"comet_dataset_name\" : record['dataset_name'],\n",
    "                    \"comet_prompt_template_name\" : record['prompt_template_name']\n",
    "                }\n",
    "                \n",
    "                trace = opik_client.trace(\n",
    "                    name=f\"record_results:{record['record_id']}\",\n",
    "                    metadata=trace_dict\n",
    "                )\n",
    "\n",
    "                # Add llm call\n",
    "                trace.span(\n",
    "                    name=\"llm call\",\n",
    "                    input={'prompt' : record['prompt']},\n",
    "                    output={'response' : record['prediction']},\n",
    "                    metadata={'model' : config.model_name, 'label' : record['label'], 'language' : record['language']}\n",
    "                )\n",
    "\n",
    "                trace.end()\n",
    "\n",
    "        store_results_in_project_comet_ml(output['output'])\n",
    "        \n",
    "        \"\"\"         n = len(dataset)\n",
    "\n",
    "        if resume_last_dict:\n",
    "            outputs = model_inference(dataset_id,dataset,config,server_client,\n",
    "                                      resume_last_dict)\n",
    "            resume_last_dict = None\n",
    "        else:\n",
    "            outputs = model_inference(dataset_id,dataset,config,server_client) \"\"\"\n",
    "\n",
    "        global_ouput.append(output)\n",
    "\n",
    "        \"\"\"         if outputs['last_idx'] < n - 1:\n",
    "            print(f\"Error during batch datasets inferencing...\")\n",
    "            return global_ouput \"\"\"\n",
    "    return global_ouput\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\n",
    "\"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\",padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_cache=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.apply_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Inference Pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Inference Pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: TJUNLP/FuxiTranyu-8B-SFT inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: unsloth/gemma-2b-it-bnb-int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config.model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add special tokens and resize vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['<|im_start|>','<|im_end|>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JINJA_TEMPLATE = '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Gemma. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Gemma. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'\n",
    "\n",
    "tokenizer.chat_template = JINJA_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config.model_name,\n",
    "    #max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B inference pipeline for dataset: v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to beam cloud GPU rental services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam import endpoint\n",
    "\n",
    "\n",
    "@endpoint(gpu=\"A100-40\")\n",
    "def handler():\n",
    "    # Prints the available GPU drivers\n",
    "    import subprocess\n",
    "    print(subprocess.check_output([\"nvidia-smi\"], shell=True))\n",
    "\n",
    "    return {\"gpu\":\"true\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I am going to compare the performance of the different open source models to a variety of different datasets.\n",
    "\n",
    "Metrics:\n",
    "* Accurracy\n",
    "* Precision\n",
    "* Recall\n",
    "* f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'model_a' : 1,\n",
    "    'model_b' : 2\n",
    "}\n",
    "id2label = {\n",
    "    1 : 'model_a',\n",
    "    2 : 'model_b'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output_llm(response) -> int:\n",
    "    \"\"\"\n",
    "    Retrieves a list specifying which of the two paired models in each training record adheres the best\n",
    "    to human responses.\n",
    "\n",
    "    Args:\n",
    "        responses List[str]: Batch of LLM's responses.\n",
    "    \"\"\"\n",
    "    #if model_abAB or Reponse Aa reponse bB are the last chars in response, then i would keep those, if not then ill keep the first\n",
    "    #occurrence\n",
    "    expression = r'([mM]odel[_\\s][abAB][\\W]?)|([rR]esponse [abAB][\\W])'#r'model.+[a,b,A,B]'\n",
    "\n",
    "\n",
    "    #Extract pattern from response\n",
    "    matches = re.findall(expression, response) \n",
    "    #print(matches)\n",
    "\n",
    "    def get_model_winner(matches,response : str) -> int:\n",
    "        \"\"\"\n",
    "        Extract which model's reponse is better from input_response\n",
    "        \"\"\"\n",
    "        #now lets extract the last match based on regexp\n",
    "        #if the last match corresponds to the last chars in response, then it will be the output to processed\n",
    "        #if not, then probably the first match is going to be the best model\n",
    "        if not matches:\n",
    "            return 3\n",
    "        \n",
    "        last_match = matches[-1]\n",
    "        #per group to parse\n",
    "        last_match = last_match[0] if last_match[0] else last_match[1]\n",
    "\n",
    "        def eval_string(string):\n",
    "            if 'a' in string:\n",
    "                return 1\n",
    "\n",
    "            return 2\n",
    "\n",
    "        #print(response.endswith(last_match))\n",
    "        if response.endswith(last_match):\n",
    "            return eval_string(last_match.lower())\n",
    "        \n",
    "        #if not, retrieve first match\n",
    "        first_match = matches[0]\n",
    "        first_match = first_match[0] if first_match[0] else first_match[1]\n",
    "\n",
    "        return eval_string(first_match.lower())\n",
    "\n",
    "\n",
    "    \n",
    "    return get_model_winner(matches,response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPerformance:\n",
    "    accuracy : float\n",
    "    precision : Optional[float] = None\n",
    "    recall : Optional[float] = None\n",
    "    f1_score : Optional[float] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(opik_client):\n",
    "    # get traces\n",
    "    traces = opik_client.search_traces()\n",
    "    data = defaultdict(list)\n",
    "\n",
    "    for i,trace in enumerate(traces):\n",
    "        span = opik_client.search_spans(trace_id=trace.id)[0]\n",
    "        label: int = label2id[span.metadata[\"label\"]]\n",
    "        \n",
    "        data[\"labels\"].append(label)\n",
    "\n",
    "        prediction: str = span.output[\"response\"]\n",
    "        #regexp prediction to extract best model's response\n",
    "        prediction:int = parse_output_llm(prediction.strip())\n",
    "\n",
    "        data[\"predictions\"].append(prediction)\n",
    "\n",
    "    return data\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name='inference-wsdm-cup-v0-qwen2.5-1.5b-int4',\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qwen_2_5_instruct_1_5b_int4_v0 = benchmark_model(opik_client)\n",
    "\n",
    "\n",
    "performance_qwen_2_5_instruct_1_5b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_instruct_1_5b_int4_v0[\"labels\"],\n",
    "                                                                        data_qwen_2_5_instruct_1_5b_int4_v0[\"predictions\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_qwen_2_5_instruct_1_5b_int4_v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name='inference-wsdm-cup-v0-qwen2.5-coder-7b-instruct-int4',\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qwen_2_5_coder_7b_int4_v0 = benchmark_model(opik_client)\n",
    "\n",
    "\n",
    "performance_qwen_2_5_coder_7b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_coder_7b_int4_v0[\"labels\"],\n",
    "                                                                        data_qwen_2_5_coder_7b_int4_v0[\"predictions\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_qwen_2_5_coder_7b_int4_v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name=os.environ['COMET_PROJECT_NAME'],\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deepseek_qwen_v0 = benchmark_model(opik_client)\n",
    "data_deepseek_qwen_v0[\"labels\"].pop()\n",
    "data_deepseek_qwen_v0[\"labels\"].pop()\n",
    "data_deepseek_qwen_v0[\"predictions\"].pop()\n",
    "data_deepseek_qwen_v0[\"predictions\"].pop()\n",
    "\n",
    "performance_deepseek_qwen_v0 = ModelPerformance(accuracy=accuracy_score(data_deepseek_qwen_v0[\"labels\"],data_deepseek_qwen_v0[\"predictions\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_deepseek_qwen_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(data_deepseek_qwen_v0['predictions'])\n",
    "_,count = np.unique(arr,return_counts=True)\n",
    "count / arr.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(data_deepseek_qwen_v0['labels'])\n",
    "_,count = np.unique(arr,return_counts=True)\n",
    "count / arr.shape[0] * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsdm-cup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
