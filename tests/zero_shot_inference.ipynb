{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created to test how current open source state-of-the-art LLMs perform for the multilingual-chatbot-arena dataset.\n",
    "\n",
    "The experiment's constraints are as follows:\n",
    "\n",
    "1. Model's inference will be held using an NVIDIA GeForce RTX 4060. Therefore the GPU computing is limited. Recall that is most likely that the current pretrained models with not perform very well on this challenge's data. The benchmarked LLMs must hold around 7-9B parameters in order for the hardware to handle inferencing/fine tuning.\n",
    "2. For the fine tuning of LLMs. The best course of action will be to use QLORA. Due to hardware constraints.\n",
    "3. There are an abundant number of capable open-source LLMs. In this Demo we will be benchmarking 3 model's families: Qwen2.5, Llama 3.X and gemini.\n",
    "4. Dataset for the experiment: training set.\n",
    "5. Performance metric: Accuracy (For the whole dataset, how many prompts what's the proportion of accurately predicted answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-19 11:43:46.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mInitializing env vars...\u001b[0m\n",
      "\u001b[32m2025-02-19 11:43:46.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLoading environment variables from: /home/kevinmg96/Kaggle competitions/WSDM Cup/multilingual-chatbot-arena/.env\u001b[0m\n",
      "/home/kevinmg96/miniconda3/envs/wsdm-cup/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "root_repo_directory = pathlib.Path().resolve().parent.__str__()\n",
    "sys.path.append(root_repo_directory)\n",
    "from multilingual_chatbot_arena import initialize\n",
    "import datasets_creator.src.constants as c\n",
    "import datasets_creator.src.utils as utils\n",
    "import pandas as pd\n",
    "from fire import Fire\n",
    "from pydantic import BaseModel\n",
    "from typing import List,Optional,Dict,Union,Any\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import opik\n",
    "from loguru import logger\n",
    "initialize()\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig, BitsAndBytesConfig\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colors\n",
    "black = mcolors.CSS4_COLORS[\"black\"]\n",
    "lime = mcolors.CSS4_COLORS[\"lime\"]\n",
    "aqua = mcolors.CSS4_COLORS[\"aqua\"]\n",
    "magenta = mcolors.CSS4_COLORS[\"magenta\"]\n",
    "red = mcolors.CSS4_COLORS[\"red\"]\n",
    "grey = mcolors.CSS4_COLORS[\"grey\"]\n",
    "orange = mcolors.CSS4_COLORS[\"orangered\"]\n",
    "gold = mcolors.CSS4_COLORS[\"gold\"]\n",
    "blue = mcolors.CSS4_COLORS[\"blue\"]\n",
    "indigo = mcolors.CSS4_COLORS[\"indigo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting configuration arguments for the whole script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vals_datatype = Union[list[int],int]\n",
    "@dataclass\n",
    "class InferenceArgs:\n",
    "    comet_dataset_name : str\n",
    "    comet_dataset_description : str\n",
    "    comet_prompt_template_name : str\n",
    "    comet_prompt_version : str\n",
    "    model_name : str\n",
    "    max_new_tokens : int\n",
    "    batch_size : Optional[int] = None\n",
    "    cache : Optional[str] = None\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceFile:\n",
    "\n",
    "    data : list[dict[str,dict_vals_datatype]]\n",
    "    inference_dataset : str\n",
    "\n",
    "    def save(self,file_path, file_name,include_version = False):\n",
    "        utils.to_pickle(self.data,file_path,file_name,include_version)\n",
    "\n",
    "\n",
    "config = InferenceArgs(\n",
    "    comet_dataset_name=\"multilingual-chatbot-arena-v0.2.1-train\",\n",
    "    comet_dataset_description=\"Challenge: WSDM CUP. Curated-smal-dataset - version0.2.1 - Training set 1.\",\n",
    "    comet_prompt_template_name = 'Prompt_template_wsdm_cup_1',\n",
    "    comet_prompt_version = \"v2.1.0\",\n",
    "    model_name=\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\",\n",
    "    cache=\"quantized KV cache: quanto\",\n",
    "    max_new_tokens= 2500,\n",
    "    batch_size=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset from Comet ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name=os.environ['COMET_PROJECT_NAME'],\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_NAME = \"Prompt_template_wsdm_cup_1\"\n",
    "comet_prompt = opik_client.get_prompt(PROMPT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset's Classes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_winner_series = df[\"answer\"].value_counts()\n",
    "df_winner_series = pd.DataFrame({\n",
    "    \"answer\" : df_winner_series.keys(),\n",
    "    \"proportion\" : df_winner_series.values / df.shape[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize= (5,5))\n",
    "sns.barplot(data=df_winner_series,x=\"answer\",y=\"proportion\",ax=ax,color=magenta)\n",
    "\n",
    "\n",
    "#ax.tick_params(axis='x',labelrotation=90,labelsize=12)\n",
    "ax.set_xlabel(\"Winner model\",fontsize=10)\n",
    "ax.set_ylabel(\"Proportion\",fontsize=10)\n",
    "ax.set_title(\"Winner Model's Proportion in Training Set\",fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_percentage = ((df_winner_series['proportion'][0] - df_winner_series['proportion'][1]) / df_winner_series['proportion'][1]) * 100\n",
    "\n",
    "print(f\"Difference in classes distribution : {diff_percentage} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client.search_traces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a trace\n",
    "trace = opik_client.trace(\n",
    "    name=\"my_trace-2\",\n",
    "    input=\"Hello, how are you?\",\n",
    "    output={\"response\": \"Comment Ã§a va?\"}\n",
    ")\n",
    "\n",
    "opik_client.log_traces_feedback_scores(\n",
    "    scores=[\n",
    "        {\"id\": trace.id, \"name\": \"overall_quality\", \"value\": 0.85, \"reason\": \"The response was helpful and accurate.\"},\n",
    "        {\"id\": trace.id, \"name\": \"coherence\", \"value\": 0.75},\n",
    "        {\"id\" : trace.id, \"name\" : \"correctness\", \"value\" : 1.0}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Add a span\n",
    "trace.span(\n",
    "    name=\"Add prompt template\",\n",
    "    input={\"text\": \"Hello, how are you?\", \"prompt_template\": \"Translate the following text to French: {text}\"},\n",
    "    output={\"text\": \"Translate the following text to French: hello, how are you?\"}\n",
    ")\n",
    "\n",
    "# Add an LLM call\n",
    "trace.span(\n",
    "    name=\"llm_call\",\n",
    "    type=\"llm\",\n",
    "    input={\"prompt\": \"Translate the following text to French: hello, how are you?\"},\n",
    "    output={\"response\": \"Comment Ã§a va?\"}\n",
    ")\n",
    "\n",
    "# End the trace\n",
    "trace.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom dataset for the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self,data : pd.DataFrame):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            data pd.DataFrame : data from dataset,\n",
    "            comet_id Optional[str] : dataset's name id from comet ML\n",
    "            prompt_id Optional[str] : prompt's template id from comet ML\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "          \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get either a single data point or a pandas Dataframe window of data points\n",
    "        data_window = self.data.iloc[idx]    \n",
    "\n",
    "        return data_window.to_dict()\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataloader(DataLoader):\n",
    "    def __init__(self, tokenizer :  PreTrainedTokenizer | PreTrainedTokenizerFast, **kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        kwargs[\"collate_fn\"] = self.chatbot_collate\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    \n",
    "    def chatbot_collate(self,batch):\n",
    "        \"\"\"Custom collate function to teach the Dataloader class how to parse the batches into an llm friendly format\n",
    "        Args:\n",
    "            original_batch : List of batch elements with len -> batch_size. Each list's element strictly follows \n",
    "            the format inside __getitem__ from Dataset class. \n",
    "        \n",
    "        \"\"\"\n",
    "        prompts,answers,languages,records_id = [],[],[],[]\n",
    "\n",
    "        \n",
    "        for dic in batch:\n",
    "            if self.tokenizer.chat_template: #tokenizer has chat template\n",
    "                \n",
    "                prompt_messages = [\n",
    "                    {\"role\": \"system\", \"content\": c.SYSTEM_TEMPLATE},\n",
    "                    {\"role\" : \"user\", \"content\" : dic[\"prompt\"]}\n",
    "                ]\n",
    "\n",
    "                try:\n",
    "                    prompt_text  = self.tokenizer.apply_chat_template(\n",
    "                        prompt_messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True,\n",
    "                    )\n",
    "                except Exception:\n",
    "                    # chat template does not support system role\n",
    "\n",
    "                    prompt_messages = [\n",
    "                    {\"role\": \"user\", \"content\": c.SYSTEM_TEMPLATE},\n",
    "                    {\"role\" : \"assistant\" , \"content\" : \"Ok\"},\n",
    "                    {\"role\" : \"user\", \"content\" : dic[\"prompt\"]}\n",
    "                    ]\n",
    "\n",
    "                    prompt_text  = self.tokenizer.apply_chat_template(\n",
    "                        prompt_messages,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True,\n",
    "                    )              \n",
    "\n",
    "            else:\n",
    "                prompt_text = \"\"\"\n",
    "                {system}{prompt}\n",
    "                \"\"\".format(system=c.SYSTEM_TEMPLATE,prompt=dic['prompt'])\n",
    "\n",
    "                \n",
    "            answers.append(dic['answer'])\n",
    "            prompts.append(prompt_text)\n",
    "            languages.append(dic['language'])\n",
    "            records_id.append(dic['id'])\n",
    "\n",
    "\n",
    "        #tokenize batch of prompts and answers\n",
    "        prompt_tokenize = self.tokenizer(prompts,\n",
    "                padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"inputs\" : prompt_tokenize, #Dict[str,torch.Tensor]\n",
    "            \"prompts\" : prompts, #list[str],\n",
    "            \"labels\" : answers, #list[str]\n",
    "            \"languages\" : languages, #list[str]\n",
    "            \"records_id\" : records_id, #list[str]\n",
    "            \"longest_seq\" : prompt_tokenize[\"input_ids\"].shape[1] #int\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepSeek models: post generated tokens processing. <br>\n",
    "These models were trained to think about the given prompt, assess its contents, and finally respond to the\n",
    "question.\n",
    "Thus, I'll be clearing its thought process from the answer, which is under the tag: think/think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def postprocess_generated_output_deepseek(pattern : str,outputs : list[str]) -> list[Any]:\n",
    "    processed = []\n",
    "    for output in outputs:\n",
    "        match = re.search(pattern, output)\n",
    "        if match:\n",
    "            processed.append(output[match.end():])\n",
    "        else:\n",
    "            processed.append(output)\n",
    "    return processed\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "@torch.no_grad()\n",
    "def model_inference(dataset_name,model,dataloader,config : InferenceArgs,\n",
    "        resume : Optional[dict[str,dict_vals_datatype]] = None) -> dict[str,dict_vals_datatype]:\n",
    "    \"\"\"\n",
    "    Retrieves two lists, the first list specifies the LLM's decisions per record, on which response was more humanly\n",
    "    seen. The other specifies the challenge's ground truth.\n",
    "\n",
    "    Args:\n",
    "        model : HuggingFace Pretrained LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \"\"\"     if resume:\n",
    "        global_output_winners = resume['predictions']\n",
    "        global_answers = resume['answers']\n",
    "        resume_idx = resume['last_idx'] + 1 \"\"\"\n",
    "    \n",
    "    global_output = []\n",
    "\n",
    "    deepseek_model_pattern = r'DeepSeek'\n",
    "    try:\n",
    "\n",
    "        i = 0\n",
    "        for i,batch in enumerate(tqdm(dataloader,desc=f\"Dataset : {dataset_name} - Model Inference\")):\n",
    "            #if i < resume_idx:\n",
    "            #    continue\n",
    "\n",
    "            # Let's send current batch into model device\n",
    "\n",
    "            inputs= batch[\"inputs\"].to(model.device)\n",
    "\n",
    "            logger.info(f\"Batch: {i}. Max Batch Input tokens size : {inputs['input_ids'].shape[1]}\")\n",
    "\n",
    "\n",
    "            #forward batch of input tokens into the model, get output token ids\n",
    "            output_token_ids  = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                #do_sample = False,\n",
    "                #cache_implementation = \"quantized\",\n",
    "                #cache_config= {\"nbits\" : 4, \"backend\" : \"quanto\"}\n",
    "            )\n",
    "\n",
    "            output_token_ids = output_token_ids.detach().cpu()\n",
    "\n",
    "            #Remove prompt from generated response\n",
    "            \n",
    "            output_token_ids = [output_token_ids[i,batch[\"longest_seq\"]:]  for i in range(\n",
    "                output_token_ids.shape[0])]\n",
    "\n",
    "            #Decode batch's output\n",
    "            #list[config.batch_size]\n",
    "            batch_decoded_responses = dataloader.tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)\n",
    "\n",
    "            #if re.search(deepseek_model_pattern, config.model_name):\n",
    "            #    think_pattern = \"</think>\"\n",
    "            #    batch_decoded_responses = postprocess_generated_output_deepseek(think_pattern,batch_decoded_responses)\n",
    "                \n",
    "\n",
    "            \n",
    "            #store batch predictions and dataset's metadata\n",
    "\n",
    "            batch.update({\n",
    "                'predictions' : batch_decoded_responses,\n",
    "                'dataset_name' : [dataset_name] * config.batch_size,\n",
    "                'comet_prompt_template_name' : [config.comet_prompt_template_name] * config.batch_size,\n",
    "                \"comet_prompt_version\" : [config.comet_prompt_version] * config.batch_size\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #transform batch dict, dict[collumn para, list] to store a DS where each element is a record,\n",
    "            #i.e. list[record in batch]\n",
    "\n",
    "            def from_batch_to_records_struct(batch):\n",
    "                \"\"\"\n",
    "                Args:\n",
    "                    batch: dict of parameters, where each param is constituted of a data struct of size batch\n",
    "                returns:\n",
    "                    list of records inside batch\n",
    "                \"\"\"\n",
    "                return [\n",
    "                    {\n",
    "                       'dataset_name' : batch['dataset_name'][i],\n",
    "                       'record_id' : batch['records_id'][i],\n",
    "                       'prompt_template_name' : batch['comet_prompt_template_name'][i],\n",
    "                       \"prompt_version\" : batch['comet_prompt_version'][i],\n",
    "                       'prompt' : batch['prompts'][i],\n",
    "                       'prediction' : batch['predictions'][i],\n",
    "                       'label' : batch['labels'][i],\n",
    "                       'language' : batch['languages'][i]\n",
    "                    }\n",
    "                    for i in range(config.batch_size)\n",
    "                ]\n",
    "            \n",
    "            #store record outputs in global container\n",
    "            global_output.extend(from_batch_to_records_struct(batch))\n",
    "\n",
    "\n",
    "            #clear GPU cache\n",
    "            torch.cuda.empty_cache()                \n",
    "    except KeyboardInterrupt as k:\n",
    "        print(k)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        return {\n",
    "            'output' : global_output,\n",
    "            'last_idx' : i\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pipeline(opik_client,model, tokenizer, num_datasets : int,config : InferenceArgs,\n",
    "                       resume : Optional[list[dict[str,dict_vals_datatype]]] = None):\n",
    "    \n",
    "    global_ouput = []\n",
    "\n",
    "    \"\"\"     resume_dataset_id = 0\n",
    "    resume_last_dict = None\n",
    "    if resume:\n",
    "        global_ouput = resume\n",
    "        resume_dataset_id = len(resume)\n",
    "        resume_last_dict = resume.pop() \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for dataset_id in range(1,num_datasets+1):\n",
    "        \"\"\"         if dataset_id < resume_dataset_id:\n",
    "            continue \"\"\"\n",
    "\n",
    "        #get dataset from commet ML\n",
    "        dataset_name = f\"{config.comet_dataset_name}-{dataset_id}\"\n",
    "        dataset = opik_client.get_or_create_dataset(dataset_name).to_pandas()\n",
    "\n",
    "        #construct Dataset and Dataloader\n",
    "        dataset = ChatbotDataset(dataset)\n",
    "        dataloader = ChatbotDataloader(tokenizer=tokenizer,dataset=dataset,batch_size=config.batch_size)\n",
    "\n",
    "        #run inference per dataset inside function...\n",
    "        output = model_inference(dataset_name,model,dataloader,config)\n",
    "\n",
    "        #store outputs from current dataset in the specified project from comet ML\n",
    "\n",
    "        def store_results_in_project_comet_ml(output):\n",
    "            for i,record in enumerate(output):\n",
    "                trace_dict = {\n",
    "                    \"comet_dataset_name\" : record['dataset_name'],\n",
    "                    \"comet_prompt_template_name\" : record['prompt_template_name'],\n",
    "                    \"comet_prompt_version\" : record['prompt_version']\n",
    "                }\n",
    "                \n",
    "                trace = opik_client.trace(\n",
    "                    name=f\"record_results:{record['record_id']}\",\n",
    "                    metadata=trace_dict\n",
    "                )\n",
    "\n",
    "                # Add llm call\n",
    "                trace.span(\n",
    "                    name=\"llm call\",\n",
    "                    input={'prompt' : record['prompt']},\n",
    "                    output={'response' : record['prediction']},\n",
    "                    metadata={'model' : config.model_name, 'label' : record['label'], 'language' : record['language']}\n",
    "                )\n",
    "\n",
    "                trace.end()\n",
    "\n",
    "        store_results_in_project_comet_ml(output['output'])\n",
    "        \n",
    "        \"\"\"         n = len(dataset)\n",
    "\n",
    "        if resume_last_dict:\n",
    "            outputs = model_inference(dataset_id,dataset,config,server_client,\n",
    "                                      resume_last_dict)\n",
    "            resume_last_dict = None\n",
    "        else:\n",
    "            outputs = model_inference(dataset_id,dataset,config,server_client) \"\"\"\n",
    "\n",
    "        global_ouput.append(output)\n",
    "\n",
    "        \"\"\"         if outputs['last_idx'] < n - 1:\n",
    "            print(f\"Error during batch datasets inferencing...\")\n",
    "            return global_ouput \"\"\"\n",
    "    return global_ouput\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\n",
    "\"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\",padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_cache=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.apply_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Inference Pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute Inference Pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: TJUNLP/FuxiTranyu-8B-SFT inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: unsloth/gemma-2b-it-bnb-int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config.model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add special tokens and resize vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['<|im_start|>','<|im_end|>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JINJA_TEMPLATE = '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Gemma. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Gemma. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'\n",
    "\n",
    "tokenizer.chat_template = JINJA_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.9: Fast Qwen2 patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4060 Laptop GPU. Max memory: 7.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config.model_name,\n",
    "    #max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   0%|          | 0/234 [00:00<?, ?it/s]\u001b[32m2025-02-19 11:48:11.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 0. Max Batch Input tokens size : 3843\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   0%|          | 1/234 [01:42<6:37:04, 102.25s/it]\u001b[32m2025-02-19 11:49:53.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 1. Max Batch Input tokens size : 2732\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   1%|          | 2/234 [03:16<6:16:02, 97.25s/it] \u001b[32m2025-02-19 11:51:27.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 2. Max Batch Input tokens size : 955\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   1%|â–         | 3/234 [04:50<6:09:12, 95.90s/it]\u001b[32m2025-02-19 11:53:01.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 3. Max Batch Input tokens size : 1719\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   2%|â–         | 4/234 [06:24<6:05:04, 95.24s/it]\u001b[32m2025-02-19 11:54:35.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 4. Max Batch Input tokens size : 2147\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   2%|â–         | 5/234 [07:57<6:00:25, 94.43s/it]\u001b[32m2025-02-19 11:56:08.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 5. Max Batch Input tokens size : 1886\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   3%|â–Ž         | 6/234 [09:31<5:58:16, 94.28s/it]\u001b[32m2025-02-19 11:57:42.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmodel_inference\u001b[0m:\u001b[36m33\u001b[0m - \u001b[1mBatch: 6. Max Batch Input tokens size : 7418\u001b[0m\n",
      "Dataset : multilingual-chatbot-arena-v0.2.1-train-1 - Model Inference:   3%|â–Ž         | 6/234 [09:31<6:02:03, 95.28s/it]\n",
      "OPIK: Started logging traces to the \"inference-wsdm-cup-v5-deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" project at https://www.comet.com/opik/kevinmedinag96/redirect/projects?name=inference-wsdm-cup-v5-deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA driver error: out of memory\n"
     ]
    }
   ],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B inference pipeline for dataset v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = inference_pipeline(opik_client,model,tokenizer,1,config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B inference pipeline for dataset: v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to beam cloud GPU rental services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beam import endpoint\n",
    "\n",
    "\n",
    "@endpoint(gpu=\"A100-40\")\n",
    "def handler():\n",
    "    # Prints the available GPU drivers\n",
    "    import subprocess\n",
    "    print(subprocess.check_output([\"nvidia-smi\"], shell=True))\n",
    "\n",
    "    return {\"gpu\":\"true\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I am going to compare the performance of the different open source models to a variety of different datasets.\n",
    "\n",
    "Metrics:\n",
    "* Accurracy\n",
    "* Precision\n",
    "* Recall\n",
    "* f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 27), match='best response is response a'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r'best response[\\s\\S]*[ab]{1}'\n",
    "s = \"best response is response a\"\n",
    "re.search(pattern,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'model_a' : 1,\n",
    "    'model_b' : 0\n",
    "}\n",
    "id2label = {\n",
    "    1 : 'model_a',\n",
    "    0 : 'model_b'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output_llm(response) -> Union[str,int]:\n",
    "    \"\"\"\n",
    "    Retrieves a list specifying which of the two paired models in each training record adheres the best\n",
    "    to human responses.\n",
    "\n",
    "    Args:\n",
    "        responses List[str]: Batch of LLM's responses.\n",
    "    \"\"\"\n",
    "    #if model_abAB or Reponse Aa reponse bB are the last chars in response, then i would keep those, if not then ill keep the first\n",
    "    #occurrence\n",
    "    expression = r'</think>'#r'([mM]odel[_\\s][abAB][\\W]?)|([rR]esponse [abAB][\\W])'#r'model.+[a,b,A,B]'\n",
    "\n",
    "\n",
    "    #Extract pattern from response\n",
    "    match = re.search(expression, response) \n",
    "    #print(matches)\n",
    "\n",
    "    def get_model_winner(match,response : str) -> Union[str,int]:\n",
    "        \"\"\"\n",
    "        Extract which model's reponse is better from input_response\n",
    "        \"\"\"\n",
    "        #now lets extract the last match based on regexp\n",
    "        #if the last match corresponds to the last chars in response, then it will be the output to processed\n",
    "        #if not, then probably the first match is going to be the best model\n",
    "\n",
    "        def eval_string(string):\n",
    "            if 'a' in string:\n",
    "                return 1\n",
    "            return 0 #response B / model_b\n",
    "\n",
    "        if not match:\n",
    "            return response\n",
    "        \n",
    "        response = response[match.end():].strip().lower()\n",
    "\n",
    "        #conditions on finding model's response:\n",
    "        #1.- find tag: <ans>answer here</ans> or **answer here**\n",
    "        #2.- find: Response [ABab] ... better\n",
    "        #3.- best reponse ... [ab]\n",
    "        #4.- ans ... response [ab]\n",
    "        #5.- model_[ab] or _[ab]\n",
    "\n",
    "\n",
    "        pattern = r'(<ans>|\\*\\*){1}[\\s\\S]*(</ans>|\\*\\*){1}'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            return eval_string(response[match.start():match.end()])\n",
    "\n",
    "        #try finding pattern 2\n",
    "\n",
    "        pattern = r'response[\\s]*[ab]{1}[\\s\\S]*better'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            pattern1 = r'response[\\s]*[ab]{1}'\n",
    "            response1 = response[match.start():match.end()]\n",
    "            match1 = re.search(pattern1,response1)\n",
    "            return eval_string(response1[match1.start():match1.end()])\n",
    "                \n",
    "        #pattern 3\n",
    "\n",
    "        pattern = r'best response[\\s\\S]*[ab]{1}'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            pattern1 = r'response[\\s]*[ab]{1}'\n",
    "            response1 = response[match.start():match.end()]\n",
    "            match1 = re.search(pattern1,response1)\n",
    "            return eval_string(response1[match1.start():match1.end()])\n",
    "        \n",
    "        #pattern 4\n",
    "\n",
    "        pattern = r'ans[\\s]*response [ab]{1}'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            pattern1 = r'response [ab]{1}'\n",
    "            response1 = response[match.start():match.end()]\n",
    "            match1 = re.search(pattern1,response1)\n",
    "            return eval_string(response1[match1.start():match1.end()])\n",
    "        \n",
    "        #pattern 5\n",
    "\n",
    "        pattern = r'(model)?_[ab]{1}'\n",
    "\n",
    "        match = re.search(pattern,response)\n",
    "\n",
    "        if match:\n",
    "            return eval_string(response[match.start():match.end()])\n",
    "        \n",
    "        #if patter not found return answer\n",
    "        return response\n",
    "\n",
    "\n",
    "    return get_model_winner(match,response)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPerformance:\n",
    "    accuracy : float\n",
    "    precision : Optional[float] = None\n",
    "    recall : Optional[float] = None\n",
    "    f1_score : Optional[float] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(opik_client):\n",
    "    # get traces\n",
    "    traces = opik_client.search_traces()\n",
    "    data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for i,trace in enumerate(traces):\n",
    "        span = opik_client.search_spans(trace_id=trace.id)[0]\n",
    "        label: int = label2id[span.metadata[\"label\"]]\n",
    "        \n",
    "        prediction = span.output[\"response\"]\n",
    "        #regexp prediction to extract best model's response\n",
    "        prediction = parse_output_llm(prediction)\n",
    "\n",
    "        if isinstance(prediction,str):\n",
    "            struct_name = 'think_pattern_not_found'\n",
    "        else:\n",
    "            struct_name = 'correct'\n",
    "\n",
    "        data[struct_name][\"predictions\"].append(prediction)\n",
    "        data[struct_name][\"labels\"].append(label)\n",
    "\n",
    "    return data\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name='inference-wsdm-cup-v0-qwen2.5-1.5b-int4',\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qwen_2_5_instruct_1_5b_int4_v0 = benchmark_model(opik_client)\n",
    "\n",
    "\n",
    "performance_qwen_2_5_instruct_1_5b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_instruct_1_5b_int4_v0[\"labels\"],\n",
    "                                                                        data_qwen_2_5_instruct_1_5b_int4_v0[\"predictions\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_qwen_2_5_instruct_1_5b_int4_v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4 inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name='inference-wsdm-cup-v0-qwen2.5-coder-7b-instruct-int4',\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_qwen_2_5_coder_7b_int4_v0 = benchmark_model(opik_client)\n",
    "\n",
    "\n",
    "performance_qwen_2_5_coder_7b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_coder_7b_int4_v0[\"labels\"],\n",
    "                                                                        data_qwen_2_5_coder_7b_int4_v0[\"predictions\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_qwen_2_5_coder_7b_int4_v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit inference pipeline for dataset: v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name=os.environ['COMET_PROJECT_NAME'],\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_deepseek_qwen_v0 = benchmark_model(opik_client)\n",
    "data_deepseek_qwen_v0[\"labels\"].pop()\n",
    "data_deepseek_qwen_v0[\"labels\"].pop()\n",
    "data_deepseek_qwen_v0[\"predictions\"].pop()\n",
    "data_deepseek_qwen_v0[\"predictions\"].pop()\n",
    "\n",
    "performance_deepseek_qwen_v0 = ModelPerformance(accuracy=accuracy_score(data_deepseek_qwen_v0[\"labels\"],data_deepseek_qwen_v0[\"predictions\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_deepseek_qwen_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(data_deepseek_qwen_v0['predictions'])\n",
    "_,count = np.unique(arr,return_counts=True)\n",
    "count / arr.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(data_deepseek_qwen_v0['labels'])\n",
    "_,count = np.unique(arr,return_counts=True)\n",
    "count / arr.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model results: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B inference pipeline for dataset: v0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name=os.environ['COMET_PROJECT_NAME'],\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' performance_qwen_2_5_coder_7b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_coder_7b_int4_v0[\"labels\"],\\n                                                                        data_qwen_2_5_coder_7b_int4_v0[\"predictions\"])) '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_deepseek_r1_qwen1_5b_v0_2_1 = benchmark_model(opik_client)\n",
    "\n",
    "\n",
    "\"\"\" performance_qwen_2_5_coder_7b_int4_v0 = ModelPerformance(accuracy=accuracy_score(data_qwen_2_5_coder_7b_int4_v0[\"labels\"],\n",
    "                                                                        data_qwen_2_5_coder_7b_int4_v0[\"predictions\"])) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'predictions': ['**MegvÃ©tÃ¶sek Ã©s megvÃ¡lasÃ­tÃ¡s:**\\n\\n**1. Az integrÃ¡lt Ã¶kolÃ³gia Ã©s AQAL modell:**\\n   - Az integrÃ¡lt Ã¶kolÃ³gia fontossÃ¡gÃ¡t Ã©s hatÃ©kony mÅ±kÃ¶dÃ©se Ã©rdekÃ©ben fontos az egyÃ©ni, kollektÃ­v, belsÅ‘ Ã©s kÃ¶rnyezett dimenziÃ³k. A hÃ¡romszintÅ± szervezett felÃ¼letÃ©ben a stratosiai, operatÃ­v Ã©s hatÃ³sÃ¡gi szintek egyÃ¼ttmÅ±kÃ¶dÃ©s hozzÃ¡jÃ¡rul az hatÃ©kony mÅ±kÃ¶dÃ©s, amelyet a termÃ©szetvÃ©delmi integrÃ¡lt folyamok megfelelÅ‘s megfelelÅ‘sÃ­tÃ¡sÃ¡val biztosÃ­tjuk.\\n\\n**2. Az \"Ã–kolÃ³giai megtÃ©rÃ©s\" kapcsolatok:**\\n   - Az \"Ã¶kolÃ³giai megtÃ©rÃ©s\" szÃ¼ksÃ©gessÃ©gÃ©t hangszÃ³ a termÃ©szetvÃ©delmi folyamatok megÅ‘rzÃ©sÃ©re, a kÃ¼lÃ¶nbÃ¶zÅ‘ szervezetek kÃ¶zÃ¶tti egyÃ¼ttmÅ±kÃ¶dÃ©s, Ã©s a termÃ©szetvÃ©delmi integrÃ¡lt folyamok megfelelÅ‘s megfelelÅ‘sÃ­tÃ¡sÃ¡val. A kapcsolatok egÃ©szÃ©ben azok az ismeret, ismeret, helyelet, fajok, rÃ©sztÃ©p, Ã©rtÃ©ke, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt.\\n\\n**3. Az \"Ãºj termÃ©szetvÃ©delmi tudomÃ¡ny\" kapcsolatok:**\\n   - A \"Ãºj termÃ©szetvÃ©delmi tudomÃ¡ny\" kapcsolatok egÃ©szÃ©ben azok az ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt, ismeret, hely, ismeret, tÃ¶rtelet, dÃ¶vÃ¶ny, szervezÃ©s, halmaz, fÃ¼ggetiszt.\\n\\n**4. A termÃ©szetvÃ©delem hatÃ©kony mÅ±kÃ¶dÃ©sÃ©nek megfelelÅ‘s Ã©s javÃ­tÃ¡sok:**\\n   - A termÃ©szetvÃ©delem hatÃ©kony mÅ±kÃ¶dÃ©sÃ©ben fontos az egyÃ©ni, kollektÃ­v, belsÅ‘ Ã©s kÃ¶rnyezett szintek Ã©s szervezett felÃ¼letek kÃ¶zÃ¶tti egyÃ¼ttmÅ±kÃ¶dÃ©s, Ã©s a termÃ©szetvÃ©delmi integrÃ¡lt folyamokok megfelelÅ‘s megfelelÅ‘sÃ­tÃ¡sÃ¡val. A hÃ¡romszintÅ± szervezett felÃ¼letÃ©n a szervezett fÃ¼ggvÃ©nyeinek hozzÃ¡jÃ¡rul az hatÃ©kony mÅ±kÃ¶dÃ©s, a termÃ©szetvÃ©delmi integrÃ¡lt folyamokok megfelelÅ‘s megfelelÅ‘sÃ­tÃ¡sÃ¡val. A termÃ©szetvÃ©delmi integrÃ¡lt folyamokok hozzÃ¡jÃ¡rul az hatÃ©kony mÅ±kÃ¶dÃ©shez, mÃ­g a kooperÃ¡ciÃ³ Ã©s a finanszÃ­rozÃ¡s javÃ­tÃ¡s javtalanak.\\n\\n**KunklÃºziÃ³:**\\n   - Az Ã¶kolÃ³giai vÃ¡lsÃ¡g kezelÃ©se szÃ©les kÃ¶rben Ã©rinti a spirituÃ¡lis Ã©s vallÃ¡si aspektusokat, melyek a Katolikus EgyhÃ¡zban is jelentÅ‘s szerepet jÃ¡tszanak. Szent Ferenc Ã¶rÃ¶ksÃ©ge Ã©s Ferenc pÃ¡pa tanÃ­tÃ¡saival egyÃ¼tt a \"Ã¶kolÃ³giai megtÃ©rÃ©s\" elÃ©rhetÅ‘vÃ© vÃ¡lik, amely magÃ¡ban foglaljuk a tÃ¡rsadalmi igazsÃ¡gossÃ¡g Ã©s a szolidaritÃ¡s elkoteleket. Az integrÃ¡lt Ã¶kolÃ³gia Ã©s az AQAL modell segÃ­t a komplex kÃ¶rnyezeti kÃ©rdÃ©sek Ã¡tfogÃ³ megÃ©rtÃ©sÃ©ben, mÃ­g a termÃ©szetvÃ©delmi stratÃ©giÃ¡k fejlÅ‘dÃ©se Ã©s a hÃ¡romszintÅ± szervezett felÃ¼letÃ©n a szervezett fÃ¼ggvÃ©nyeinek hozzÃ¡jÃ¡rul az hatÃ©kony mÅ±kÃ¶dÃ©shez. A termÃ©szetvÃ©delmi integrÃ¡lt folyamokok megfelelÅ‘s megfelelÅ‘sÃ­tÃ¡sÃ¡val a termÃ©szetvÃ©delem hatÃ©kony mÅ±kÃ¶dÃ©shez jÃ³lÃ­tik az hatÃ©kony mÅ±kÃ¶dÃ©s, Ã©s a kooperÃ¡ciÃ³ Ã©s a finanszÃ­rozÃ¡s javtalanak.\\n\\n**VÃ©lemek:**\\n1. **Az \"Ã–kolÃ³giai VÃ¡lsÃ¡g\" rÃ©sze:**\\n   - Az Ã¶kolÃ³giai vÃ¡lsÃ¡g nem csak termÃ©szeti, hanem spirituÃ¡lis Ã©s vallÃ¡si gyÃ¶kerekkel rendelkezik.\\n   - A Katolikus EgyhÃ¡z tÃ¡mogatja a \"Ã¶kolÃ³giai megtÃ©rÃ©s\", amely segÃ­thet az emberisÃ©gnek elmozdulni a termÃ©szetvÃ©delmi vilÃ¡ghoz, Ã©s felelÅ‘s megfelelÅ‘sÃ©get vÃ¡lik.\\n\\n2. **Szent Ferenc Ã–rÃ¶ksÃ©ge:**\\n   - Szent Ferenc Ã©lete Ã©s tanÃ­tÃ¡saiban megjelennek nÃ©gy fÅ‘ elem: Istennel valÃ³ kapcsolat, szemÃ©lykÃ¶zi kapcsolatok, teremtett vilÃ¡g gondozÃ¡sa, Ã©s Ã¶nmagunk talÃ¡lÃ³.\\n   - Ferenc pÃ¡pa a \"Laudato Si\\'\" enciklikÃ¡ban hangsÃºlyozva a tÃ¡rsadalmi igazsÃ¡gossÃ¡g Ã©s szolidaritÃ¡s fontossÃ¡gÃ¡t az Ã¶kolÃ³giai vÃ¡lszacs megoldÃ¡sa Ã©rdekÃ©ben.\\n\\n3. **IntegrÃ¡lt Ã–kolÃ³gia Ã©s AQAL modell:**\\n   - Az integrÃ¡lt Ã¶kolÃ³gia fontossÃ¡gÃ¡t Ã©s hatÃ©kony mÅ±kÃ¶dÃ©sÃ©ben fontos az egyÃ©ni, kollektÃ­v, belsÅ‘ Ã©s kÃ¶rnyezett szintek Ã©s szervezett felÃ¼letek. A hÃ¡romszintÅ± szervezett felÃ¼letÃ©n a szervezett fÃ¼ggvÃ©nyeinek hozzÃ¡jÃ¡rul az hatÃ©kony mÅ±kÃ¶dÃ©s, a termÃ©szetvÃ©delmi integrÃ¡lt folyamokok megfelelÅ‘s megfelelÅ‘sÃ­tÃ¡sÃ¡val. A hÃ¡romszintÅ± szervezett felÃ¼letÃ©n a szervezett fÃ¼ggvÃ©nyeinek hozzÃ¡jÃ¡rul az hatÃ©kony mÅ±kÃ¶dÃ©s, Ã©s a kooperÃ¡ciÃ³ Ã©s a finanszÃ­rozÃ¡s javtalanak.\\n\\n4. **TermÃ©szetvÃ©delmi stratÃ©giÃ¡k:**\\n   - A termÃ©szetvÃ©delmi stratÃ©giÃ¡k megfelelÅ‘s megfelelÅ‘sÃ­tÃ¡sÃ¡val a termÃ©szetvÃ©delem hatÃ©kony mÅ±kÃ¶dÃ©shez jÃ³lÃ­tik az hatÃ©kony mÅ±kÃ¶dÃ©s, Ã©s a kooperÃ¡ciÃ³ Ã©s a finanszÃ­rozÃ¡s javtalanak.\\n\\n5. **TermÃ©szetvÃ©delmi stratÃ©giÃ¡k fejlÅ‘dÃ©se:**\\n   - A termÃ©szetvÃ©delmi stratÃ©giÃ¡k megfelelÅ‘s megfelelÅ‘sÃ­tÃ¡sÃ¡val a termÃ©szetvÃ©delem hatÃ©kony mÅ±kÃ¶dÃ©shez jÃ³lÃ­tik az hatÃ©kony mÅ±kÃ¶dÃ©s, Ã©s a kooperÃ¡ciÃ³ Ã©s a finanszÃ­rozÃ¡s javtalanak.\\n\\n6. **TermÃ©szetvÃ©delmi stratÃ©giÃ¡k javtalanak:**\\n   - A termÃ©szetvÃ©delmi stratÃ©giÃ¡k fejlÅ‘dÃ©sek javtalanak, mivel termeszettes tÃ¡rsadalmi asztricit Ã©s tÃ¡rsadalmi Ã©rtÃ©keket biztosÃ­tanak.\\n\\n**VÃ©lemek:**\\n1. **Az \"TermÃ©szetvÃ©delem hatÃ©kony mÅ±kÃ¶dÃ©sÃ©re javtalanak:**\\n   - A termÃ©szetvÃ©delmi integrÃ¡lt folyamokok hozzÃ¡jÃ¡rul az hatÃ©kony mÅ±kÃ¶dÃ©shez, mivel termeszettes tÃ¡rsadalmi asztricit Ã©s tÃ¡rsadalmi Ã©rtÃ©keket biztosÃ­tanak.\\n\\n2. **A kooperÃ¡ciÃ³ Ã©s a finanszÃ­rozÃ¡s javtalanak:**\\n   - A kooperÃ¡ciÃ³ Ã©s a finanszÃ­rozÃ¡s javtalanak, mivel a termÃ©szetvÃ©delmi integrÃ¡lt folyamokok hozzÃ¡jÃ¡rul az hatÃ©kony mÅ±kÃ¶dÃ©shez, mivel termeszettes tÃ¡rsadalmi asztricit Ã©s tÃ¡rsadalmi Ã©rtÃ©keket biztosÃ­tanak.\\n\\n3. **Az AQAL modell segÃ­t a komplex kÃ¶rnyezeti kÃ©rdÃ©kek Ã¡tfogÃ³ megÃ©rt',\n",
       "              'Okay, so I need to figure out which response is better between Response A and Response B based on factors like clarity, relevance, tone, and overall quality. Let me start by reading both responses carefully.\\n\\nResponse A is \"Hello! ðŸ‘‹ How can I help you today? ðŸ˜Š\". I notice that it starts with \"Hello!\" and uses emojis. The tone seems friendly and approachable, which is good for a human. The use of emojis might make it seem more natural, but I\\'m not sure if that\\'s the main thing to focus on here.\\n\\nResponse B is \"Hi there! How can I help you today?\" It starts with \"Hi there!\" which is a common greeting, and it\\'s also friendly. The tone here is a bit more formal, which might be better for a more serious context.\\n\\nNow, considering the main factors: clarity. Both responses are clear, but Response B uses \"Hi there!\" which is a bit more formal. However, clarity isn\\'t the main issue here.\\n\\nNext, relevance. Both responses are relevant to the same question. The main difference is in the tone. Since the prompt didn\\'t specify any particular tone, but generally, responses should be natural and conversational.\\n\\nTone is important. A more natural tone would be better for a human. Response A uses a friendly emoji, which is a good approach, but Response B is a bit more formal, which might be better for a more serious or formal context.\\n\\nOverall quality: Both are good, but Response A might be a bit more approachable and less formal, which could be better for a human to read and understand.\\n\\nWait, but the prompt didn\\'t specify any context. So, if I have to choose based solely on factors mentioned, the tone is the most important. So, Response A\\'s friendly tone might be better than Response B\\'s formal tone.\\n\\nBut wait, the user just asked me to choose between the two, so maybe I should look at both aspects. The main factors are clarity, relevance, tone, and quality.\\n\\nClarity: Both are clear.\\n\\nRelevance: Both are relevant.\\n\\nTone: Response A is more approachable, Response B is more formal.\\n\\nQuality: Both are good, but maybe Response A is a bit more approachable, which is better for a human.\\n\\nSo, considering all factors, Response A might be better because it\\'s more approachable and less formal, which would be better for a human to read and understand.\\n\\nWait, but sometimes humans prefer more formal language. Hmm, I\\'m a bit conflicted here.\\n\\nWait, the user didn\\'t specify any context, so I should go with the most approachable tone, which is Response A.\\n\\nBut wait, the initial prompt didn\\'t mention any context, so maybe I should consider both.\\n\\nAlternatively, maybe the user wants the most natural and approachable response, so Response A.\\n\\nBut I\\'m not entirely sure. I think Response A is better because it\\'s more approachable, which is likely what humans prefer.\\n\\nWait, but sometimes people prefer more formal responses. I\\'m a bit stuck here.\\n\\nWait, perhaps the best response is Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nAlternatively, Response B is more formal, which might be better for a more serious context.\\n\\nBut since the prompt doesn\\'t specify, I think Response A is better because it\\'s more approachable, which is likely what humans prefer.\\n\\nWait, but I\\'m not entirely sure. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, no, maybe I should consider that humans prefer more natural and approachable responses, so Response A is better.\\n\\nBut I\\'m not confident. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. Maybe I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I\\'m a bit stuck. I think I\\'ll go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nBut I\\'m not entirely sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nWait, but the user didn\\'t specify any context, so maybe I should choose the most natural and approachable response.\\n\\nAlternatively, maybe Response B is more formal, which is better for a serious context.\\n\\nI think I should go with Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nWait, but the user just wants the best response based on the factors, so I should choose the response that is more approachable and less formal, which is Response A.\\n\\nBut I\\'m not sure. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I\\'m stuck. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. Maybe I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I\\'ll go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. Maybe I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI\\'m stuck. I think I should choose Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. Maybe I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI\\'m stuck. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI\\'m stuck. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nHmm, I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI\\'m stuck. I think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not sure. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait, but I\\'m not confident. I think I should choose Response B because it\\'s more formal, which might be better for a more serious context.\\n\\nI think I should go with Response A because it\\'s more approachable and less formal, which is better for a human to read and understand.\\n\\nWait',\n",
       "              'Ð”Ð¾Ð±Ñ€Ðµ, ÑÐ¿Ð¾Ñ‡Ð°Ñ‚ÐºÑƒ Ñ Ñ€Ð¾Ð·Ð³Ð»ÑÐ½ÑƒÑ‚Ðµ Ñ†ÑŽ Ð¿ÐµÑ€ÐµÐ»Ñ–ÐºÐ½Ñƒ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ, ÑÐºÑƒ Ð¹Ð¾Ð³Ð¾ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ²Ð°Ð² ÐœÐ°ÐºÑÐ¸Ð¼. Ð¦Ñ Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ Ñ†Ðµ Ð¿Ñ€Ð¾ Ð¿Ð¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð½Ñ Ñ–Ð»ÑŽÐ·Ñ–Ð¾Ð½Ñ–ÑÑ‚Ñ–, Ñ‰Ð¾ Ð·Ð¼ÐµÐ½ÑˆÑƒÑ”Ñ‚ÑŒÑÑ Ñ‡ÐµÑ€ÐµÐ· Ð¾Ð¿Ð¸Ñ‚Ð°Ð½Ð½Ñ Ð· Ð´Ð¾ÐºÐ»Ð°Ð´Ð½Ð¾ÑŽ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ”ÑŽ. Ð¦Ðµ Ñ†Ðµ Ð´Ð¾ÑÐ²Ñ–Ð´ Ñ–Ð· Ñ„Ð°Ð½Ñ‚Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð¸Ð¼ ÐµÑ„ÐµÐºÑ‚Ð¾Ð¼, Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ð¹Ð½Ñ–ÑÑ‚ÑŽ, ÑÐºÑƒ Ð¹Ð¾Ð³Ð¾ Ð¿Ð¾Ð³Ð»ÑÐ´Ð°Ñ”.\\n\\nÐŸÐµÑ€ÑˆÐ° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð° Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ—: ÐœÐ°ÐºÑÐ¸Ð¼ Ð²Ð·ÑÑ” Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð²Ñ–Ð´Ñ‡ÑƒÐ²Ð°Ñ” Ñ‰Ð¾ Ñ” Ñ‡Ð¾Ñ€Ð½Ð° Ñ…Ð¼Ð°Ñ€Ð°. Ð’Ñ–Ð½ Ð·Ð¼Ñ–Ð½ÑŽÑ” Ñ—Ñ—, Ð¹Ð¾Ð³Ð¾ Ð¿Ð°Ð»ÑŒÑ†Ñ– Ð½ÐµÑ€Ð²Ð¾Ð²Ð¾ Ð¿Ð¾Ð³Ð»Ð°Ð´ÑƒÑŽÑ‚ÑŒ Ð½Ð° Ð½Ð°Ñ‚ÑÐ¶ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð¿Ð¾Ð³Ð»ÑÐ´ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑŽÐ²Ð°Ñ‚Ð¸ Ñ‚Ð° Ð²Ñ–Ð´Ñ‡Ð°Ñ‚Ð°ÑŽÑ‚ÑŒÑÑ. Ð¦Ðµ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° Ð´ÐµÑ‚ÐµÐºÑ‚Ð¸Ð²Ð½Ñ–ÑÑ‚ÑŒ.\\n\\nÐ”Ð°Ð»Ñ–, ÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ñ‰Ð¾ Ð·Ð´Ð°Ñ”Ñ‚ÑŒÑÑ Ð¼Ð°Ñ” Ð±Ñ–Ð»ÑŒÑˆÐµ Ð¼Ñ–Ð¶Ñ„Ñ–Ð»Ð´Ñ–Ð², Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑŽÐ²Ð°Ñ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐ”Ñ€ÑƒÐ³Ð° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°: ÐœÐ°ÐºÑÐ¸Ð¼ Ð¿Ð¾Ð³Ð»ÑÐ´Ð°Ñ” Ð½Ð° Ð²Ñ–Ð·Ð¸Ñ‚ Ð´Ð¾ Ð¼Ñ–ÑÑ‚Ð°, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±Ñ– Ð· Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑŽÐ²Ð°Ñ‚Ð¸ Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ– Ð½Ñ–Ð¶ÐºÐ¸ Ð¿Ñ–Ð´Ñ…Ñ–Ð´ Ð¼Ð¾Ð¶ÑƒÑ‚ÑŒ Ð±ÑƒÑ‚Ð¸ Ð²Ñ–Ð´Ð¾Ð¼Ñ–ÑˆÑ–ÑˆÑ–.\\n\\nÐ¢Ñ€ÐµÑ‚Ñ Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°: ÐœÐ°ÐºÑÐ¸Ð¼ Ð¿Ñ–Ð´Ñ…Ñ–Ð´Ð½Ð¸Ð¹ Ð´Ð¾ Ð²Ñ–Ñ‚ÐµÑ€Ð°, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ñ†Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–ÑŽ Ð¾Ñ…Ñ–Ð´Ñ†Ñ–Ð².\\n\\nÐ§ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð°ÑˆÐ° Ñ‡Ð°ÑÑ‚Ð¸Ð½Ð°: ÐœÐ°ÐºÑÐ¸Ð¼ Ñ€Ð¾Ð±Ð¸Ñ‚ÑŒ ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð²Ñ–Ð´Ñ‡ÑƒÐ²Ð°Ñ” Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑŽÐ²Ð°Ñ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑŽÐ²Ð°Ñ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐŸÑ–ÑÐ»Ñ Ñ†ÑŒÐ¾Ð³Ð¾, ÐœÐ°ÐºÑÐ¸Ð¼ Ð·ÑƒÐ¿Ð¸Ð½Ð¸Ð²ÑÑ Ñƒ Ð¿Ð°Ð¿ÐºÑƒ, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð¿Ñ–Ð´Ñ…Ñ–Ð´Ð½Ð¸Ð¹ Ð´Ð¾ Ð¼Ñ–ÑÑ‚Ð°. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¿Ñ–Ð´Ñ…Ñ–Ð´ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´ Ð´Ð¾ Ð¼Ñ–ÑÑ‚Ð°.\\n\\nÐžÐ´Ð½Ñ–Ð¹ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ð¹Ð½Ð¸Ð¹ Ð´Ñ–Ð°Ð»Ð¾Ð³: ÐœÐ°ÐºÑÐ¸Ð¼ ÑÐ¿Ð¾Ð´Ñ–Ð²Ð°Ñ”Ñ‚ÑŒÑÑ, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑŽÐ²Ð°Ñ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐŸÑ–ÑÐ»Ñ Ñ†ÑŒÐ¾Ð³Ð¾, ÐœÐ°ÐºÑÐ¸Ð¼ ÐºÐ¸Ð²Ð½ÑƒÑ”, Ð¹Ð¾Ð³Ð¾ Ð¾Ð±Ð»Ð¸Ñ‡Ñ‡Ñ Ð±ÑƒÐ»Ð¾ Ð½Ð°Ð¿Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ð¼, Ñ– Ð½Ñ–Ñ‡ Ð·Ð´Ð°Ñ”Ñ‚ÑŒÑÑ, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸Ñ‚ÑŒÑÑ.\\n\\nÐ’Ñ–Ð½ Ð²Ñ–Ð´ÐºÑ€Ð¸Ð² Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´Ð½Ð¸Ð¹ Ð´Ð¾ Ð¼Ñ–ÑÑ‚Ð°, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ñ†Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¿Ñ–Ð´Ñ…Ñ–Ð´ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´ Ð´Ð¾ Ð¼Ñ–ÑÑ‚Ð°.\\n\\nÐŸÑ–ÑÐ»Ñ Ñ†ÑŒÐ¾Ð³Ð¾, ÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐŸÑ–ÑÐ»Ñ Ñ†ÑŒÐ¾Ð³Ð¾, ÐœÐ°ÐºÑÐ¸Ð¼ ÐºÐ¸Ð²Ð½ÑƒÑ”, Ð¹Ð¾Ð³Ð¾ Ð¾Ð±Ð»Ð¸Ñ‡Ñ‡Ñ Ð±ÑƒÐ»Ð¾ Ð½Ð°Ð¿Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ð¼, Ñ– Ð½Ñ–Ñ‡ Ð·Ð´Ð°Ñ”Ñ‚ÑŒÑÑ, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–Ñ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸Ñ‚ÑŒÑÑ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚Ð¾Ñ€Ñ–ÑŽ.\\n\\nÐœÐ°ÐºÑÐ¸Ð¼ Ð·Ð²ÐµÑ€Ñ‚Ð°Ñ” ÑƒÐ²Ð°Ð³Ñƒ Ð½Ð° ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚, Ð´Ðµ Ð¹Ð¾Ð³Ð¾ Ð±Ð¾Ñ€Ð¾Ñ‚ÑŒÐ±ou Ð·Ñ–ÑÐ½Ð°Ð½Ð½ÑÐ¼, Ñ– Ð²Ñ–Ð½ Ð·Ð±ÐµÑ€Ñ–Ð³Ð°Ñ” ÐºÐ°Ð±Ñ–Ð½ÐµÑ‚ Ñ– Ð¿Ð°Ð¿ÐºÑƒ. Ð¦Ðµ Ð¿Ð¾ÐºÐ°Ð·ÑƒÑ”, Ñ‰Ð¾ Ð¹Ð¾Ð³Ð¾ Ð´Ñ–ÑÑ†Ñ– Ð·Ð¼Ñ–Ð½ÑŽÑŽÑ‚ÑŒ Ð¿Ð°Ð¿ÐºÑƒ Ñ– Ð¿Ñ–Ð´Ñ…Ñ–Ð´, Ñ‰Ð¾ Ð¼Ð¾Ð¶Ðµ Ð·Ð¼Ñ–Ð½ÑƒÑ‚Ð¸ Ð¹Ð¾Ð³Ð¾ Ñ–ÑÑ‚',\n",
       "              'Okay, so I\\'m trying to figure out which response from Response A or Response B is more likely to be preferred by humans. Let me start by reading both responses carefully.\\n\\nResponse A is: \"toki! mi ken toki e toki pona en lawa nimi ante mute. sina wile toki kepeken seme la mi ken pali e ni.\"\\n\\nResponse B is a long, complex English translation starting with \"Mi moku e toki Inli, toki Espania, toki Latiina, toki Han, toki Rusi, toki Hindi, toki Bengali, toki Portugali, toki Nipon, toki Inli putona, toki Inli sina, toki Inli micin, toki Inli micin sina, toki Inli putona micin, toki Inli putona micin sina, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki Inli putona sina micin, toki In'],\n",
       "             'labels': [1, 0, 1, 1]})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_deepseek_r1_qwen1_5b_v0_2_1['think_pattern_not_found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =data_deepseek_r1_qwen1_5b_v0_2_1['correct']['predictions']\n",
    "labels = data_deepseek_r1_qwen1_5b_v0_2_1['correct']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 13, 213]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_arr = np.array(predictions)\n",
    "np.unique(preds_arr,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 97, 129]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_arr = np.array(labels)\n",
    "np.unique(labels_arr,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5398230088495575"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels_arr,preds_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsdm-cup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
