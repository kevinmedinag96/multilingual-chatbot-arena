{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created to test how current open source state-of-the-art LLMs perform for the multilingual-chatbot-arena dataset.\n",
    "\n",
    "The experiment's constraints are as follows:\n",
    "\n",
    "1. Model's inference will be held using an NVIDIA GeForce RTX 4060. Therefore the GPU computing is limited. Recall that is most likely that the current pretrained models with not perform very well on this challenge's data. The benchmarked LLMs must hold around 7-9B parameters in order for the hardware to handle inferencing/fine tuning.\n",
    "2. For the fine tuning of LLMs. The best course of action will be to use QLORA. Due to hardware constraints.\n",
    "3. There are an abundant number of capable open-source LLMs. In this Demo we will be benchmarking 3 model's families: Qwen2.5, Llama 3.X and gemini.\n",
    "4. Dataset for the experiment: training set.\n",
    "5. Performance metric: Accuracy (For the whole dataset, how many prompts what's the proportion of accurately predicted answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading challenge's  data from Comet ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinmg96/miniconda3/envs/multilingual-chatbot-arena/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "\u001b[32m2025-01-24 08:09:49.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mInitializing env vars...\u001b[0m\n",
      "\u001b[32m2025-01-24 08:09:49.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLoading environment variables from: /home/kevinmg96/Kaggle competitions/WSDM Cup/multilingual-chatbot-arena/.env\u001b[0m\n",
      "/home/kevinmg96/miniconda3/envs/multilingual-chatbot-arena/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "root_repo_directory = pathlib.Path().resolve().parent.__str__()\n",
    "sys.path.append(root_repo_directory)\n",
    "from multilingual_chatbot_arena import initialize\n",
    "import datasets_creator.src.constants as c\n",
    "import datasets_creator.src.utils as utils\n",
    "import pandas as pd\n",
    "from fire import Fire\n",
    "from pydantic import BaseModel\n",
    "from typing import List,Optional,Dict,Union\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import opik\n",
    "from loguru import logger\n",
    "initialize()\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opik_client = opik.Opik(project_name=os.environ['COMET_PROJECT_NAME'],\n",
    "        workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TracePublic(id='019498ae-d271-7cde-bde6-e1bbdf0ab224', project_id='01948f5c-3138-7b41-8061-5b1f11b296e4', name='my_trace-2', start_time=datetime.datetime(2025, 1, 24, 14, 19, 41, 554018, tzinfo=TzInfo(UTC)), end_time=datetime.datetime(2025, 1, 24, 14, 19, 41, 555282, tzinfo=TzInfo(UTC)), input={'user_question': 'Hello, how are you?'}, output={'response': 'Comment ça va?'}, metadata=None, tags=None, error_info=None, usage=None, created_at=datetime.datetime(2025, 1, 24, 14, 19, 43, 631524, tzinfo=TzInfo(UTC)), last_updated_at=datetime.datetime(2025, 1, 24, 14, 19, 43, 720692, tzinfo=TzInfo(UTC)), created_by='kevinmedinag96', last_updated_by='kevinmedinag96', feedback_scores=[FeedbackScorePublic(name='coherence', category_name=None, value=0.75, reason=None, source='sdk', created_at=datetime.datetime(2025, 1, 24, 14, 19, 43, 622745, tzinfo=TzInfo(UTC)), last_updated_at=datetime.datetime(2025, 1, 24, 14, 19, 43, 622745, tzinfo=TzInfo(UTC)), created_by='kevinmedinag96', last_updated_by='kevinmedinag96'), FeedbackScorePublic(name='correctness', category_name=None, value=1.0, reason=None, source='sdk', created_at=datetime.datetime(2025, 1, 24, 14, 19, 43, 622745, tzinfo=TzInfo(UTC)), last_updated_at=datetime.datetime(2025, 1, 24, 14, 19, 43, 622745, tzinfo=TzInfo(UTC)), created_by='kevinmedinag96', last_updated_by='kevinmedinag96'), FeedbackScorePublic(name='overall_quality', category_name=None, value=0.85, reason='The response was helpful and accurate.', source='sdk', created_at=datetime.datetime(2025, 1, 24, 14, 19, 43, 622745, tzinfo=TzInfo(UTC)), last_updated_at=datetime.datetime(2025, 1, 24, 14, 19, 43, 622745, tzinfo=TzInfo(UTC)), created_by='kevinmedinag96', last_updated_by='kevinmedinag96')], total_estimated_cost=None, duration=1.264),\n",
       " TracePublic(id='019498a6-09c3-75a3-9866-b48d6fe103c6', project_id='01948f5c-3138-7b41-8061-5b1f11b296e4', name='my_trace', start_time=datetime.datetime(2025, 1, 24, 14, 10, 5, 891478, tzinfo=TzInfo(UTC)), end_time=datetime.datetime(2025, 1, 24, 14, 10, 5, 893646, tzinfo=TzInfo(UTC)), input={'user_question': 'Hello, how are you?'}, output={'response': 'Comment ça va?'}, metadata=None, tags=None, error_info=None, usage=None, created_at=datetime.datetime(2025, 1, 24, 14, 10, 7, 897573, tzinfo=TzInfo(UTC)), last_updated_at=datetime.datetime(2025, 1, 24, 14, 10, 7, 985741, tzinfo=TzInfo(UTC)), created_by='kevinmedinag96', last_updated_by='kevinmedinag96', feedback_scores=None, total_estimated_cost=None, duration=2.168)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opik_client.search_traces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a trace\n",
    "trace = opik_client.trace(\n",
    "    name=\"my_trace-2\",\n",
    "    input=\"Hello, how are you?\",\n",
    "    output={\"response\": \"Comment ça va?\"}\n",
    ")\n",
    "\n",
    "opik_client.log_traces_feedback_scores(\n",
    "    scores=[\n",
    "        {\"id\": trace.id, \"name\": \"overall_quality\", \"value\": 0.85, \"reason\": \"The response was helpful and accurate.\"},\n",
    "        {\"id\": trace.id, \"name\": \"coherence\", \"value\": 0.75},\n",
    "        {\"id\" : trace.id, \"name\" : \"correctness\", \"value\" : 1.0}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Add a span\n",
    "trace.span(\n",
    "    name=\"Add prompt template\",\n",
    "    input={\"text\": \"Hello, how are you?\", \"prompt_template\": \"Translate the following text to French: {text}\"},\n",
    "    output={\"text\": \"Translate the following text to French: hello, how are you?\"}\n",
    ")\n",
    "\n",
    "# Add an LLM call\n",
    "trace.span(\n",
    "    name=\"llm_call\",\n",
    "    type=\"llm\",\n",
    "    input={\"prompt\": \"Translate the following text to French: hello, how are you?\"},\n",
    "    output={\"response\": \"Comment ça va?\"}\n",
    ")\n",
    "\n",
    "# End the trace\n",
    "trace.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'model_a' : 1,\n",
    "    'model_b' : 2\n",
    "}\n",
    "id2label = {\n",
    "    1 : 'model_a',\n",
    "    2 : 'model_b'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom dataset for the imported data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self,data : pd.DataFrame):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get either a single data point or a pandas Dataframe window of data points\n",
    "        data_window = self.data.iloc[idx]    \n",
    "\n",
    "        return data_window.to_dict()\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Setup (Inference API server version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section models inferencing pipelines for each of the benchmark models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accurracy_metric(predictions,labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    unmatched_idxs = [] #incorrectly predicted records idxs\n",
    "    accurracy = 0\n",
    "    for i,(pred,lab) in enumerate(zip(predictions,labels)):\n",
    "        if pred == lab:\n",
    "            accurracy += 1\n",
    "        else:\n",
    "            unmatched_idxs.append(i)\n",
    "\n",
    "    return accurracy / len(predictions), unmatched_idxs\n",
    "\n",
    "\n",
    "def get_model_winner(matches) -> int:\n",
    "    \"\"\"\n",
    "    Extract which model's reponse is better from input_response\n",
    "    \"\"\"\n",
    "    for match in matches:\n",
    "        if 'a' in match:\n",
    "            return 1\n",
    "        elif 'b' in match:\n",
    "            return 2\n",
    "    return 3\n",
    "\n",
    "def parse_output_llm(response) -> int:\n",
    "    \"\"\"\n",
    "    Retrieves a list specifying which of the two paired models in each training record adheres the best\n",
    "    to human responses.\n",
    "\n",
    "    Args:\n",
    "        responses List[str]: Batch of LLM's responses.\n",
    "    \"\"\"\n",
    "    pattern = r'model_[ab]'\n",
    "\n",
    "\n",
    "    #Extract pattern from response\n",
    "    match = re.search(pattern, response)        \n",
    "    \n",
    "    return get_model_winner(match.group(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vals_dataype = Union[list[int],int]\n",
    "def model_inference(id,dataset,config,client,resume : Optional[dict[str,dict_vals_dataype]] = None) -> dict[str,dict_vals_dataype]:\n",
    "    \"\"\"\n",
    "    Retrieves two lists, the first list specifies the LLM's decisions per record, on which response was more humanly\n",
    "    seen. The other specifies the challenge's ground truth.\n",
    "\n",
    "    Args:\n",
    "        model : HuggingFace Pretrained LLM.\n",
    "    \"\"\"\n",
    "    global_output_winners = []\n",
    "    global_answers = []\n",
    "    resume_idx = 0\n",
    "    if resume:\n",
    "        global_output_winners = resume['predictions']\n",
    "        global_answers = resume['answers']\n",
    "        resume_idx = resume['last_idx'] + 1\n",
    "    \n",
    "    try:\n",
    "        i = 0\n",
    "        for i,record in enumerate(tqdm(dataset,desc=f\"Dataset : {id} - Model Inference\")):\n",
    "\n",
    "            if i < resume_idx:\n",
    "                continue\n",
    "\n",
    "            if i % 15 == 0:\n",
    "                time.sleep(60.)\n",
    "                \n",
    "\n",
    "            \"\"\"             messages = [\n",
    "                {'role' : 'system', 'content' : c.SYSTEM_TEMPLATE},\n",
    "                {'role' : 'user',     'content' : record['prompt'] }] \"\"\"\n",
    "            \n",
    "            PROMPT = \"\"\"\n",
    "            {system}{prompt}\n",
    "            \"\"\".format(system=c.SYSTEM_TEMPLATE,prompt=record['prompt'])\n",
    "\n",
    "            server_response = None\n",
    "            try:\n",
    "                \"\"\" server_response = client.chat.completions.create(\n",
    "                        model= config.model_name,\n",
    "                        messages=messages,\n",
    "                        max_tokens=50,\n",
    "                        stream = False\n",
    "                    )\n",
    "                response: str = server_response.choices[0].message.content \"\"\"\n",
    "\n",
    "                \"\"\" input = { \"messages\": messages }\n",
    "                server_response = requests.post(f\"{CLOUDFARE_URL}{config.model_name}\", headers=HEADER, json=input).json()\n",
    "                response = server_response['result']['response'] \"\"\"\n",
    "\n",
    "                server_response = client.generate_content(\n",
    "                    contents= PROMPT,\n",
    "                    stream = False)\n",
    "                \n",
    "                response = server_response.text\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Something has gone wrong requesting from API server...\")\n",
    "                print(e)\n",
    "            finally:\n",
    "                if not server_response:\n",
    "                    response = 'c'\n",
    "\n",
    "        \n",
    "            global_output_winners.append(parse_output_llm(response))\n",
    "            global_answers.append(label2id[record['answer']])\n",
    "            \n",
    "    except KeyboardInterrupt as k:\n",
    "        print(k)\n",
    "    finally:\n",
    "        return {\n",
    "            'predictions' : global_output_winners,\n",
    "            'answers' : global_answers,\n",
    "            'last_idx' : i\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Inference Arguments and saving results into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceArgs:\n",
    "    model_name : str\n",
    "    inference_file_path : str = root_repo_directory\n",
    "    batch_size : Optional[int] = None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceFile:\n",
    "\n",
    "    data : list[dict[str,dict_vals_dataype]]\n",
    "    inference_dataset : str\n",
    "\n",
    "    def save(self,file_path, file_name,include_version = False):\n",
    "        utils.to_pickle(self.data,file_path,file_name,include_version)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pipeline(opik_client, num_datasets : int,config : InferenceArgs,server_client,\n",
    "                       resume : Optional[list[dict[str,dict_vals_dataype]]] = None):\n",
    "    global_ouputs = []\n",
    "\n",
    "    resume_dataset_id = 0\n",
    "    resume_last_dict = None\n",
    "    if resume:\n",
    "        global_ouputs = resume\n",
    "        resume_dataset_id = len(resume)\n",
    "        resume_last_dict = resume.pop()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for dataset_id in range(1,num_datasets+1):\n",
    "        if dataset_id < resume_dataset_id:\n",
    "            continue\n",
    "\n",
    "        #get dataset from commet\n",
    "        dataset = opik_client.get_or_create_dataset(f\"multilingual-chatbot-arena-validation-complete-{dataset_id}\").to_pandas()\n",
    "\n",
    "        #construct Dataset object\n",
    "        dataset = ChatbotDataset(dataset)\n",
    "        n = len(dataset)\n",
    "\n",
    "        if resume_last_dict:\n",
    "            outputs = model_inference(dataset_id,dataset,config,server_client,\n",
    "                                      resume_last_dict)\n",
    "            resume_last_dict = None\n",
    "        else:\n",
    "            outputs = model_inference(dataset_id,dataset,config,server_client)\n",
    "\n",
    "        global_ouputs.append(outputs)\n",
    "\n",
    "        if outputs['last_idx'] < n - 1:\n",
    "            print(f\"Error during batch datasets inferencing...\")\n",
    "            return global_ouputs\n",
    "    return global_ouputs\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Together AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "client = Together(api_key=os.environ['TOGETHER_AI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    inference_file_path = root_repo_directory + c.SLASH + 'datasets_creator/data/inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset : 1 - Model Inference: 100%|██████████| 988/988 [02:36<00:00,  6.33it/s] \n",
      "Dataset : 2 - Model Inference: 100%|██████████| 988/988 [03:28<00:00,  4.74it/s]\n",
      "Dataset : 3 - Model Inference: 100%|██████████| 988/988 [03:32<00:00,  4.65it/s]\n",
      "Dataset : 4 - Model Inference:   8%|▊         | 75/988 [00:15<03:12,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something has gone wrong requesting from API server...\n",
      "Error code: 402 - {\"message\": \"Credit limit exceeded. Please navigate to https://api.together.xyz/settings/billing to upgrade your plan.\", \"type_\": \"credit_limit\"}\n",
      "Error during batch datasets inferencing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_dataset = 'challenge_complete_inference-2'\n",
    "try:\n",
    "    outputs  = inference_pipeline(opik_client,49,config,client,outputs)\n",
    "finally:\n",
    "    out_groq_file = InferenceFile(inference_dataset=inference_dataset,data=outputs)\n",
    "    out_groq_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama-3.1-8b-instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    inference_file_path = root_repo_directory + c.SLASH + 'datasets_creator/data/inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset : 1 - Model Inference:  26%|██▌       | 255/988 [03:35<10:18,  1.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something has gone wrong requesting from API server...\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01jj4kakh5fkrah8srqqd2abrw` service tier `on_demand` on : Limit 500000, Used 496731, Requested 4544. Please try again in 3m40.2668s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}\n",
      "Error during batch datasets inferencing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_dataset = 'challenge_complete_inference-2'\n",
    "try:\n",
    "    outputs  = inference_pipeline(opik_client,49,config,client,outputs)\n",
    "finally:\n",
    "    out_groq_file = InferenceFile(inference_dataset=inference_dataset,data=outputs)\n",
    "    out_groq_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google AI Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models/gemini-1.5-flash-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"models/gemini-1.5-flash-8b\",\n",
    "    inference_file_path = root_repo_directory + c.SLASH + 'datasets_creator/data/inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = genai.GenerativeModel(config.model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kevinmg96/Kaggle competitions/WSDM Cup/multilingual-chatbot-arena/datasets_creator/data/inference/models/gemini-1.5-flash-8b/challenge_complete_inference.pkl'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_repo_directory + '/datasets_creator/data/inference/models/gemini-1.5-flash-8b/challenge_complete_inference.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root_repo_directory + '/datasets_creator/data/inference/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo/challenge_complete_inference-2.pkl', 'rb') as f:\n",
    "    outputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'answers': [1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'last_idx': 74}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset : 1 - Model Inference:   5%|▍         | 45/988 [03:59<1:23:41,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error during batch datasets inferencing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_dataset = 'challenge_complete_inference'\n",
    "try:\n",
    "    outputs  = inference_pipeline(opik_client,49,config,model)\n",
    "finally:\n",
    "    out_groq_file = InferenceFile(inference_dataset=inference_dataset,data=outputs)\n",
    "    out_groq_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudfare Workers AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUDFARE_URL = \"https://api.cloudflare.com/client/v4/accounts/5b297e1ca90d051d39f0c1851824f0ad/ai/run/\"\n",
    "HEADER = {'Authorization' : f\"Bearer {os.environ['CLOUDFARE_API_KEY']}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta/llama-3.3-70b-instruct-fp8-fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"@cf/meta/llama-3.3-70b-instruct-fp8-fast\",\n",
    "    inference_file_path = root_repo_directory + c.SLASH + 'datasets_creator/data/inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = 'validation_dataset_1'\n",
    "out_val_1  = model_inference(validation_dataset_1,config,client)\n",
    "out_val_1_file = InferenceFile(inference_dataset=inference_dataset,**out_val_1)\n",
    "out_val_1_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qwen/Qwen2.5-72B-Instruct - SambaNovaCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.environ['SAMBANOVA_API_KEY'],\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"Qwen2.5-Coder-32B-Instruct\",\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_winners_validation_dataset_1  = model_inference(validation_dataset_1,config,client)\n",
    "output_winners_validation_dataset_2  = model_inference(validation_dataset_2,config,client)\n",
    "output_winners_validation_dataset_3  = model_inference(validation_dataset_3,config,client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_accurracy_metric(output_winners,label_winners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pipeline inference test. Task: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPipelineDataset(Dataset):\n",
    "    def __init__(self,data : pd.DataFrame):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get either a single data point or a pandas Dataframe window of data points\n",
    "        data_window = self.data.iloc[idx]    \n",
    "\n",
    "        return data_window['prompt']\n",
    "    \n",
    "train_pipeline_dataset = CustomPipelineDataset(data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task='zero-shot-classification',model=config.model_name,device_map='auto')\n",
    "pipe.model.config.id2label = id2label\n",
    "pipe.model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_classification_inference(pipe,dataset : Dataset):\n",
    "    try:\n",
    "        prediction_results = defaultdict(int) #predicted model : count\n",
    "\n",
    "        prediction_results = []\n",
    "        for i,prediction_dict in enumerate(tqdm(pipe(dataset,candidate_labels= ['model_a','model_b']),\n",
    "                                                 total=len(dataset))):\n",
    "\n",
    "            best_model_id = np.argmax(prediction_dict['scores']) + 1\n",
    "            prediction_results.append(best_model_id)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"Inference pipeline is interrumpted...\")\n",
    "    finally:\n",
    "        return {'predictions' : prediction_results, 'current_id' : i+1}\n",
    "    \n",
    "\n",
    "predictions =   zero_shot_classification_inference(pipe,train_pipeline_dataset)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently not functional. Model text generation does not align with prompt's requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8\",\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model : {config.model_name} max context length : {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"dataset\" :train_dataset,\n",
    "    \"batch_size\" : config.batch_size\n",
    "}\n",
    "\n",
    "llama_3_2_1b_spinquant_4int_dataloader = ChatbotDataloader(\n",
    "    tokenizer,**arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model into VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/kevinmg96/Kaggle competitions/WSDM Cup/multilingual-chatbot-arena/models/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8/hf_weights\", \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    my_batch = next(iter(llama_3_2_1b_spinquant_4int_dataloader))\n",
    "    my_batch_input = my_batch[\"inputs\"].to(\"cuda\")\n",
    "\n",
    "    #logits = model(**my_batch_input).logits\n",
    "\n",
    "    output_ids  = model.generate(\n",
    "        **my_batch_input,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "    output_ids = output_ids.detach().cpu()\n",
    "\n",
    "    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    print(response)\n",
    "    output_ids = [output_ids[i,my_batch[\"longest_seq\"]:]  for i in range(\n",
    "            output_ids.shape[0])]\n",
    "    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    print(response)\n",
    "\n",
    "    print(parse_output_llm(response))\n",
    "    print(my_batch[\"labels\"])\n",
    "    print(parse_output_llm(my_batch[\"labels\"]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-chatbot-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
