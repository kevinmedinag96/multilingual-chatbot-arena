{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created to test how current open source state-of-the-art LLMs perform for the multilingual-chatbot-arena dataset.\n",
    "\n",
    "The experiment's constraints are as follows:\n",
    "\n",
    "1. Model's inference will be held using an NVIDIA GeForce RTX 4060. Therefore the GPU computing is limited. Recall that is most likely that the current pretrained models with not perform very well on this challenge's data. The benchmarked LLMs must hold around 7-9B parameters in order for the hardware to handle inferencing/fine tuning.\n",
    "2. For the fine tuning of LLMs. The best course of action will be to use QLORA. Due to hardware constraints.\n",
    "3. There are an abundant number of capable open-source LLMs. In this Demo we will be benchmarking 3 model's families: Qwen2.5, Llama 3.X and gemini.\n",
    "4. Dataset for the experiment: training set.\n",
    "5. Performance metric: Accuracy (For the whole dataset, how many prompts what's the proportion of accurately predicted answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch processing workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading challenge's  data from Comet ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-21 13:32:15.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mInitializing env vars...\u001b[0m\n",
      "\u001b[32m2025-01-21 13:32:15.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmultilingual_chatbot_arena\u001b[0m:\u001b[36minitialize\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mLoading environment variables from: /home/kevinmg96/Kaggle competitions/WSDM Cup/multilingual-chatbot-arena/.env\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "root_repo_directory = pathlib.Path().resolve().parent.__str__()\n",
    "sys.path.append(root_repo_directory)\n",
    "from multilingual_chatbot_arena import initialize\n",
    "import datasets_creator.src.constants as c\n",
    "import datasets_creator.src.utils as utils\n",
    "import pandas as pd\n",
    "from fire import Fire\n",
    "from pydantic import BaseModel\n",
    "from typing import List,Optional,Dict,Union\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import opik\n",
    "from loguru import logger\n",
    "initialize()\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'model_a' : 1,\n",
    "    'model_b' : 2\n",
    "}\n",
    "id2label = {\n",
    "    1 : 'model_a',\n",
    "    2 : 'model_b'\n",
    "}\n",
    "\n",
    "model_human_preferance_pattern = r'model_.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting challenge's train dataset\n",
    "client = opik.Opik(workspace=os.environ['COMET_WORKSPACE'],api_key=os.environ['COMET_API_KEY'])\n",
    "dataset_comet = client.get_or_create_dataset(\"multilingual-chatbot-arena-validation-1\")\n",
    "data_validation_1 = dataset_comet.to_pandas()\n",
    "dataset_comet = client.get_or_create_dataset(\"multilingual-chatbot-arena-validation-2\")\n",
    "data_validation_2 = dataset_comet.to_pandas()\n",
    "dataset_comet = client.get_or_create_dataset(\"multilingual-chatbot-arena-validation-3\")\n",
    "data_validation_3 = dataset_comet.to_pandas()\n",
    "\n",
    "def get_answer_id(x):\n",
    "    match = re.search(model_human_preferance_pattern,x.answer)\n",
    "    return label2id[x.answer[match.start():match.end()]]\n",
    "\n",
    "data_validation_1['answer_model_id'] = data_validation_1.apply(get_answer_id,axis=1)\n",
    "data_validation_2['answer_model_id'] = data_validation_2.apply(get_answer_id,axis=1)\n",
    "data_validation_3['answer_model_id'] = data_validation_3.apply(get_answer_id,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>language</th>\n",
       "      <th>prompt</th>\n",
       "      <th>id</th>\n",
       "      <th>answer_model_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b2b-7183-7e0d-bafe-7ddb2f496d16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best model is model_a based on its human prefe...</td>\n",
       "      <td>Russian</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b2b-7182-70b9-84d2-1e794292d8e0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best model is model_b based on its human prefe...</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b2b-7181-7fd4-88bc-6304ae7d015b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best model is model_a based on its human prefe...</td>\n",
       "      <td>English</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b2b-7180-7dda-82bb-cf64466d9b07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best model is model_a based on its human prefe...</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>\\nYou are an expert in assesing LLM's model re...</td>\n",
       "      <td>01945b2b-717f-7c43-96f4-1b4d180881cb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer language  \\\n",
       "0  Best model is model_b based on its human prefe...  Chinese   \n",
       "1  Best model is model_a based on its human prefe...  Russian   \n",
       "2  Best model is model_b based on its human prefe...  Chinese   \n",
       "3  Best model is model_a based on its human prefe...  English   \n",
       "4  Best model is model_a based on its human prefe...  Chinese   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  \\nYou are an expert in assesing LLM's model re...   \n",
       "1  \\nYou are an expert in assesing LLM's model re...   \n",
       "2  \\nYou are an expert in assesing LLM's model re...   \n",
       "3  \\nYou are an expert in assesing LLM's model re...   \n",
       "4  \\nYou are an expert in assesing LLM's model re...   \n",
       "\n",
       "                                     id  answer_model_id  \n",
       "0  01945b2b-7183-7e0d-bafe-7ddb2f496d16                2  \n",
       "1  01945b2b-7182-70b9-84d2-1e794292d8e0                1  \n",
       "2  01945b2b-7181-7fd4-88bc-6304ae7d015b                2  \n",
       "3  01945b2b-7180-7dda-82bb-cf64466d9b07                1  \n",
       "4  01945b2b-717f-7c43-96f4-1b4d180881cb                1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validation_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove from prompt column, system's message declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_substring = \"\"\"\\nYou are an expert in assesing LLM's model response based on a prompt. I will give you an input prompt (**prompt**) with two different responses coming from fellow LLM models; the first model's response is called **response_a** and second model's response is **response_b**. You can find the previous information after the double slashes (//), respecting the correct title based on the proper input.Your task is to assess the content of each response based on its quality and human's language similarity, then choose the model's response which adheres best to the given guidelines.\\nYour response must obey the following format: \"Best model is model_[] based on its human preferability response for the input prompt.\". You will substitute \"[]\" with either \"a\" if you think **response_a** is better than **response_b**, or \"b\" otherwise.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_system_sentence(x):\n",
    "    prompt = x.prompt\n",
    "\n",
    "    return prompt.split(system_substring)[-1]\n",
    "\n",
    "\n",
    "data_validation_1['instructionless_prompt'] = data_validation_1.apply(del_system_sentence,axis=1)\n",
    "data_validation_2['instructionless_prompt'] = data_validation_2.apply(del_system_sentence,axis=1)\n",
    "data_validation_3['instructionless_prompt'] = data_validation_3.apply(del_system_sentence,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom dataset for the imported data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self,data : pd.DataFrame):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get either a single data point or a pandas Dataframe window of data points\n",
    "        data_window = self.data.iloc[idx]    \n",
    "\n",
    "        return data_window.to_dict()\n",
    "    \n",
    "validation_dataset_1 = ChatbotDataset(data_validation_1)\n",
    "validation_dataset_2 = ChatbotDataset(data_validation_2)\n",
    "validation_dataset_3 = ChatbotDataset(data_validation_3)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ChatbotDataloader(DataLoader):\n",
    "    def __init__(self, tokenizer :  PreTrainedTokenizer | PreTrainedTokenizerFast, **kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        kwargs[\"collate_fn\"] = self.chatbot_collate\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    \n",
    "    def chatbot_collate(self,batch):\n",
    "        \"\"\"Custom collate function to teach the Dataloader class how to parse the batches into an llm friendly format\n",
    "        Args:\n",
    "            original_batch : List of batch elements with len -> batch_size. Each list's element strictly follows \n",
    "            the format inside __getitem__ from Dataset class. \n",
    "        \n",
    "        \"\"\"\n",
    "        prompts = []\n",
    "        answers = []\n",
    "        languages = []\n",
    "        for dic in batch:\n",
    "            if self.tokenizer.chat_template: #tokenizer has chate template\n",
    "                \n",
    "                prompt_messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_substring},\n",
    "                    {\"role\" : \"user\", \"content\" : dic[\"instructionless_prompt\"]}\n",
    "                ]\n",
    "\n",
    "                prompt_text  = self.tokenizer.apply_chat_template(\n",
    "                    prompt_messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                prompt_text  = dic[\"prompt\"]\n",
    "                \n",
    "\n",
    "            \"\"\" answer_messages = [\n",
    "                {\"role\" : \"user\", \"content\" : dic[\"answer\"]}\n",
    "            ]\n",
    "\n",
    "            answer_text  = self.tokenizer.apply_chat_template(\n",
    "                answer_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            ) \"\"\"\n",
    "\n",
    "\n",
    "            \n",
    "            prompts.append(prompt_text)\n",
    "            #answers.append(answer_text)\n",
    "            answers.append(dic[\"answer\"])\n",
    "            languages.append(dic[\"language\"])\n",
    "\n",
    "        #tokenize batch of prompts and answers\n",
    "        prompt_tokenize = self.tokenizer(prompts,\n",
    "                padding='longest',truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "        \"\"\" answer_tokenize = self.tokenizer(answers,\n",
    "                padding='longest',truncation=True,return_tensors=\"pt\") \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"inputs\" : prompt_tokenize, #Dict[str,torch.Tensor]\n",
    "            \"labels\" : answers, #List[str]  ##answer_tokenize, #Dict[str,torch.Tensor],\n",
    "            \"languages\" : languages, #List[str]\n",
    "            \"longest_seq\" : prompt_tokenize[\"input_ids\"].shape[1] #int\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Setup (Inference API server version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section models inferencing pipelines for each of the benchmark models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accurracy_metric(predictions,labels):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    unmatched_idxs = [] #incorrectly predicted records idxs\n",
    "    accurracy = 0\n",
    "    for i,(pred,lab) in enumerate(zip(predictions,labels)):\n",
    "        if pred == lab:\n",
    "            accurracy += 1\n",
    "        else:\n",
    "            unmatched_idxs.append(i)\n",
    "\n",
    "    return accurracy / len(predictions), unmatched_idxs\n",
    "\n",
    "\n",
    "def get_model_winner(matches) -> int:\n",
    "    \"\"\"\n",
    "    Extract which model's reponse is better from input_response\n",
    "    \"\"\"\n",
    "    for match in matches:\n",
    "        if 'a' in match:\n",
    "            return 1\n",
    "        elif 'b' in match:\n",
    "            return 2\n",
    "    return 3\n",
    "\n",
    "def parse_output_llm(response) -> int:\n",
    "    \"\"\"\n",
    "    Retrieves a list specifying which of the two paired models in each training record adheres the best\n",
    "    to human responses.\n",
    "\n",
    "    Args:\n",
    "        responses List[str]: Batch of LLM's responses.\n",
    "    \"\"\"\n",
    "    pattern = r'Best model(.*?)based on its human preferability response'\n",
    "\n",
    "\n",
    "    #Extract pattern from response\n",
    "    matches = re.findall(pattern, response,re.DOTALL)        \n",
    "    \n",
    "    return get_model_winner(matches)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_inference(dataset,config,client,resume : Optional[dict] = None) -> dict[str,Union[list[int],int]]:\n",
    "    \"\"\"\n",
    "    Retrieves two lists, the first list specifies the LLM's decisions per record, on which response was more humanly\n",
    "    seen. The other specifies the challenge's ground truth.\n",
    "\n",
    "    Args:\n",
    "        model : HuggingFace Pretrained LLM.\n",
    "    \"\"\"\n",
    "    global_output_winners = []\n",
    "    resume_idx = 0\n",
    "    if resume:\n",
    "        global_output_winners = resume['predictions']\n",
    "        resume_idx = resume['last_idx'] + 1\n",
    "    \n",
    "    try:\n",
    "        current_id = 0\n",
    "        for i,record in enumerate(tqdm(dataset,desc=\"Training set - Model Inference\")):\n",
    "            if i < resume_idx:\n",
    "                continue\n",
    "\n",
    "            messages = [\n",
    "                {'role' : 'system', 'content' : system_substring},\n",
    "                {'role' : 'user',     'content' : record['instructionless_prompt'] }]\n",
    "\n",
    "            server_response = None\n",
    "            try:\n",
    "                server_response = client.chat.completions.create(\n",
    "                        model= config.model_name,\n",
    "                        messages=messages,\n",
    "                        max_tokens=50,\n",
    "                        stream = False\n",
    "                    )\n",
    "                response: str = server_response.choices[0].message.content\n",
    "                \"\"\" input = { \"messages\": messages }\n",
    "                server_response = requests.post(f\"{CLOUDFARE_URL}{config.model_name}\", headers=HEADER, json=input).json()\n",
    "                response = server_response['result']['response'] \"\"\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Something has gone wrong requesting from API server...\")\n",
    "                print(e)\n",
    "            finally:\n",
    "                if not server_response:\n",
    "                    response = 'c'\n",
    "\n",
    "        \n",
    "            global_output_winners.append(parse_output_llm(response))\n",
    "            current_id = i\n",
    "    except KeyboardInterrupt as k:\n",
    "        print(k)\n",
    "    finally:\n",
    "        return {\n",
    "            'predictions' : global_output_winners,\n",
    "            'last_idx' : current_id \n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Inference Arguments and saving results into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InferenceArgs:\n",
    "    model_name : str\n",
    "    inference_file_path : str = root_repo_directory\n",
    "    batch_size : Optional[int] = None\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceFile:\n",
    "\n",
    "    predictions : list[int]\n",
    "    last_idx : int\n",
    "    inference_dataset : str\n",
    "\n",
    "    def save(self,file_path, file_name,include_version = False):\n",
    "\n",
    "        data = {\n",
    "            'predictions' : self.predictions,\n",
    "            'last_idx' : self.last_idx,\n",
    "            'inference_dataset' : self.inference_dataset\n",
    "        }\n",
    "        utils.to_pickle([data],file_path,file_name,include_version)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Together AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from together import Together\n",
    "client = Together(api_key=os.environ['TOGETHER_AI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    inference_file_path = root_repo_directory + c.SLASH + 'datasets_creator/data/inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_dataset = 'validation_dataset_1'\n",
    "out_val_1  = model_inference(validation_dataset_1,config,client)\n",
    "out_val_1_file = InferenceFile(inference_dataset=inference_dataset,**out_val_1)\n",
    "out_val_1_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset)\n",
    "\n",
    "\"\"\" inference_dataset = 'validation_dataset_2'\n",
    "out_val_2  = model_inference(validation_dataset_2,config,client)\n",
    "out_val_2_file = InferenceFile(inference_dataset=inference_dataset,**out_val_2)\n",
    "out_val_2_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset)\n",
    "\n",
    "inference_dataset = 'validation_dataset_3'\n",
    "out_val_3  = model_inference(validation_dataset_3,config,client)\n",
    "out_val_3_file = InferenceFile(inference_dataset=inference_dataset,**out_val_3)\n",
    "out_val_3_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_val_2  = model_inference(validation_dataset_2,config,client,out_val_2)\n",
    "out_val_3  = model_inference(validation_dataset_3,config,client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ[\"GROQ_API_KEY\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama-3.1-8b-instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    inference_file_path = root_repo_directory + c.SLASH + 'datasets_creator/data/inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = 'validation_dataset_2'\n",
    "out_val_2  = model_inference(validation_dataset_2,config,client)\n",
    "out_val_2_file = InferenceFile(inference_dataset=inference_dataset,**out_val_2)\n",
    "out_val_2_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = 'validation_dataset_3'\n",
    "out_val_3  = model_inference(validation_dataset_3,config,client)\n",
    "out_val_3_file = InferenceFile(inference_dataset=inference_dataset,**out_val_3)\n",
    "out_val_3_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloudfare Workers AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUDFARE_URL = \"https://api.cloudflare.com/client/v4/accounts/5b297e1ca90d051d39f0c1851824f0ad/ai/run/\"\n",
    "HEADER = {'Authorization' : f\"Bearer {os.environ['CLOUDFARE_API_KEY']}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta/llama-3.3-70b-instruct-fp8-fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"@cf/meta/llama-3.3-70b-instruct-fp8-fast\",\n",
    "    inference_file_path = root_repo_directory + c.SLASH + 'datasets_creator/data/inference'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = 'validation_dataset_1'\n",
    "out_val_1  = model_inference(validation_dataset_1,config,client)\n",
    "out_val_1_file = InferenceFile(inference_dataset=inference_dataset,**out_val_1)\n",
    "out_val_1_file.save(config.inference_file_path + c.SLASH + config.model_name,inference_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qwen/Qwen2.5-72B-Instruct - SambaNovaCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.environ['SAMBANOVA_API_KEY'],\n",
    "    base_url=\"https://api.sambanova.ai/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"Qwen2.5-Coder-32B-Instruct\",\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_winners_validation_dataset_1  = model_inference(validation_dataset_1,config,client)\n",
    "output_winners_validation_dataset_2  = model_inference(validation_dataset_2,config,client)\n",
    "output_winners_validation_dataset_3  = model_inference(validation_dataset_3,config,client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_accurracy_metric(output_winners,label_winners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model pipeline inference test. Task: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPipelineDataset(Dataset):\n",
    "    def __init__(self,data : pd.DataFrame):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #get either a single data point or a pandas Dataframe window of data points\n",
    "        data_window = self.data.iloc[idx]    \n",
    "\n",
    "        return data_window['prompt']\n",
    "    \n",
    "train_pipeline_dataset = CustomPipelineDataset(data)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task='zero-shot-classification',model=config.model_name,device_map='auto')\n",
    "pipe.model.config.id2label = id2label\n",
    "pipe.model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_classification_inference(pipe,dataset : Dataset):\n",
    "    try:\n",
    "        prediction_results = defaultdict(int) #predicted model : count\n",
    "\n",
    "        prediction_results = []\n",
    "        for i,prediction_dict in enumerate(tqdm(pipe(dataset,candidate_labels= ['model_a','model_b']),\n",
    "                                                 total=len(dataset))):\n",
    "\n",
    "            best_model_id = np.argmax(prediction_dict['scores']) + 1\n",
    "            prediction_results.append(best_model_id)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"Inference pipeline is interrumpted...\")\n",
    "    finally:\n",
    "        return {'predictions' : prediction_results, 'current_id' : i+1}\n",
    "    \n",
    "\n",
    "predictions =   zero_shot_classification_inference(pipe,train_pipeline_dataset)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently not functional. Model text generation does not align with prompt's requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = InferenceArgs(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8\",\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "config.model_name,padding_side=\"left\",legacy=False)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model : {config.model_name} max context length : {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\n",
    "    \"dataset\" :train_dataset,\n",
    "    \"batch_size\" : config.batch_size\n",
    "}\n",
    "\n",
    "llama_3_2_1b_spinquant_4int_dataloader = ChatbotDataloader(\n",
    "    tokenizer,**arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model into VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/kevinmg96/Kaggle competitions/WSDM Cup/multilingual-chatbot-arena/models/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8/hf_weights\", \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    my_batch = next(iter(llama_3_2_1b_spinquant_4int_dataloader))\n",
    "    my_batch_input = my_batch[\"inputs\"].to(\"cuda\")\n",
    "\n",
    "    #logits = model(**my_batch_input).logits\n",
    "\n",
    "    output_ids  = model.generate(\n",
    "        **my_batch_input,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "\n",
    "    output_ids = output_ids.detach().cpu()\n",
    "\n",
    "    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    print(response)\n",
    "    output_ids = [output_ids[i,my_batch[\"longest_seq\"]:]  for i in range(\n",
    "            output_ids.shape[0])]\n",
    "    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    print(response)\n",
    "\n",
    "    print(parse_output_llm(response))\n",
    "    print(my_batch[\"labels\"])\n",
    "    print(parse_output_llm(my_batch[\"labels\"]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilingual-chatbot-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
